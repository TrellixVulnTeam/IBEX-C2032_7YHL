import numpy as np
import numpy
import pickle
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from torch.utils.data import Dataset
import pandas as pd
import os
import time
import copy
from tqdm import tqdm
from torch.autograd import Variable
from sklearn import metrics
import random
import math
import torch.nn.functional as F
import torch.optim as optim

def save_obj(obj, name):
    with open('data/'+ name + '.pkl', 'wb') as f:
        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)

def load_obj(name):
    with open('data/' + name + '.pkl', 'rb') as f:
        return pickle.load(f)
    
def rand_log(a, b):
        x = np.random.sample()
        return 10.0 ** ((np.log10(b) - np.log10(a)) * x + np.log10(a))

def rand_sqrt(a, b):
        x = np.random.sample()
        return (b - a) * np.sqrt(x) + a
    
# train_kwargs, cnn_1s
train_kwargs = {}
train_kwargs['tune_metric'] = 'auprc' # ['auroc', 'auprc', 'avgrank']
train_kwargs['random_seed'] = 1337
train_kwargs['optim'] = 'Adam' # ['SGD', 'Adam', 'Adagrad', 'RMSProp']
train_kwargs['optim_param'] = {'betas': [0.9, 0.999], 'lr': 0.005, 'weight_decay': 1e-5}
train_kwargs['scheduler'] = 'StepLR' # ['StepLR', 'MultiStepLR', 'ReduceLROnPlateau']
train_kwargs['scheduler_param'] = {'step_size': 10, 'gamma': 0.1}
train_kwargs['loss'] = 'CrossEntropyLoss' # ['CrossEntropyLoss', 'NLLLoss', 'MSELoss']
train_kwargs['cudnn'] = False
train_kwargs['imbalance'] = [1.0, 10.0]
train_kwargs['batch_size'] = 128
train_kwargs['encode_mode'] = 'N_2_zero'
# train_kwargs_cnn_2s['num_epochs'] = 40
train_kwargs['num_epochs'] = 50
train_kwargs['shuffle'] = True
train_kwargs['num_workers'] = 8

filename_model='model6'
PATCH_SIZE=10
SEQ_LENGTH=201

if 1==1:
    gpu = torch.cuda.is_available()
else:
    gpu = False

PATCH=[[0.25,0.25,0.25,0.25] for i in range(PATCH_SIZE)]

DATA=load_obj('RAWDATA')
print('Data Size: ',len(DATA))
print('Positive Data Size: ',len([x for x in DATA.keys() if DATA[x]['TYPE']=='POSITIVE']))
print('Negative Data Size: ',len([x for x in DATA.keys() if DATA[x]['TYPE']=='NEGATIVE']))

class MyDataset(Dataset):

    def __init__(self):
        super(MyDataset,self).__init__()

        all_data=[]

        for ID in DATA.keys():
            SEQUENCE=list(DATA[ID]['SEQUENCE'])
            COVERAGE=DATA[ID]['COVERAGE']
            #COVERAGE_NEW=[]
            #for i in range(len(COVERAGE)-1):
            #    COVERAGE_NEW.append(COVERAGE[i+1]-COVERAGE[i])
            #COVERAGE=COVERAGE_NEW
            #COVERAGE.append(0)
            COVERAGE=np.array(COVERAGE)/np.mean(COVERAGE)
            COVERAGE=COVERAGE.T
            #print(COVERAGE)
            
            TYPE=DATA[ID]['TYPE']
            alphabet = np.array(['A', 'T', 'C', 'G'])
            seq = np.array(SEQUENCE, dtype = '|U1').reshape(-1, 1)
            ONEHOT_SEQUENCE = (seq == alphabet).astype(np.float32)
            ONEHOT_SEQUENCE = np.vstack((np.vstack((PATCH,ONEHOT_SEQUENCE)),PATCH))
            #print(ONEHOT_SEQUENCE)
            ONEHOT_SEQUENCE=ONEHOT_SEQUENCE.T.astype(np.float32)

            if TYPE=='POSITIVE':
                all_data.append((ONEHOT_SEQUENCE,COVERAGE,1))
            elif TYPE=='NEGATIVE':
                all_data.append((ONEHOT_SEQUENCE,COVERAGE,0))

        random.shuffle(all_data)
        self.all_data=all_data

    def __len__(self):
        return len(self.all_data)

    def __getitem__(self, index):
        x1,x2,label= self.all_data[index]
        return x1,x2,label

    def get_cross_data(self,index):
        return np.array(self.all_data)[index]
    
class cnn_2s(nn.Module):
    """ Hyperparameters:
        - conv out_channels (filter number)
        - conv kernel_size (kernel size)
        - conv stride
        - conv padding
        - conv dilation
        - conv groups (separate the conv layers towards output)
    """
    def __init__(self):
        super(cnn_2s, self).__init__()

        # x1

        self.conv1 = nn.Conv1d(in_channels=4, out_channels=16, \
            kernel_size=10, stride=1,padding=0, dilation=1,groups=1, bias=True)
        self.relu1 = nn.ReLU(inplace=False)
        self.maxpool1 = nn.MaxPool1d(kernel_size=10, stride=10)
        self.dropout1 = nn.Dropout(p=0.2, inplace=False)
        self.bn1 = nn.BatchNorm1d(self.conv1.out_channels) # conv1.out_channels
        self.fc1 = nn.Linear(336, 2)

        # x2

        self.conv2_1 = nn.Conv1d(in_channels=1, out_channels=16, \
            kernel_size=10, stride=1,padding=0, dilation=1,groups=1, bias=True)
        self.relu2_1 = nn.ReLU(inplace=False)
        self.maxpool2_1 = nn.MaxPool1d(kernel_size=10, stride=10)
        self.dropout2_1 = nn.Dropout(p=0.2, inplace=False)
        self.bn2_1 = nn.BatchNorm1d(self.conv2_1.out_channels) # conv1.out_channels
        self.fc2 = nn.Linear(304, 2)

        # init weights
        for m in self.modules():
            if isinstance(m, nn.Conv1d):
                n = m.kernel_size[0] * m.out_channels
                m.weight.data.normal_(0, math.sqrt(2.0 / n))
                # m.weight.data.normal_(0, kwargs['stdv'])

            # if isinstance(m, nn.BatchNorm1d):
            #     m.weight.data.fill_(1)
            #     m.bias.data.zero_()

    def forward(self,x1,x2):

        # x1
        #print(x1,x1.shape)
        x1 = self.conv1(x1)
        x1 = self.bn1(x1)
        x1 = self.maxpool1(x1)
        x1 = self.relu1(x1)
        x1 = self.dropout1(x1)

        x1 = x1.view(-1, self.num_flat_features(x1))

        #print(x1,x1.shape)
        x1 = self.fc1(x1)
        #print(x1,x1.shape)

        # x2

        x2 = self.conv2_1(x2)
        x2 = self.bn2_1(x2)
        x2 = self.maxpool2_1(x2)
        x2 = self.relu2_1(x2)
        x2 = self.dropout2_1(x2)

        x2 = x2.view(-1, self.num_flat_features(x2))

        x2=self.fc2(x2)
        # x1+x2

        x=torch.mul(x1,x2)

        return x

    def num_flat_features(self, x):
        size = x.size()[1:]
        num_features = 1
        for s in size:
            num_features *= s
        return num_features

class CrossValidationDataset(Dataset):
    """total dataset."""

    def __init__(self, data):
        self.data= data

    def __len__(self):
        return len(self.data)

    def __getitem__(self, index):
        x1,x2,label= self.data[index]
        return x1,x2,label

def partitions(number, k):
    '''
    Distribution of the folds
    Args:
        number: number of patients
        k: folds number
    '''
    n_partitions = np.ones(k) * int(number/k)
    n_partitions[0:(number % k)] += 1
    return n_partitions

def get_indices(n_splits = 3, subjects = 145):
    '''
    Indices of the set test
    Args:
        n_splits: folds number
        subjects: number of patients
    '''
    l = partitions(subjects, n_splits)
    fold_sizes = l
    indices = np.arange(subjects).astype(int)
    current = 0
    for fold_size in fold_sizes:
        start = current
        stop =  current + fold_size
        current = stop
        yield(indices[int(start):int(stop)])

def k_folds(n_splits = 3, subjects = 145):
    '''
    Generates folds for cross validation
    Args:
        n_splits: folds number
        subjects: number of patients
    '''
    indices = np.arange(subjects).astype(int)
    for test_idx in get_indices(n_splits, subjects):
        train_idx = np.setdiff1d(indices, test_idx)
        yield train_idx, test_idx

# read datasets
all_dataset = MyDataset()

tune_metric = train_kwargs['tune_metric']
random_seed = train_kwargs['random_seed']
optim = train_kwargs['optim']
optim_param = train_kwargs['optim_param']
scheduler = train_kwargs['scheduler']
scheduler_param = train_kwargs['scheduler_param']
loss = train_kwargs['loss']
cudnn = train_kwargs['cudnn']
imbalance = train_kwargs['imbalance']
batch_size = train_kwargs['batch_size']
encode_mode = train_kwargs['encode_mode']
num_epochs = train_kwargs['num_epochs']
shuffle = train_kwargs['shuffle']
num_workers = train_kwargs['num_workers']


print("[INFO] dataloaders generated")
print('[INFO] start to train and tune')
print('-' * 10)


num_sequences=all_dataset.__len__()
crossvalidation_count=0

count=0
for train_idx, test_idx in k_folds(n_splits = 5,subjects=num_sequences):
    if count!=0:
        break
    count+=1
    
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.fastest = cudnn
    torch.manual_seed(random_seed)

    if gpu:
        model = cnn_2s().cuda()
        class_imbalance = torch.FloatTensor(imbalance).cuda()
    else:
        model = cnn_2s()
        class_imbalance = torch.FloatTensor(imbalance)

    # build optimizer
    if optim == 'SGD':
        optimizer = torch.optim.SGD(model.parameters(), **optim_param)
    elif optim == 'Adam':
        optimizer = torch.optim.Adam(model.parameters(), **optim_param)
    elif optim == 'Adagrad':
        optimizer = torch.optim.Adagrad(model.parameters(), **optim_param)
    elif optim == 'RMSProp':
        optimizer = torch.optim.RMSProp(model.parameters(), **optim_param)

    # build scheduler
    if scheduler == 'StepLR':
        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, **scheduler_param)
    elif scheduler == 'MultiStepLR':
        scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, **scheduler_param)
    elif scheduler == 'ReduceLROnPlateau':
        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, **scheduler_param)

    if loss == 'CrossEntropyLoss':
        criterion = nn.CrossEntropyLoss(weight=class_imbalance)
    elif loss == 'NLLLoss':
        criterion = nn.NLLLoss(weight=class_imbalance)
    elif loss == 'MSELoss':
        criterion = nn.MSELoss(weight=class_imbalance)

    #print(train_idx,test_idx)
    cross_train_data=all_dataset.get_cross_data(train_idx)
    dataset_train=CrossValidationDataset(data=cross_train_data)
    cross_test_data=all_dataset.get_cross_data(test_idx)
    dataset_test=CrossValidationDataset(data=cross_test_data)

    trainloader = DataLoader(dataset_train, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers)
    valloader = DataLoader(dataset_test, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers)

    dataloaders = {'train': trainloader, 'val': valloader}

    start_time = time.time()
    best_model_wts = copy.deepcopy(model.state_dict())
    best_acc = 0.0
    best_sensitivity = 0.0
    best_auroc = 0.0
    best_auprc = 0.0
    best_epoch = -1
    best_labels_list = []
    epoch_loss_min = float('inf')
    epoch_loss_prev = float('inf')
    uptrend = 0 # num of continuous saturated epoches
    max_uptrend = 10 # max num of continuous aturated epoches
    early_stopping = False
    performance_hist = pd.DataFrame(columns=["loss", "acc", "sensitivity", "auroc", "auprc", "epoch", "phase"])


    for epoch in range(num_epochs):

        print('Epoch {}/{} CrossValidation {}'.format(epoch, num_epochs - 1,crossvalidation_count))
        print('-' * 10)

        dataset_sizes = {'train': 0.0, 'val':0.0} # set to float for later division

        for phase in ['train', 'val']:

            if phase == 'train':
                scheduler.step()
                model.train(True)
            else:
                model.train(False)

            running_loss = 0.0
            running_corrects = 0.0
            running_tp = 0.0 # num of true positive cases
            labels_list = []

            for data in tqdm(dataloaders[phase]):
                #print(data)
                inputs1,inputs2,labels = data
                inputs2=torch.from_numpy(numpy.array([x.numpy().tolist() for x in inputs2]))
                inputs2=inputs2.unsqueeze(1).float()
                #print(inputs1,inputs2,labels)
                if gpu:
                    inputs1,inputs2,labels= Variable(inputs1.cuda()),Variable(inputs2.cuda()),Variable(labels.cuda())
                else:
                    inputs1,inputs2,labels = Variable(inputs1),Variable(inputs2),Variable(labels)

                optimizer.zero_grad()
                outputs = model(inputs1,inputs2)
                #print(outputs.data)
                _, preds = torch.max(outputs.data, 1)
                loss = criterion(outputs, labels)

                labels_list.extend(labels.data)

                if phase == 'train':
                    loss.backward()
                    optimizer.step()

                running_loss += loss * inputs1.size(0)
                running_corrects += torch.sum(preds == labels.data)
                running_tp += torch.sum((preds + labels.data) == 2)
                dataset_sizes[phase] += len(labels)

            epoch_loss = float(running_loss / dataset_sizes[phase])
            epoch_acc = float(running_corrects/ dataset_sizes[phase])
            epoch_sensitivity = float(running_tp / dataset_sizes[phase])

            performance = {'loss': epoch_loss, 'acc': epoch_acc, 'sensitivity': epoch_sensitivity, 'epoch': epoch, 'crossvalidation':crossvalidation_count,'phase': phase}
            performance_hist = performance_hist.append(performance, ignore_index=True)
            print('{} Loss: {:.4f} ACC: {:.4f}, Sensitivity:{:.4f}'.format(phase, epoch_loss, epoch_acc, epoch_sensitivity))


            if phase == 'val':
                if epoch_loss < epoch_loss_min:
                    best_epoch = epoch
                    best_crossvalidation=crossvalidation_count
                    epoch_loss_min = epoch_loss
                    best_acc = epoch_acc
                    best_sensitivity = epoch_sensitivity
                    torch.save(model, 'output/'+filename_model+"_%s"%str(crossvalidation_count))
                    print('[INFO] save model after {} epoch of crossvalition {} to {}'.format(epoch,crossvalidation_count, filename_model+"_%s"%str(crossvalidation_count)))

                if epoch_loss < epoch_loss_prev:
                    uptrend = 0
                    epoch_loss_prev = epoch_loss
                else:
                    uptrend += 1
                    epoch_loss_prev = epoch_loss

                if uptrend == max_uptrend:
                    early_stopping = True
                    print('[INFO] loss: {}, acc: {}, sensitivity: {}, AUROC: {}, AUPRC: {}, best_epoch: {}, total_epoch: {},crossvalidation: {}, phase: {}'.format(epoch_loss, best_acc, best_sensitivity, best_auroc, best_auprc, best_epoch, epoch,crossvalidation_count, phase))

                if early_stopping:
                    print('[INFO] early stop')
                    break
        if early_stopping:
            break

    time_elapsed = time.time()-start_time
    print('Training completes in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))
    performance_hist.to_csv('output/'+filename_model+'_log'+"_%s"%str(crossvalidation_count), sep='\t', index=False)
    print('[INFO] performance_hist saved to {}'.format(filename_model+'_log'))
    print('[INFO] fine-tuned model saved to {}'.format(filename_model))

    with open('output/'+filename_model+'_output'+"_%s"%str(crossvalidation_count), 'w') as f:
        f.write('Acc\tSensitivity\tAUROC\tAUPRC\n')
        f.write('{:.4f}\t{:.4f}\n'.format(best_acc, best_sensitivity))
        f.write('-'*10+'\n')
        f.write('train_kwargs: {}\n'.format(train_kwargs))
        f.write('model: {}\n'.format(filename_model))
        f.write('-'*10+'\n')
    print('[INFO] resnet_2s2a model training output (with settings) saved to {}'.format(filename_output))

    crossvalidation_count+=1


