#!/usr/bin/env python3
# predict.py

import numpy as np
import tensorflow as tf
import sys, os
import argparse
import pandas as pd
from sklearn import metrics
from model_seqcov_varilen import Net
import argparse

############ Parameters ############
BATCH_SIZE = 64
PATCH_SIZE = 10
DEPTH = 32
NUM_HIDDEN = 64
SEQ_LEN = 201 + 2*PATCH_SIZE-2
NUM_CHANNELS = 4
NUM_LABELS = 2
NUM_EPOCHS = 30
TEST_BATCH_SIZE = 50000
############ **************** ############

def pad_dataset1(dataset):
    ''' Pad sequences to (length + 2*DEPTH - 2) wtih 0.25 '''
    new_dataset = np.ones([dataset.shape[0], dataset.shape[1]+2*PATCH_SIZE-2, dataset.shape[2], dataset.shape[3]], dtype = np.float32) * 0.25
    new_dataset[:, PATCH_SIZE-1:-(PATCH_SIZE-1), :, :] = dataset
    return new_dataset

def pad_dataset2(dataset):
    ''' Pad sequences to (length + 2*DEPTH - 2) wtih mean(coverage) '''
    new_dataset = np.ones([dataset.shape[0], dataset.shape[1]+2*PATCH_SIZE-2, dataset.shape[2], dataset.shape[3]], dtype = np.float32) * np.mean(dataset)
    new_dataset[:, PATCH_SIZE-1:-(PATCH_SIZE-1), :, :] = dataset
    return new_dataset

def pad_labels(labels):
    labels = (np.arange(NUM_LABELS) == labels[:,None]).astype(np.float32)
    return labels

def get_data(data_root,up,down):

    data1 = [] # seq
    data2 = [] # coverage
    data3 = [] # information
    labels = []
    
    with open(data_root,'r') as f:
        for line in f:
            line=line.rstrip('\n').split('\t')
            if (line[0]!='#pas_id')&(line[0]!='pas_id'):
                #if int(line[1].split('=')[1])!=0:        # motif
                    SEQUENCE=list(line[7])
                    ID=line[0]
                    data3.append(ID)
                    if len(SEQUENCE)==up+down+1:
                        alphabet = np.array(['A', 'T', 'C', 'G'])
                        seq = np.array(SEQUENCE, dtype = '|U1').reshape(-1, 1)
                        seq_data = (seq == alphabet).astype(np.float32)
                        data1.append(seq_data)
                        #print(seq_data)
                        COVERAGE=[int(float(x)) for x in line[8::]]
                        #print(ID,SEQUENCE,COVERAGE)
                        COVERAGE=np.array(COVERAGE)/max(COVERAGE)
                        data2.append(COVERAGE)
                        if ID[0]=='E': # real pas
                            labels.append(0)
                        else: # fake pas
                            labels.append(1)
                    else:
                        print('Wrong')
                
    data1 = np.stack(data1).reshape([-1, up+down+1, 1, 4])
    data2 = np.stack(data2).reshape([-1, up+down+1, 1, 1])
    
    labels=np.array(labels)
    
    return data1, data2, data3,labels

def get_accuracy(predictions, labels):
      return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))
          / predictions.shape[0])

    
if __name__ == '__main__':
    
    parser = argparse.ArgumentParser(description='Prediction')
    parser.add_argument('--model', help='pretrained model', default='Optimize_UP100_DOWN75_USAGE0.2_TRIMMED10_onlymotif_50_28.npz')
    parser.add_argument('--str', help='pretrained model', default='str1')
    parser.add_argument('--chr', help='pretrained model', default='chr1')
    
    args = parser.parse_args()
    
    pretrained=args.model
    STR=args.str
    CHR=args.chr
    
    with tf.device('/cpu:0'):

                # Build model and trainning graph
                hyper_dict = {'tf_learning_rate': 0.001257480894209189, 'tf_momentum': 0.9800240743506459, 'tf_motif_init_weight': 0.12476684341236771, 'tf_fc_init_weight': 0.02674129616233189, 'tf_motif_weight_decay': 0.00044213243479539205, 'tf_fc_weight_decay': 0.00032473212014796254, 'tf_keep_prob': 0.5}

                model = Net(hyper_dict,100,75)
                model.build_training_graph()

                # Kick off training
                config = tf.ConfigProto(allow_soft_placement=True)
                sess = tf.Session(config=config)
                sess.run(tf.global_variables_initializer())

                model.load_weights(pretrained, sess)
                print('Fine-tuning the pre-trained model %s'%pretrained)

                test_data1,test_data2,test_data3,test_labels=get_data('/home/zhouj0d/c2032/MOUNTAIN/human_brain_scangenome/humanBrain.pAs.scanGenome.step1.%s.REP1.%s.Trimmed10.txt'%(str(STR),str(CHR)),100,75)
                test_data1=pad_dataset1(test_data1)
                test_data2=pad_dataset2(test_data2)
                test_labels=pad_labels(test_labels)
                labels_new=np.argmax(test_labels, 1)
                
                preds_new=[]
                preds_new1=[]
                
                print('Data loaded!')
                
                for i in range(len(labels_new)//TEST_BATCH_SIZE):
                
                    test_data1_tmp=test_data1[(i*TEST_BATCH_SIZE):((i+1)*TEST_BATCH_SIZE)]
                    test_data2_tmp=test_data2[(i*TEST_BATCH_SIZE):((i+1)*TEST_BATCH_SIZE)]

                    pred = model.get_prediction(sess, test_data1_tmp, test_data2_tmp, False)

                    preds_tmp=np.argmax(pred, 1)
                    preds_new=np.append(preds_new,preds_tmp)
                    preds_new1=np.append(preds_new1,pred[:,1])
                    print('TRAIN_ID: %s'%pretrained,i)
                
                test_data1_tmp=test_data1[((i+1)*TEST_BATCH_SIZE)::]
                test_data2_tmp=test_data2[((i+1)*TEST_BATCH_SIZE)::]

                pred = model.get_prediction(sess, test_data1_tmp, test_data2_tmp, False)

                preds_tmp=np.argmax(pred, 1)
                preds_new=np.append(preds_new,preds_tmp)
                preds_new1=np.append(preds_new1,pred[:,1])

                df=pd.DataFrame({'ID':test_data3,'label':labels_new,'pred':preds_new,'pred_raw':preds_new1})
                df.to_csv('/home/zhouj0d/c2032/MOUNTAIN/human_brain_scangenome/Prediction_HM/humanBrain.pAs.scanGenome.step1.%s.REP1.%s.Trimmed10.summary.txt'%(str(STR),str(CHR)),index=False)

                print(STR,CHR)


                
