#!/usr/bin/env python3
# Juexiao
# Data: 1769-09-30


import numpy as np
import tensorflow as tf
import sys, os
import argparse
from sklearn import metrics
from model import Net

############ Parameters ############
BATCH_SIZE = 64
PATCH_SIZE = 10
DEPTH = 32
NUM_HIDDEN = 64
SEQ_LEN = 176 + 2*PATCH_SIZE-2
NUM_CHANNELS = 4
NUM_LABELS = 2
NUM_EPOCHS = 100
############ **************** ############

def pad_dataset1(dataset, labels):
	''' Pad sequences to (length + 2*DEPTH - 2) wtih 0.25 '''
	new_dataset = np.ones([dataset.shape[0], dataset.shape[1]+2*PATCH_SIZE-2, dataset.shape[2], dataset.shape[3]], dtype = np.float32) * 0.25
	new_dataset[:, PATCH_SIZE-1:-(PATCH_SIZE-1), :, :] = dataset
	labels = (np.arange(NUM_LABELS) == labels[:,None]).astype(np.float32)
	return new_dataset, labels

def pad_dataset2(dataset, labels):
	''' Pad sequences to (length + 2*DEPTH - 2) wtih mean(coverage) '''
	new_dataset = np.ones([dataset.shape[0], dataset.shape[1]+2*PATCH_SIZE-2, dataset.shape[2], dataset.shape[3]], dtype = np.float32) * np.mean(dataset)
	new_dataset[:, PATCH_SIZE-1:-(PATCH_SIZE-1), :, :] = dataset
	labels = (np.arange(NUM_LABELS) == labels[:,None]).astype(np.float32)
	return new_dataset, labels


if __name__ == '__main__':

	parser = argparse.ArgumentParser()
	parser.add_argument('data', help='Path to data file, can be .txt file containing sequeces or .npz file containing one-hotencoded sequences')
	parser.add_argument('wts', help='Trained model (.npz file)')
	parser.add_argument('--out', default=None, help='Save predictions to (.txt file)')
	parser.add_argument('--testid', default=None, help='Test ID')
	opts = parser.parse_args()


	data=opts.data
	out=opts.out
	TEST_ID = opts.testid

	# Load and pad data
	data = np.load(data)
	test_data1, test_labels = pad_dataset1(data['test_dataset1'], data['test_labels'])
	test_data2, test_labels = pad_dataset2(data['test_dataset2'], data['test_labels'])
	test_id = data['test_dataset3']
	print("Read %d sequences and %d labels from %s."%(len(test_data1), len(test_labels), opts.data))


	wts=opts.wts
	hyper_dict = {'tf_learning_rate': 0.001257480894209189, 'tf_momentum': 0.9800240743506459, 'tf_motif_init_weight': 0.12476684341236771, 'tf_fc_init_weight': 0.02674129616233189, 'tf_motif_weight_decay': 0.00044213243479539205, 'tf_fc_weight_decay': 0.00032473211764796254, 'tf_keep_prob': 0.5}

	#sess = tf.Session()
	#model = Net()
	model = Net(hyper_dict)

	config = tf.ConfigProto(allow_soft_placement=True)
	sess = tf.Session(config=config)
	sess.run(tf.global_variables_initializer())

	model.load_weights(wts, sess)
	print('test the pre-trained model %s'%wts)

	pred = model.get_prediction(sess, test_data1, test_data2, False)


	predictions = np.argmax(pred, 1)
	with open('predict/%s.txt'%TEST_ID, 'w') as f:
		for idx,pred in enumerate(predictions):
			f.write('%s\t%s\n'%(str(test_id[idx]),str(pred)))
	print('Finished writting TEST_ID: %s'%TEST_ID)
