{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:scikit-learn version 0.21.3 is not supported. Minimum required version: 0.17. Maximum required version: 0.19.2. Disabling scikit-learn conversion API.\n",
      "WARNING:root:Keras version 2.3.1 detected. Last version known to be fully compatible of Keras is 2.2.4 .\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import coremltools\n",
    "from scipy import stats\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Activation,concatenate,BatchNormalization\n",
    "from tensorflow.keras.layers import Input, Dense, Flatten, Add,Lambda,LeakyReLU\n",
    "from tensorflow.keras.optimizers import SGD,Adam,schedules\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.layers import Dropout, Conv1D, MaxPooling1D,MaxPooling1D,GlobalMaxPooling1D,SpatialDropout1D\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras import initializers\n",
    "from tensorflow.keras import constraints\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import Layer, InputSpec\n",
    "from tensorflow.keras.metrics import binary_accuracy\n",
    "from tensorflow.keras.initializers import Ones, Zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class GroupNormalization(Layer):\n",
    "\tdef __init__(self,groups=32,axis=-1,epsilon=1e-5,center=True,scale=True,beta_initializer='zeros',gamma_initializer='ones',beta_regularizer=None,\n",
    "\t\t\t\t gamma_regularizer=None,beta_constraint=None,gamma_constraint=None,**kwargs):\n",
    "\t\tsuper(GroupNormalization, self).__init__(**kwargs)\n",
    "\t\tself.supports_masking = True\n",
    "\t\tself.groups = groups\n",
    "\t\tself.axis = axis\n",
    "\t\tself.epsilon = epsilon\n",
    "\t\tself.center = center\n",
    "\t\tself.scale = scale\n",
    "\t\tself.beta_initializer = initializers.get(beta_initializer)\n",
    "\t\tself.gamma_initializer = initializers.get(gamma_initializer)\n",
    "\t\tself.beta_regularizer = regularizers.get(beta_regularizer)\n",
    "\t\tself.gamma_regularizer = regularizers.get(gamma_regularizer)\n",
    "\t\tself.beta_constraint = constraints.get(beta_constraint)\n",
    "\t\tself.gamma_constraint = constraints.get(gamma_constraint)\n",
    "\n",
    "\tdef build(self, input_shape):\n",
    "\t\tdim = input_shape[self.axis]\n",
    "\n",
    "\t\tif dim is None:\n",
    "\t\t\traise ValueError('Axis '+str(self.axis)+' of input tensor should have a defined dimension but the layer received an input with shape '+str(input_shape)+'.')\n",
    "\n",
    "\t\tif dim < self.groups:\n",
    "\t\t\traise ValueError('Number of groups ('+str(self.groups)+') cannot be more than the number of channels ('+str(dim)+').')\n",
    "\n",
    "\t\tif dim % self.groups != 0:\n",
    "\t\t\traise ValueError('Number of groups ('+str(self.groups)+') must be a multiple of the number of channels ('+str(dim)+').')\n",
    "\n",
    "\t\tself.input_spec = InputSpec(ndim=len(input_shape),axes={self.axis: dim})\n",
    "\t\tshape = (dim,)\n",
    "\n",
    "\t\tif self.scale:\n",
    "\t\t\tself.gamma = self.add_weight(shape=shape,name='gamma',initializer=self.gamma_initializer,regularizer=self.gamma_regularizer,constraint=self.gamma_constraint)\n",
    "\t\telse:\n",
    "\t\t\tself.gamma = None\n",
    "\n",
    "\t\tif self.center:\n",
    "\t\t\tself.beta = self.add_weight(shape=shape,name='beta',initializer=self.beta_initializer,regularizer=self.beta_regularizer,constraint=self.beta_constraint)\n",
    "\t\telse:\n",
    "\t\t\tself.beta = None\n",
    "\n",
    "\t\tself.built = True\n",
    "\n",
    "\tdef call(self, inputs, **kwargs):\n",
    "\t\tinput_shape = K.int_shape(inputs)\n",
    "\t\ttensor_input_shape = K.shape(inputs)\n",
    "\n",
    "\t\t# Prepare broadcasting shape.\n",
    "\t\treduction_axes = list(range(len(input_shape)))\n",
    "\t\tdel reduction_axes[self.axis]\n",
    "\t\tbroadcast_shape = [1] * len(input_shape)\n",
    "\t\tbroadcast_shape[self.axis] = input_shape[self.axis] // self.groups\n",
    "\t\tbroadcast_shape.insert(1, self.groups)\n",
    "\n",
    "\t\treshape_group_shape = K.shape(inputs)\n",
    "\t\tgroup_axes = [reshape_group_shape[i] for i in range(len(input_shape))]\n",
    "\t\tgroup_axes[self.axis] = input_shape[self.axis] // self.groups\n",
    "\t\tgroup_axes.insert(1, self.groups)\n",
    "\n",
    "\t\t# reshape inputs to new group shape\n",
    "\t\tgroup_shape = [group_axes[0], self.groups] + group_axes[2:]\n",
    "\t\tgroup_shape = K.stack(group_shape)\n",
    "\t\tinputs = K.reshape(inputs, group_shape)\n",
    "\n",
    "\t\tgroup_reduction_axes = list(range(len(group_axes)))\n",
    "\t\tgroup_reduction_axes = group_reduction_axes[2:]\n",
    "\n",
    "\t\tmean = K.mean(inputs, axis=group_reduction_axes, keepdims=True)\n",
    "\t\tvariance = K.var(inputs, axis=group_reduction_axes, keepdims=True)\n",
    "\t\tinputs = (inputs - mean) / (K.sqrt(variance + self.epsilon))\n",
    "\n",
    "\t\t# prepare broadcast shape\n",
    "\t\tinputs = K.reshape(inputs, group_shape)\n",
    "\t\toutputs = inputs\n",
    "\n",
    "\t\t# In this case we must explicitly broadcast all parameters.\n",
    "\t\tif self.scale:\n",
    "\t\t\tbroadcast_gamma = K.reshape(self.gamma, broadcast_shape)\n",
    "\t\t\toutputs = outputs * broadcast_gamma\n",
    "\n",
    "\t\tif self.center:\n",
    "\t\t\tbroadcast_beta = K.reshape(self.beta, broadcast_shape)\n",
    "\t\t\toutputs = outputs + broadcast_beta\n",
    "\n",
    "\t\toutputs = K.reshape(outputs, tensor_input_shape)\n",
    "\n",
    "\t\treturn outputs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CNN(inputs):\n",
    "    x = Conv1D(filters= 16, kernel_size= 12, padding = 'valid', kernel_regularizer = regularizers.l2(1e-4), bias_regularizer = regularizers.l2(1e-4))(inputs)\n",
    "    x = GroupNormalization(groups = 4, axis = -1)(x) \n",
    "    x = Activation('relu')(x)\n",
    "    x = MaxPooling1D(pool_size = 12)(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    #x = Conv1D(filters= 16, kernel_size= 12, padding = 'valid', kernel_regularizer = regularizers.l2(1e-4), bias_regularizer = regularizers.l2(1e-4))(x)\n",
    "    #x = Activation('relu')(x)\n",
    "    #x = MaxPooling1D(pool_size = 12)(x)\n",
    "    #x = Dropout(0.5)(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(32,  kernel_regularizer = regularizers.l2(1e-4),bias_regularizer = regularizers.l2(1e-4))(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Regression_CNN(length):\n",
    "\n",
    "    input_shape1 = (length,1)\n",
    "    cov_input = Input(shape = input_shape1,name=\"cov_input\")\n",
    "    input_layers = cov_input\n",
    "\n",
    "    x = CNN(cov_input)\n",
    "    outLayer= Dense(1, activation='linear')(x)\n",
    "\n",
    "    model = Model(inputs=input_layers, outputs=outLayer)\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "LENGTH = 1001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Regression_CNN(LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "cov_input (InputLayer)       [(None, 1001, 1)]         0         \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 990, 16)           208       \n",
      "_________________________________________________________________\n",
      "group_normalization (GroupNo (None, 990, 16)           32        \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 990, 16)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 82, 16)            0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 82, 16)            0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1312)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 32)                42016     \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 42,289\n",
      "Trainable params: 42,289\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prep_data_regression import prep_data,DataGenerator,EvaDataGenerator,get_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of dataset: 11112\n"
     ]
    }
   ],
   "source": [
    "train_data,train_labels,train_id,valid_data,valid_labels,valid_id = prep_data('coverage_data/Finetune.hepg2_control.usage.txt',5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalization(train_data,train_labels,valid_data,valid_labels):\n",
    "    train_data = np.log(train_data+0.01)\n",
    "    train_labels = np.log(train_labels)\n",
    "    valid_data = np.log(valid_data+0.01)\n",
    "    valid_labels = np.log(valid_labels)\n",
    "    data_mean = np.mean(train_data)\n",
    "    data_std  = np.std(train_data)\n",
    "    label_mean = np.mean(train_labels)\n",
    "    label_std  = np.std(train_labels)\n",
    "    \n",
    "    data_max = np.max(train_data)\n",
    "    \n",
    "    train_data = (train_data-data_mean)/data_std\n",
    "    #train_data = train_data/data_max\n",
    "    #train_data  = train_data/300\n",
    "    train_labels = (train_labels-label_mean)/label_std\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    valid_data  = (valid_data-data_mean)/data_std\n",
    "    #valid_data    = valid_data/data_max\n",
    "    #valid_data  = valid_data/300\n",
    "    valid_labels = (valid_labels-label_mean)/label_std\n",
    "    \n",
    "    \n",
    "    return data_mean,data_std,data_max,label_mean,label_std,train_data,train_labels,valid_data,valid_labels\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_mean,data_std,data_max,label_mean,label_std, train_x,train_y,valid_x,valid_y = normalization(train_data,train_labels,valid_data,valid_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-2.1233 2.1027465\n",
      "4.212751 1.6764556\n"
     ]
    }
   ],
   "source": [
    "print(data_mean,data_std)\n",
    "print(label_mean,label_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fa7a547a278>]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(train_x.shape[1]),train_x[5,:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_generator =  DataGenerator(train_x,train_y,train_id,16,LENGTH)\n",
    "validation_generator = DataGenerator(valid_x,valid_y,valid_id,0,LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b= next(iter(training_generator))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(LENGTH),a[9])\n",
    "plt.xticks(np.arange(0, LENGTH, 100))\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_schedule = schedules.ExponentialDecay(5e-4,decay_steps=5000,decay_rate=0.96)\n",
    "#model.compile(loss='mean_squared_logarithmic_error', optimizer= SGD(momentum = 0.98, learning_rate = lr_schedule), metrics=['mse'])\n",
    "model.compile(loss='mean_squared_error', optimizer= SGD(momentum = 0.98, learning_rate = lr_schedule), metrics=['mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainid='hepg2.mle.linear'\n",
    "checkpoint_path = \"Regression_Model/\"+trainid+\"-{epoch:04d}.ckpt\"\n",
    "cp_callback = ModelCheckpoint(filepath=checkpoint_path,save_weights_only=True,verbose=1,period=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 277 steps, validate for 69 steps\n",
      "Epoch 1/2000\n",
      "277/277 [==============================] - 3s 11ms/step - loss: 1.0067 - mse: 0.9990 - val_loss: 0.8415 - val_mse: 0.8333\n",
      "Epoch 2/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.8106 - mse: 0.8024 - val_loss: 0.7218 - val_mse: 0.7136\n",
      "Epoch 3/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.7655 - mse: 0.7573 - val_loss: 0.6486 - val_mse: 0.6404\n",
      "Epoch 4/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.7419 - mse: 0.7337 - val_loss: 0.6168 - val_mse: 0.6086\n",
      "Epoch 5/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.7272 - mse: 0.7190 - val_loss: 0.6436 - val_mse: 0.6354\n",
      "Epoch 6/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.6953 - mse: 0.6871 - val_loss: 0.5957 - val_mse: 0.5875\n",
      "Epoch 7/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.6941 - mse: 0.6859 - val_loss: 0.6308 - val_mse: 0.6226\n",
      "Epoch 8/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.6760 - mse: 0.6678 - val_loss: 0.5862 - val_mse: 0.5780\n",
      "Epoch 9/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.6801 - mse: 0.6718 - val_loss: 0.6075 - val_mse: 0.5993\n",
      "Epoch 10/2000\n",
      "274/277 [============================>.] - ETA: 0s - loss: 0.6680 - mse: 0.6598\n",
      "Epoch 00010: saving model to Regression_Model/hepg2.mle.linear-0010.ckpt\n",
      "277/277 [==============================] - 3s 12ms/step - loss: 0.6673 - mse: 0.6591 - val_loss: 0.5906 - val_mse: 0.5823\n",
      "Epoch 11/2000\n",
      "277/277 [==============================] - 2s 6ms/step - loss: 0.6680 - mse: 0.6597 - val_loss: 0.5900 - val_mse: 0.5818\n",
      "Epoch 12/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.6853 - mse: 0.6771 - val_loss: 0.5889 - val_mse: 0.5807\n",
      "Epoch 13/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.6619 - mse: 0.6537 - val_loss: 0.5931 - val_mse: 0.5849\n",
      "Epoch 14/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.6491 - mse: 0.6408 - val_loss: 0.5955 - val_mse: 0.5872\n",
      "Epoch 15/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.6557 - mse: 0.6475 - val_loss: 0.6074 - val_mse: 0.5991\n",
      "Epoch 16/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.6614 - mse: 0.6531 - val_loss: 0.5801 - val_mse: 0.5719\n",
      "Epoch 17/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.6506 - mse: 0.6423 - val_loss: 0.6011 - val_mse: 0.5929\n",
      "Epoch 18/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.6392 - mse: 0.6310 - val_loss: 0.5997 - val_mse: 0.5915\n",
      "Epoch 19/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.6393 - mse: 0.6310 - val_loss: 0.5791 - val_mse: 0.5709\n",
      "Epoch 20/2000\n",
      "268/277 [============================>.] - ETA: 0s - loss: 0.6587 - mse: 0.6504\n",
      "Epoch 00020: saving model to Regression_Model/hepg2.mle.linear-0020.ckpt\n",
      "277/277 [==============================] - 2s 5ms/step - loss: 0.6532 - mse: 0.6449 - val_loss: 0.5818 - val_mse: 0.5735\n",
      "Epoch 21/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.6500 - mse: 0.6418 - val_loss: 0.5880 - val_mse: 0.5797\n",
      "Epoch 22/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.6406 - mse: 0.6324 - val_loss: 0.6019 - val_mse: 0.5936\n",
      "Epoch 23/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.6351 - mse: 0.6268 - val_loss: 0.5857 - val_mse: 0.5774\n",
      "Epoch 24/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.6369 - mse: 0.6287 - val_loss: 0.5938 - val_mse: 0.5855\n",
      "Epoch 25/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.6395 - mse: 0.6312 - val_loss: 0.6114 - val_mse: 0.6032\n",
      "Epoch 26/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.6349 - mse: 0.6266 - val_loss: 0.5880 - val_mse: 0.5797\n",
      "Epoch 27/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.6323 - mse: 0.6240 - val_loss: 0.5875 - val_mse: 0.5792\n",
      "Epoch 28/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.6322 - mse: 0.6239 - val_loss: 0.5834 - val_mse: 0.5751\n",
      "Epoch 29/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.6336 - mse: 0.6253 - val_loss: 0.5887 - val_mse: 0.5804\n",
      "Epoch 30/2000\n",
      "263/277 [===========================>..] - ETA: 0s - loss: 0.6292 - mse: 0.6209\n",
      "Epoch 00030: saving model to Regression_Model/hepg2.mle.linear-0030.ckpt\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.6318 - mse: 0.6235 - val_loss: 0.5761 - val_mse: 0.5677\n",
      "Epoch 31/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.6373 - mse: 0.6290 - val_loss: 0.5790 - val_mse: 0.5707\n",
      "Epoch 32/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.6254 - mse: 0.6171 - val_loss: 0.5710 - val_mse: 0.5627\n",
      "Epoch 33/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.6239 - mse: 0.6156 - val_loss: 0.5803 - val_mse: 0.5720\n",
      "Epoch 34/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.6280 - mse: 0.6197 - val_loss: 0.6009 - val_mse: 0.5925\n",
      "Epoch 35/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.6291 - mse: 0.6208 - val_loss: 0.6020 - val_mse: 0.5936\n",
      "Epoch 36/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.6305 - mse: 0.6221 - val_loss: 0.5698 - val_mse: 0.5615\n",
      "Epoch 37/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.6198 - mse: 0.6114 - val_loss: 0.5746 - val_mse: 0.5663\n",
      "Epoch 38/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.6252 - mse: 0.6168 - val_loss: 0.5672 - val_mse: 0.5589\n",
      "Epoch 39/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.6248 - mse: 0.6165 - val_loss: 0.5783 - val_mse: 0.5700\n",
      "Epoch 40/2000\n",
      "264/277 [===========================>..] - ETA: 0s - loss: 0.6241 - mse: 0.6158\n",
      "Epoch 00040: saving model to Regression_Model/hepg2.mle.linear-0040.ckpt\n",
      "277/277 [==============================] - 2s 6ms/step - loss: 0.6224 - mse: 0.6141 - val_loss: 0.5785 - val_mse: 0.5702\n",
      "Epoch 41/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.6256 - mse: 0.6173 - val_loss: 0.5819 - val_mse: 0.5736\n",
      "Epoch 42/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.6173 - mse: 0.6090 - val_loss: 0.5659 - val_mse: 0.5576\n",
      "Epoch 43/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.6129 - mse: 0.6046 - val_loss: 0.5739 - val_mse: 0.5656\n",
      "Epoch 44/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.6172 - mse: 0.6089 - val_loss: 0.5741 - val_mse: 0.5658\n",
      "Epoch 45/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.6230 - mse: 0.6147 - val_loss: 0.5766 - val_mse: 0.5682\n",
      "Epoch 46/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.6311 - mse: 0.6228 - val_loss: 0.5717 - val_mse: 0.5634\n",
      "Epoch 47/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.6209 - mse: 0.6126 - val_loss: 0.5696 - val_mse: 0.5613\n",
      "Epoch 48/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.6162 - mse: 0.6079 - val_loss: 0.5715 - val_mse: 0.5632\n",
      "Epoch 49/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.6140 - mse: 0.6057 - val_loss: 0.5655 - val_mse: 0.5572\n",
      "Epoch 50/2000\n",
      "267/277 [===========================>..] - ETA: 0s - loss: 0.6111 - mse: 0.6028\n",
      "Epoch 00050: saving model to Regression_Model/hepg2.mle.linear-0050.ckpt\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.6112 - mse: 0.6029 - val_loss: 0.5681 - val_mse: 0.5598\n",
      "Epoch 51/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.6198 - mse: 0.6115 - val_loss: 0.5759 - val_mse: 0.5676\n",
      "Epoch 52/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.6129 - mse: 0.6046 - val_loss: 0.5656 - val_mse: 0.5573\n",
      "Epoch 53/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.6114 - mse: 0.6031 - val_loss: 0.5692 - val_mse: 0.5609\n",
      "Epoch 54/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.6244 - mse: 0.6161 - val_loss: 0.5765 - val_mse: 0.5682\n",
      "Epoch 55/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.6190 - mse: 0.6107 - val_loss: 0.5700 - val_mse: 0.5617\n",
      "Epoch 56/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.6172 - mse: 0.6089 - val_loss: 0.5900 - val_mse: 0.5817\n",
      "Epoch 57/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.6143 - mse: 0.6060 - val_loss: 0.5671 - val_mse: 0.5588\n",
      "Epoch 58/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.6139 - mse: 0.6056 - val_loss: 0.5697 - val_mse: 0.5614\n",
      "Epoch 59/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.6196 - mse: 0.6113 - val_loss: 0.5634 - val_mse: 0.5551\n",
      "Epoch 60/2000\n",
      "273/277 [============================>.] - ETA: 0s - loss: 0.6089 - mse: 0.6006\n",
      "Epoch 00060: saving model to Regression_Model/hepg2.mle.linear-0060.ckpt\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.6080 - mse: 0.5997 - val_loss: 0.5656 - val_mse: 0.5573\n",
      "Epoch 61/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.6230 - mse: 0.6146 - val_loss: 0.5625 - val_mse: 0.5542\n",
      "Epoch 62/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.6156 - mse: 0.6073 - val_loss: 0.5677 - val_mse: 0.5594\n",
      "Epoch 63/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.6126 - mse: 0.6042 - val_loss: 0.5681 - val_mse: 0.5598\n",
      "Epoch 64/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.6184 - mse: 0.6101 - val_loss: 0.5664 - val_mse: 0.5581\n",
      "Epoch 65/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.6053 - mse: 0.5970 - val_loss: 0.5661 - val_mse: 0.5578\n",
      "Epoch 66/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.6173 - mse: 0.6090 - val_loss: 0.5663 - val_mse: 0.5580\n",
      "Epoch 67/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.6103 - mse: 0.6020 - val_loss: 0.5728 - val_mse: 0.5645\n",
      "Epoch 68/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.6091 - mse: 0.6008 - val_loss: 0.5654 - val_mse: 0.5571\n",
      "Epoch 69/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.6069 - mse: 0.5986 - val_loss: 0.5804 - val_mse: 0.5721\n",
      "Epoch 70/2000\n",
      "275/277 [============================>.] - ETA: 0s - loss: 0.6125 - mse: 0.6042\n",
      "Epoch 00070: saving model to Regression_Model/hepg2.mle.linear-0070.ckpt\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.6134 - mse: 0.6051 - val_loss: 0.5688 - val_mse: 0.5605\n",
      "Epoch 71/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.6152 - mse: 0.6069 - val_loss: 0.5748 - val_mse: 0.5665\n",
      "Epoch 72/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.6052 - mse: 0.5969 - val_loss: 0.5645 - val_mse: 0.5562\n",
      "Epoch 73/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.6032 - mse: 0.5949 - val_loss: 0.5665 - val_mse: 0.5583\n",
      "Epoch 74/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.6118 - mse: 0.6035 - val_loss: 0.5606 - val_mse: 0.5523\n",
      "Epoch 75/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.6170 - mse: 0.6087 - val_loss: 0.5603 - val_mse: 0.5520\n",
      "Epoch 76/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.6140 - mse: 0.6057 - val_loss: 0.5726 - val_mse: 0.5643\n",
      "Epoch 77/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.6049 - mse: 0.5966 - val_loss: 0.5699 - val_mse: 0.5616\n",
      "Epoch 78/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.6049 - mse: 0.5966 - val_loss: 0.5749 - val_mse: 0.5666\n",
      "Epoch 79/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.6045 - mse: 0.5962 - val_loss: 0.5556 - val_mse: 0.5473\n",
      "Epoch 80/2000\n",
      "270/277 [============================>.] - ETA: 0s - loss: 0.6091 - mse: 0.6008\n",
      "Epoch 00080: saving model to Regression_Model/hepg2.mle.linear-0080.ckpt\n",
      "277/277 [==============================] - 2s 6ms/step - loss: 0.6080 - mse: 0.5997 - val_loss: 0.5555 - val_mse: 0.5472\n",
      "Epoch 81/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.6102 - mse: 0.6019 - val_loss: 0.5591 - val_mse: 0.5508\n",
      "Epoch 82/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.6112 - mse: 0.6029 - val_loss: 0.5608 - val_mse: 0.5525\n",
      "Epoch 83/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.6067 - mse: 0.5985 - val_loss: 0.5743 - val_mse: 0.5660\n",
      "Epoch 84/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.6103 - mse: 0.6020 - val_loss: 0.5567 - val_mse: 0.5484\n",
      "Epoch 85/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5997 - mse: 0.5914 - val_loss: 0.5566 - val_mse: 0.5483\n",
      "Epoch 86/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.6168 - mse: 0.6085 - val_loss: 0.5631 - val_mse: 0.5548\n",
      "Epoch 87/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.6075 - mse: 0.5993 - val_loss: 0.5607 - val_mse: 0.5524\n",
      "Epoch 88/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.6068 - mse: 0.5985 - val_loss: 0.5686 - val_mse: 0.5604\n",
      "Epoch 89/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.6015 - mse: 0.5932 - val_loss: 0.5593 - val_mse: 0.5510\n",
      "Epoch 90/2000\n",
      "271/277 [============================>.] - ETA: 0s - loss: 0.6060 - mse: 0.5977\n",
      "Epoch 00090: saving model to Regression_Model/hepg2.mle.linear-0090.ckpt\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.6074 - mse: 0.5991 - val_loss: 0.5749 - val_mse: 0.5667\n",
      "Epoch 91/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.6055 - mse: 0.5972 - val_loss: 0.5565 - val_mse: 0.5482\n",
      "Epoch 92/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.6065 - mse: 0.5982 - val_loss: 0.5581 - val_mse: 0.5498\n",
      "Epoch 93/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.6059 - mse: 0.5976 - val_loss: 0.5627 - val_mse: 0.5544\n",
      "Epoch 94/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.6000 - mse: 0.5917 - val_loss: 0.5628 - val_mse: 0.5545\n",
      "Epoch 95/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.6097 - mse: 0.6014 - val_loss: 0.5642 - val_mse: 0.5559\n",
      "Epoch 96/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.6132 - mse: 0.6049 - val_loss: 0.5629 - val_mse: 0.5546\n",
      "Epoch 97/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.6065 - mse: 0.5982 - val_loss: 0.5720 - val_mse: 0.5637\n",
      "Epoch 98/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.6130 - mse: 0.6047 - val_loss: 0.5734 - val_mse: 0.5651\n",
      "Epoch 99/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.6135 - mse: 0.6052 - val_loss: 0.5640 - val_mse: 0.5557\n",
      "Epoch 100/2000\n",
      "276/277 [============================>.] - ETA: 0s - loss: 0.6040 - mse: 0.5957\n",
      "Epoch 00100: saving model to Regression_Model/hepg2.mle.linear-0100.ckpt\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.6045 - mse: 0.5962 - val_loss: 0.5671 - val_mse: 0.5588\n",
      "Epoch 101/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.6019 - mse: 0.5936 - val_loss: 0.5570 - val_mse: 0.5487\n",
      "Epoch 102/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.6037 - mse: 0.5954 - val_loss: 0.5622 - val_mse: 0.5540\n",
      "Epoch 103/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5972 - mse: 0.5889 - val_loss: 0.5580 - val_mse: 0.5497\n",
      "Epoch 104/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.6035 - mse: 0.5952 - val_loss: 0.5704 - val_mse: 0.5621\n",
      "Epoch 105/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.6014 - mse: 0.5932 - val_loss: 0.5651 - val_mse: 0.5568\n",
      "Epoch 106/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5996 - mse: 0.5913 - val_loss: 0.5566 - val_mse: 0.5483\n",
      "Epoch 107/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.6012 - mse: 0.5929 - val_loss: 0.5588 - val_mse: 0.5506\n",
      "Epoch 108/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5927 - mse: 0.5844 - val_loss: 0.5595 - val_mse: 0.5512\n",
      "Epoch 109/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.6111 - mse: 0.6028 - val_loss: 0.5601 - val_mse: 0.5518\n",
      "Epoch 110/2000\n",
      "271/277 [============================>.] - ETA: 0s - loss: 0.6090 - mse: 0.6007\n",
      "Epoch 00110: saving model to Regression_Model/hepg2.mle.linear-0110.ckpt\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.6062 - mse: 0.5979 - val_loss: 0.5699 - val_mse: 0.5617\n",
      "Epoch 111/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5992 - mse: 0.5909 - val_loss: 0.5619 - val_mse: 0.5537\n",
      "Epoch 112/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.6094 - mse: 0.6012 - val_loss: 0.5552 - val_mse: 0.5469\n",
      "Epoch 113/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.6010 - mse: 0.5928 - val_loss: 0.5589 - val_mse: 0.5506\n",
      "Epoch 114/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5994 - mse: 0.5912 - val_loss: 0.5560 - val_mse: 0.5477\n",
      "Epoch 115/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5922 - mse: 0.5839 - val_loss: 0.5574 - val_mse: 0.5492\n",
      "Epoch 116/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5989 - mse: 0.5907 - val_loss: 0.5657 - val_mse: 0.5575\n",
      "Epoch 117/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.6034 - mse: 0.5952 - val_loss: 0.5602 - val_mse: 0.5519\n",
      "Epoch 118/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.6042 - mse: 0.5960 - val_loss: 0.5710 - val_mse: 0.5627\n",
      "Epoch 119/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.6055 - mse: 0.5973 - val_loss: 0.5596 - val_mse: 0.5513\n",
      "Epoch 120/2000\n",
      "272/277 [============================>.] - ETA: 0s - loss: 0.6001 - mse: 0.5919\n",
      "Epoch 00120: saving model to Regression_Model/hepg2.mle.linear-0120.ckpt\n",
      "277/277 [==============================] - 2s 6ms/step - loss: 0.6013 - mse: 0.5930 - val_loss: 0.5652 - val_mse: 0.5569\n",
      "Epoch 121/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5902 - mse: 0.5819 - val_loss: 0.5553 - val_mse: 0.5470\n",
      "Epoch 122/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5980 - mse: 0.5898 - val_loss: 0.5695 - val_mse: 0.5613\n",
      "Epoch 123/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.6083 - mse: 0.6000 - val_loss: 0.5600 - val_mse: 0.5517\n",
      "Epoch 124/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.6045 - mse: 0.5962 - val_loss: 0.5593 - val_mse: 0.5510\n",
      "Epoch 125/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5939 - mse: 0.5856 - val_loss: 0.5555 - val_mse: 0.5472\n",
      "Epoch 126/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.6095 - mse: 0.6013 - val_loss: 0.5659 - val_mse: 0.5576\n",
      "Epoch 127/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5976 - mse: 0.5893 - val_loss: 0.5524 - val_mse: 0.5442\n",
      "Epoch 128/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5990 - mse: 0.5908 - val_loss: 0.5568 - val_mse: 0.5485\n",
      "Epoch 129/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5911 - mse: 0.5829 - val_loss: 0.5580 - val_mse: 0.5498\n",
      "Epoch 130/2000\n",
      "275/277 [============================>.] - ETA: 0s - loss: 0.5995 - mse: 0.5913\n",
      "Epoch 00130: saving model to Regression_Model/hepg2.mle.linear-0130.ckpt\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5986 - mse: 0.5904 - val_loss: 0.5608 - val_mse: 0.5525\n",
      "Epoch 131/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5937 - mse: 0.5855 - val_loss: 0.5598 - val_mse: 0.5516\n",
      "Epoch 132/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.6014 - mse: 0.5931 - val_loss: 0.5617 - val_mse: 0.5535\n",
      "Epoch 133/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.6021 - mse: 0.5939 - val_loss: 0.5836 - val_mse: 0.5753\n",
      "Epoch 134/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5929 - mse: 0.5847 - val_loss: 0.5626 - val_mse: 0.5544\n",
      "Epoch 135/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5975 - mse: 0.5893 - val_loss: 0.5674 - val_mse: 0.5591\n",
      "Epoch 136/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5988 - mse: 0.5906 - val_loss: 0.5807 - val_mse: 0.5724\n",
      "Epoch 137/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5937 - mse: 0.5855 - val_loss: 0.5929 - val_mse: 0.5847\n",
      "Epoch 138/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5992 - mse: 0.5910 - val_loss: 0.5616 - val_mse: 0.5534\n",
      "Epoch 139/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5895 - mse: 0.5813 - val_loss: 0.5536 - val_mse: 0.5454\n",
      "Epoch 140/2000\n",
      "276/277 [============================>.] - ETA: 0s - loss: 0.5984 - mse: 0.5902\n",
      "Epoch 00140: saving model to Regression_Model/hepg2.mle.linear-0140.ckpt\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5987 - mse: 0.5905 - val_loss: 0.5599 - val_mse: 0.5517\n",
      "Epoch 141/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5926 - mse: 0.5844 - val_loss: 0.5569 - val_mse: 0.5487\n",
      "Epoch 142/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.6035 - mse: 0.5953 - val_loss: 0.5588 - val_mse: 0.5506\n",
      "Epoch 143/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5996 - mse: 0.5914 - val_loss: 0.5593 - val_mse: 0.5510\n",
      "Epoch 144/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5947 - mse: 0.5865 - val_loss: 0.5633 - val_mse: 0.5551\n",
      "Epoch 145/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5961 - mse: 0.5879 - val_loss: 0.5584 - val_mse: 0.5502\n",
      "Epoch 146/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5914 - mse: 0.5832 - val_loss: 0.5563 - val_mse: 0.5481\n",
      "Epoch 147/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5974 - mse: 0.5892 - val_loss: 0.5543 - val_mse: 0.5461\n",
      "Epoch 148/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5960 - mse: 0.5878 - val_loss: 0.5617 - val_mse: 0.5535\n",
      "Epoch 149/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5950 - mse: 0.5868 - val_loss: 0.5601 - val_mse: 0.5518\n",
      "Epoch 150/2000\n",
      "273/277 [============================>.] - ETA: 0s - loss: 0.5969 - mse: 0.5887\n",
      "Epoch 00150: saving model to Regression_Model/hepg2.mle.linear-0150.ckpt\n",
      "277/277 [==============================] - 2s 6ms/step - loss: 0.5996 - mse: 0.5914 - val_loss: 0.5556 - val_mse: 0.5474\n",
      "Epoch 151/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5881 - mse: 0.5799 - val_loss: 0.5581 - val_mse: 0.5499\n",
      "Epoch 152/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5965 - mse: 0.5883 - val_loss: 0.5635 - val_mse: 0.5553\n",
      "Epoch 153/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5881 - mse: 0.5799 - val_loss: 0.5562 - val_mse: 0.5480\n",
      "Epoch 154/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5923 - mse: 0.5841 - val_loss: 0.5598 - val_mse: 0.5516\n",
      "Epoch 155/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5925 - mse: 0.5843 - val_loss: 0.5543 - val_mse: 0.5461\n",
      "Epoch 156/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5930 - mse: 0.5848 - val_loss: 0.5588 - val_mse: 0.5507\n",
      "Epoch 157/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5970 - mse: 0.5888 - val_loss: 0.5582 - val_mse: 0.5500\n",
      "Epoch 158/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5929 - mse: 0.5847 - val_loss: 0.5574 - val_mse: 0.5492\n",
      "Epoch 159/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5895 - mse: 0.5813 - val_loss: 0.5631 - val_mse: 0.5550\n",
      "Epoch 160/2000\n",
      "275/277 [============================>.] - ETA: 0s - loss: 0.5881 - mse: 0.5799\n",
      "Epoch 00160: saving model to Regression_Model/hepg2.mle.linear-0160.ckpt\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5891 - mse: 0.5809 - val_loss: 0.5584 - val_mse: 0.5503\n",
      "Epoch 161/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5916 - mse: 0.5834 - val_loss: 0.5598 - val_mse: 0.5516\n",
      "Epoch 162/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5839 - mse: 0.5757 - val_loss: 0.5577 - val_mse: 0.5496\n",
      "Epoch 163/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5897 - mse: 0.5816 - val_loss: 0.5550 - val_mse: 0.5468\n",
      "Epoch 164/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5931 - mse: 0.5849 - val_loss: 0.5622 - val_mse: 0.5540\n",
      "Epoch 165/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5858 - mse: 0.5777 - val_loss: 0.5600 - val_mse: 0.5519\n",
      "Epoch 166/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5891 - mse: 0.5809 - val_loss: 0.5599 - val_mse: 0.5517\n",
      "Epoch 167/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5832 - mse: 0.5750 - val_loss: 0.5645 - val_mse: 0.5563\n",
      "Epoch 168/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5890 - mse: 0.5808 - val_loss: 0.5599 - val_mse: 0.5517\n",
      "Epoch 169/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5966 - mse: 0.5884 - val_loss: 0.5608 - val_mse: 0.5526\n",
      "Epoch 170/2000\n",
      "270/277 [============================>.] - ETA: 0s - loss: 0.5941 - mse: 0.5859\n",
      "Epoch 00170: saving model to Regression_Model/hepg2.mle.linear-0170.ckpt\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5917 - mse: 0.5835 - val_loss: 0.5556 - val_mse: 0.5474\n",
      "Epoch 171/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5845 - mse: 0.5763 - val_loss: 0.5593 - val_mse: 0.5511\n",
      "Epoch 172/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5826 - mse: 0.5744 - val_loss: 0.5583 - val_mse: 0.5501\n",
      "Epoch 173/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5920 - mse: 0.5839 - val_loss: 0.5690 - val_mse: 0.5609\n",
      "Epoch 174/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5959 - mse: 0.5878 - val_loss: 0.5619 - val_mse: 0.5537\n",
      "Epoch 175/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5910 - mse: 0.5829 - val_loss: 0.5653 - val_mse: 0.5572\n",
      "Epoch 176/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5888 - mse: 0.5807 - val_loss: 0.5673 - val_mse: 0.5591\n",
      "Epoch 177/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5898 - mse: 0.5817 - val_loss: 0.5591 - val_mse: 0.5509\n",
      "Epoch 178/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5866 - mse: 0.5785 - val_loss: 0.5569 - val_mse: 0.5487\n",
      "Epoch 179/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5890 - mse: 0.5809 - val_loss: 0.5596 - val_mse: 0.5515\n",
      "Epoch 180/2000\n",
      "268/277 [============================>.] - ETA: 0s - loss: 0.5873 - mse: 0.5792\n",
      "Epoch 00180: saving model to Regression_Model/hepg2.mle.linear-0180.ckpt\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5874 - mse: 0.5793 - val_loss: 0.5629 - val_mse: 0.5547\n",
      "Epoch 181/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5897 - mse: 0.5816 - val_loss: 0.5596 - val_mse: 0.5515\n",
      "Epoch 182/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5927 - mse: 0.5846 - val_loss: 0.5555 - val_mse: 0.5474\n",
      "Epoch 183/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5821 - mse: 0.5740 - val_loss: 0.5599 - val_mse: 0.5518\n",
      "Epoch 184/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5880 - mse: 0.5799 - val_loss: 0.5632 - val_mse: 0.5551\n",
      "Epoch 185/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.6015 - mse: 0.5934 - val_loss: 0.5552 - val_mse: 0.5471\n",
      "Epoch 186/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5842 - mse: 0.5761 - val_loss: 0.5576 - val_mse: 0.5495\n",
      "Epoch 187/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5906 - mse: 0.5825 - val_loss: 0.5554 - val_mse: 0.5473\n",
      "Epoch 188/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5892 - mse: 0.5812 - val_loss: 0.5610 - val_mse: 0.5529\n",
      "Epoch 189/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5912 - mse: 0.5831 - val_loss: 0.5584 - val_mse: 0.5503\n",
      "Epoch 190/2000\n",
      "269/277 [============================>.] - ETA: 0s - loss: 0.5920 - mse: 0.5839\n",
      "Epoch 00190: saving model to Regression_Model/hepg2.mle.linear-0190.ckpt\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5929 - mse: 0.5848 - val_loss: 0.5597 - val_mse: 0.5516\n",
      "Epoch 191/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5909 - mse: 0.5828 - val_loss: 0.5569 - val_mse: 0.5488\n",
      "Epoch 192/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5863 - mse: 0.5782 - val_loss: 0.5559 - val_mse: 0.5478\n",
      "Epoch 193/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5809 - mse: 0.5728 - val_loss: 0.5644 - val_mse: 0.5563\n",
      "Epoch 194/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5861 - mse: 0.5780 - val_loss: 0.5552 - val_mse: 0.5471\n",
      "Epoch 195/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5869 - mse: 0.5788 - val_loss: 0.5568 - val_mse: 0.5487\n",
      "Epoch 196/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5886 - mse: 0.5805 - val_loss: 0.5570 - val_mse: 0.5489\n",
      "Epoch 197/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5897 - mse: 0.5817 - val_loss: 0.5624 - val_mse: 0.5543\n",
      "Epoch 198/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5882 - mse: 0.5802 - val_loss: 0.5555 - val_mse: 0.5475\n",
      "Epoch 199/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5801 - mse: 0.5720 - val_loss: 0.5556 - val_mse: 0.5475\n",
      "Epoch 200/2000\n",
      "273/277 [============================>.] - ETA: 0s - loss: 0.5906 - mse: 0.5825\n",
      "Epoch 00200: saving model to Regression_Model/hepg2.mle.linear-0200.ckpt\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5909 - mse: 0.5829 - val_loss: 0.5559 - val_mse: 0.5478\n",
      "Epoch 201/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5885 - mse: 0.5804 - val_loss: 0.5557 - val_mse: 0.5476\n",
      "Epoch 202/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5880 - mse: 0.5799 - val_loss: 0.5626 - val_mse: 0.5546\n",
      "Epoch 203/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5870 - mse: 0.5790 - val_loss: 0.5560 - val_mse: 0.5479\n",
      "Epoch 204/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5904 - mse: 0.5824 - val_loss: 0.5634 - val_mse: 0.5554\n",
      "Epoch 205/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5879 - mse: 0.5799 - val_loss: 0.5532 - val_mse: 0.5451\n",
      "Epoch 206/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5916 - mse: 0.5835 - val_loss: 0.5610 - val_mse: 0.5530\n",
      "Epoch 207/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5914 - mse: 0.5833 - val_loss: 0.5551 - val_mse: 0.5470\n",
      "Epoch 208/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5917 - mse: 0.5837 - val_loss: 0.5542 - val_mse: 0.5461\n",
      "Epoch 209/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5858 - mse: 0.5778 - val_loss: 0.5569 - val_mse: 0.5489\n",
      "Epoch 210/2000\n",
      "273/277 [============================>.] - ETA: 0s - loss: 0.5865 - mse: 0.5785\n",
      "Epoch 00210: saving model to Regression_Model/hepg2.mle.linear-0210.ckpt\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5850 - mse: 0.5769 - val_loss: 0.5561 - val_mse: 0.5481\n",
      "Epoch 211/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5835 - mse: 0.5755 - val_loss: 0.5601 - val_mse: 0.5520\n",
      "Epoch 212/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5823 - mse: 0.5743 - val_loss: 0.5634 - val_mse: 0.5554\n",
      "Epoch 213/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5910 - mse: 0.5830 - val_loss: 0.5562 - val_mse: 0.5482\n",
      "Epoch 214/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5896 - mse: 0.5816 - val_loss: 0.5555 - val_mse: 0.5474\n",
      "Epoch 215/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5849 - mse: 0.5769 - val_loss: 0.5611 - val_mse: 0.5531\n",
      "Epoch 216/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5812 - mse: 0.5732 - val_loss: 0.5575 - val_mse: 0.5495\n",
      "Epoch 217/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5883 - mse: 0.5803 - val_loss: 0.5548 - val_mse: 0.5468\n",
      "Epoch 218/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5859 - mse: 0.5779 - val_loss: 0.5584 - val_mse: 0.5504\n",
      "Epoch 219/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5824 - mse: 0.5744 - val_loss: 0.5534 - val_mse: 0.5454\n",
      "Epoch 220/2000\n",
      "266/277 [===========================>..] - ETA: 0s - loss: 0.5940 - mse: 0.5860\n",
      "Epoch 00220: saving model to Regression_Model/hepg2.mle.linear-0220.ckpt\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5896 - mse: 0.5816 - val_loss: 0.5551 - val_mse: 0.5471\n",
      "Epoch 221/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5827 - mse: 0.5747 - val_loss: 0.5624 - val_mse: 0.5544\n",
      "Epoch 222/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5788 - mse: 0.5708 - val_loss: 0.5532 - val_mse: 0.5452\n",
      "Epoch 223/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5758 - mse: 0.5678 - val_loss: 0.5517 - val_mse: 0.5436\n",
      "Epoch 224/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5802 - mse: 0.5722 - val_loss: 0.5552 - val_mse: 0.5472\n",
      "Epoch 225/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5935 - mse: 0.5855 - val_loss: 0.5533 - val_mse: 0.5453\n",
      "Epoch 226/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5884 - mse: 0.5804 - val_loss: 0.5586 - val_mse: 0.5506\n",
      "Epoch 227/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5834 - mse: 0.5754 - val_loss: 0.5564 - val_mse: 0.5484\n",
      "Epoch 228/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5788 - mse: 0.5708 - val_loss: 0.5551 - val_mse: 0.5471\n",
      "Epoch 229/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5898 - mse: 0.5818 - val_loss: 0.5626 - val_mse: 0.5546\n",
      "Epoch 230/2000\n",
      "268/277 [============================>.] - ETA: 0s - loss: 0.5884 - mse: 0.5804\n",
      "Epoch 00230: saving model to Regression_Model/hepg2.mle.linear-0230.ckpt\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5900 - mse: 0.5820 - val_loss: 0.5531 - val_mse: 0.5451\n",
      "Epoch 231/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5853 - mse: 0.5774 - val_loss: 0.5633 - val_mse: 0.5553\n",
      "Epoch 232/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5792 - mse: 0.5712 - val_loss: 0.5534 - val_mse: 0.5454\n",
      "Epoch 233/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5787 - mse: 0.5707 - val_loss: 0.5583 - val_mse: 0.5503\n",
      "Epoch 234/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5875 - mse: 0.5795 - val_loss: 0.5586 - val_mse: 0.5506\n",
      "Epoch 235/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5874 - mse: 0.5794 - val_loss: 0.5569 - val_mse: 0.5489\n",
      "Epoch 236/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5850 - mse: 0.5770 - val_loss: 0.5587 - val_mse: 0.5507\n",
      "Epoch 237/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5833 - mse: 0.5753 - val_loss: 0.5685 - val_mse: 0.5605\n",
      "Epoch 238/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5854 - mse: 0.5775 - val_loss: 0.5653 - val_mse: 0.5573\n",
      "Epoch 239/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5827 - mse: 0.5747 - val_loss: 0.5584 - val_mse: 0.5504\n",
      "Epoch 240/2000\n",
      "269/277 [============================>.] - ETA: 0s - loss: 0.5790 - mse: 0.5710\n",
      "Epoch 00240: saving model to Regression_Model/hepg2.mle.linear-0240.ckpt\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5799 - mse: 0.5719 - val_loss: 0.5605 - val_mse: 0.5525\n",
      "Epoch 241/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5779 - mse: 0.5700 - val_loss: 0.5581 - val_mse: 0.5501\n",
      "Epoch 242/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5854 - mse: 0.5774 - val_loss: 0.5783 - val_mse: 0.5704\n",
      "Epoch 243/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5894 - mse: 0.5814 - val_loss: 0.5634 - val_mse: 0.5555\n",
      "Epoch 244/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5873 - mse: 0.5793 - val_loss: 0.5604 - val_mse: 0.5524\n",
      "Epoch 245/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5843 - mse: 0.5763 - val_loss: 0.5587 - val_mse: 0.5507\n",
      "Epoch 246/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5834 - mse: 0.5754 - val_loss: 0.5580 - val_mse: 0.5501\n",
      "Epoch 247/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5842 - mse: 0.5763 - val_loss: 0.5623 - val_mse: 0.5544\n",
      "Epoch 248/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5805 - mse: 0.5726 - val_loss: 0.5556 - val_mse: 0.5476\n",
      "Epoch 249/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5795 - mse: 0.5716 - val_loss: 0.5654 - val_mse: 0.5574\n",
      "Epoch 250/2000\n",
      "275/277 [============================>.] - ETA: 0s - loss: 0.5799 - mse: 0.5720\n",
      "Epoch 00250: saving model to Regression_Model/hepg2.mle.linear-0250.ckpt\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5791 - mse: 0.5712 - val_loss: 0.5552 - val_mse: 0.5473\n",
      "Epoch 251/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5822 - mse: 0.5742 - val_loss: 0.5577 - val_mse: 0.5497\n",
      "Epoch 252/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5783 - mse: 0.5704 - val_loss: 0.5579 - val_mse: 0.5499\n",
      "Epoch 253/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5824 - mse: 0.5745 - val_loss: 0.5728 - val_mse: 0.5648\n",
      "Epoch 254/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5835 - mse: 0.5755 - val_loss: 0.5607 - val_mse: 0.5528\n",
      "Epoch 255/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5835 - mse: 0.5756 - val_loss: 0.5546 - val_mse: 0.5467\n",
      "Epoch 256/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5781 - mse: 0.5702 - val_loss: 0.5589 - val_mse: 0.5510\n",
      "Epoch 257/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5735 - mse: 0.5656 - val_loss: 0.5537 - val_mse: 0.5458\n",
      "Epoch 258/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5768 - mse: 0.5688 - val_loss: 0.5553 - val_mse: 0.5474\n",
      "Epoch 259/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5779 - mse: 0.5700 - val_loss: 0.5549 - val_mse: 0.5470\n",
      "Epoch 260/2000\n",
      "268/277 [============================>.] - ETA: 0s - loss: 0.5801 - mse: 0.5722\n",
      "Epoch 00260: saving model to Regression_Model/hepg2.mle.linear-0260.ckpt\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5806 - mse: 0.5727 - val_loss: 0.5561 - val_mse: 0.5482\n",
      "Epoch 261/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5748 - mse: 0.5669 - val_loss: 0.5545 - val_mse: 0.5466\n",
      "Epoch 262/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5858 - mse: 0.5779 - val_loss: 0.5574 - val_mse: 0.5495\n",
      "Epoch 263/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5822 - mse: 0.5743 - val_loss: 0.5651 - val_mse: 0.5572\n",
      "Epoch 264/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5816 - mse: 0.5737 - val_loss: 0.5590 - val_mse: 0.5511\n",
      "Epoch 265/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5762 - mse: 0.5683 - val_loss: 0.5642 - val_mse: 0.5563\n",
      "Epoch 266/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5797 - mse: 0.5718 - val_loss: 0.5645 - val_mse: 0.5566\n",
      "Epoch 267/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5894 - mse: 0.5815 - val_loss: 0.5583 - val_mse: 0.5504\n",
      "Epoch 268/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5798 - mse: 0.5719 - val_loss: 0.5631 - val_mse: 0.5552\n",
      "Epoch 269/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5787 - mse: 0.5709 - val_loss: 0.5609 - val_mse: 0.5530\n",
      "Epoch 270/2000\n",
      "274/277 [============================>.] - ETA: 0s - loss: 0.5848 - mse: 0.5769\n",
      "Epoch 00270: saving model to Regression_Model/hepg2.mle.linear-0270.ckpt\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5860 - mse: 0.5781 - val_loss: 0.5558 - val_mse: 0.5479\n",
      "Epoch 271/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5796 - mse: 0.5717 - val_loss: 0.5584 - val_mse: 0.5505\n",
      "Epoch 272/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5703 - mse: 0.5624 - val_loss: 0.5546 - val_mse: 0.5467\n",
      "Epoch 273/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5828 - mse: 0.5749 - val_loss: 0.5526 - val_mse: 0.5447\n",
      "Epoch 274/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5775 - mse: 0.5696 - val_loss: 0.5570 - val_mse: 0.5492\n",
      "Epoch 275/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5831 - mse: 0.5752 - val_loss: 0.5593 - val_mse: 0.5514\n",
      "Epoch 276/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5783 - mse: 0.5705 - val_loss: 0.5551 - val_mse: 0.5472\n",
      "Epoch 277/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5837 - mse: 0.5758 - val_loss: 0.5539 - val_mse: 0.5460\n",
      "Epoch 278/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5808 - mse: 0.5729 - val_loss: 0.5616 - val_mse: 0.5537\n",
      "Epoch 279/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5782 - mse: 0.5703 - val_loss: 0.5561 - val_mse: 0.5482\n",
      "Epoch 280/2000\n",
      "271/277 [============================>.] - ETA: 0s - loss: 0.5852 - mse: 0.5773\n",
      "Epoch 00280: saving model to Regression_Model/hepg2.mle.linear-0280.ckpt\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5836 - mse: 0.5758 - val_loss: 0.5565 - val_mse: 0.5486\n",
      "Epoch 281/2000\n",
      "277/277 [==============================] - 2s 5ms/step - loss: 0.5695 - mse: 0.5616 - val_loss: 0.5555 - val_mse: 0.5477\n",
      "Epoch 282/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5771 - mse: 0.5692 - val_loss: 0.5594 - val_mse: 0.5516\n",
      "Epoch 283/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5795 - mse: 0.5716 - val_loss: 0.5565 - val_mse: 0.5487\n",
      "Epoch 284/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5812 - mse: 0.5734 - val_loss: 0.5532 - val_mse: 0.5453\n",
      "Epoch 285/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5834 - mse: 0.5755 - val_loss: 0.5646 - val_mse: 0.5568\n",
      "Epoch 286/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5834 - mse: 0.5755 - val_loss: 0.5543 - val_mse: 0.5464\n",
      "Epoch 287/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5808 - mse: 0.5729 - val_loss: 0.5564 - val_mse: 0.5485\n",
      "Epoch 288/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5768 - mse: 0.5690 - val_loss: 0.5564 - val_mse: 0.5486\n",
      "Epoch 289/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5845 - mse: 0.5767 - val_loss: 0.5597 - val_mse: 0.5519\n",
      "Epoch 290/2000\n",
      "268/277 [============================>.] - ETA: 0s - loss: 0.5764 - mse: 0.5685\n",
      "Epoch 00290: saving model to Regression_Model/hepg2.mle.linear-0290.ckpt\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5769 - mse: 0.5690 - val_loss: 0.5616 - val_mse: 0.5538\n",
      "Epoch 291/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5805 - mse: 0.5726 - val_loss: 0.5543 - val_mse: 0.5465\n",
      "Epoch 292/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5853 - mse: 0.5775 - val_loss: 0.5615 - val_mse: 0.5536\n",
      "Epoch 293/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5772 - mse: 0.5694 - val_loss: 0.5575 - val_mse: 0.5497\n",
      "Epoch 294/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5784 - mse: 0.5706 - val_loss: 0.5589 - val_mse: 0.5510\n",
      "Epoch 295/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5763 - mse: 0.5685 - val_loss: 0.5671 - val_mse: 0.5593\n",
      "Epoch 296/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5721 - mse: 0.5643 - val_loss: 0.5600 - val_mse: 0.5522\n",
      "Epoch 297/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5794 - mse: 0.5716 - val_loss: 0.5638 - val_mse: 0.5560\n",
      "Epoch 298/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5753 - mse: 0.5675 - val_loss: 0.5552 - val_mse: 0.5474\n",
      "Epoch 299/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5735 - mse: 0.5657 - val_loss: 0.5580 - val_mse: 0.5502\n",
      "Epoch 300/2000\n",
      "273/277 [============================>.] - ETA: 0s - loss: 0.5763 - mse: 0.5685\n",
      "Epoch 00300: saving model to Regression_Model/hepg2.mle.linear-0300.ckpt\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5762 - mse: 0.5684 - val_loss: 0.5555 - val_mse: 0.5476\n",
      "Epoch 301/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5716 - mse: 0.5638 - val_loss: 0.5531 - val_mse: 0.5453\n",
      "Epoch 302/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5692 - mse: 0.5614 - val_loss: 0.5598 - val_mse: 0.5520\n",
      "Epoch 303/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5836 - mse: 0.5758 - val_loss: 0.5539 - val_mse: 0.5461\n",
      "Epoch 304/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5726 - mse: 0.5648 - val_loss: 0.5563 - val_mse: 0.5485\n",
      "Epoch 305/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5707 - mse: 0.5629 - val_loss: 0.5557 - val_mse: 0.5479\n",
      "Epoch 306/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5755 - mse: 0.5677 - val_loss: 0.5593 - val_mse: 0.5515\n",
      "Epoch 307/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5827 - mse: 0.5749 - val_loss: 0.5675 - val_mse: 0.5597\n",
      "Epoch 308/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5804 - mse: 0.5726 - val_loss: 0.5576 - val_mse: 0.5498\n",
      "Epoch 309/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5768 - mse: 0.5690 - val_loss: 0.5610 - val_mse: 0.5532\n",
      "Epoch 310/2000\n",
      "264/277 [===========================>..] - ETA: 0s - loss: 0.5799 - mse: 0.5721\n",
      "Epoch 00310: saving model to Regression_Model/hepg2.mle.linear-0310.ckpt\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5772 - mse: 0.5694 - val_loss: 0.5576 - val_mse: 0.5498\n",
      "Epoch 311/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5750 - mse: 0.5672 - val_loss: 0.5590 - val_mse: 0.5512\n",
      "Epoch 312/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5830 - mse: 0.5752 - val_loss: 0.5549 - val_mse: 0.5471\n",
      "Epoch 313/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5734 - mse: 0.5656 - val_loss: 0.5552 - val_mse: 0.5474\n",
      "Epoch 314/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5825 - mse: 0.5747 - val_loss: 0.5552 - val_mse: 0.5474\n",
      "Epoch 315/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5778 - mse: 0.5700 - val_loss: 0.5563 - val_mse: 0.5485\n",
      "Epoch 316/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5782 - mse: 0.5704 - val_loss: 0.5565 - val_mse: 0.5487\n",
      "Epoch 317/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5726 - mse: 0.5648 - val_loss: 0.5548 - val_mse: 0.5471\n",
      "Epoch 318/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5782 - mse: 0.5704 - val_loss: 0.5536 - val_mse: 0.5459\n",
      "Epoch 319/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5820 - mse: 0.5742 - val_loss: 0.5592 - val_mse: 0.5515\n",
      "Epoch 320/2000\n",
      "266/277 [===========================>..] - ETA: 0s - loss: 0.5730 - mse: 0.5653\n",
      "Epoch 00320: saving model to Regression_Model/hepg2.mle.linear-0320.ckpt\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5721 - mse: 0.5644 - val_loss: 0.5627 - val_mse: 0.5549\n",
      "Epoch 321/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5768 - mse: 0.5690 - val_loss: 0.5540 - val_mse: 0.5462\n",
      "Epoch 322/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5796 - mse: 0.5719 - val_loss: 0.5548 - val_mse: 0.5470\n",
      "Epoch 323/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5780 - mse: 0.5702 - val_loss: 0.5554 - val_mse: 0.5476\n",
      "Epoch 324/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5805 - mse: 0.5727 - val_loss: 0.5535 - val_mse: 0.5457\n",
      "Epoch 325/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5660 - mse: 0.5583 - val_loss: 0.5528 - val_mse: 0.5451\n",
      "Epoch 326/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5794 - mse: 0.5717 - val_loss: 0.5573 - val_mse: 0.5495\n",
      "Epoch 327/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5846 - mse: 0.5768 - val_loss: 0.5533 - val_mse: 0.5455\n",
      "Epoch 328/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5732 - mse: 0.5655 - val_loss: 0.5543 - val_mse: 0.5466\n",
      "Epoch 329/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5758 - mse: 0.5681 - val_loss: 0.5562 - val_mse: 0.5484\n",
      "Epoch 330/2000\n",
      "275/277 [============================>.] - ETA: 0s - loss: 0.5726 - mse: 0.5649\n",
      "Epoch 00330: saving model to Regression_Model/hepg2.mle.linear-0330.ckpt\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5735 - mse: 0.5657 - val_loss: 0.5637 - val_mse: 0.5559\n",
      "Epoch 331/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5774 - mse: 0.5697 - val_loss: 0.5621 - val_mse: 0.5544\n",
      "Epoch 332/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5747 - mse: 0.5669 - val_loss: 0.5567 - val_mse: 0.5490\n",
      "Epoch 333/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5849 - mse: 0.5772 - val_loss: 0.5546 - val_mse: 0.5468\n",
      "Epoch 334/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5773 - mse: 0.5696 - val_loss: 0.5552 - val_mse: 0.5475\n",
      "Epoch 335/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5782 - mse: 0.5704 - val_loss: 0.5594 - val_mse: 0.5517\n",
      "Epoch 336/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5703 - mse: 0.5626 - val_loss: 0.5600 - val_mse: 0.5523\n",
      "Epoch 337/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5786 - mse: 0.5709 - val_loss: 0.5604 - val_mse: 0.5527\n",
      "Epoch 338/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5721 - mse: 0.5643 - val_loss: 0.5558 - val_mse: 0.5481\n",
      "Epoch 339/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5784 - mse: 0.5706 - val_loss: 0.5596 - val_mse: 0.5519\n",
      "Epoch 340/2000\n",
      "272/277 [============================>.] - ETA: 0s - loss: 0.5744 - mse: 0.5667\n",
      "Epoch 00340: saving model to Regression_Model/hepg2.mle.linear-0340.ckpt\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5769 - mse: 0.5692 - val_loss: 0.5549 - val_mse: 0.5472\n",
      "Epoch 341/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5700 - mse: 0.5623 - val_loss: 0.5568 - val_mse: 0.5491\n",
      "Epoch 342/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5756 - mse: 0.5679 - val_loss: 0.5572 - val_mse: 0.5495\n",
      "Epoch 343/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5764 - mse: 0.5687 - val_loss: 0.5619 - val_mse: 0.5541\n",
      "Epoch 344/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5741 - mse: 0.5664 - val_loss: 0.5595 - val_mse: 0.5518\n",
      "Epoch 345/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5800 - mse: 0.5723 - val_loss: 0.5565 - val_mse: 0.5488\n",
      "Epoch 346/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5746 - mse: 0.5669 - val_loss: 0.5539 - val_mse: 0.5461\n",
      "Epoch 347/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5740 - mse: 0.5663 - val_loss: 0.5588 - val_mse: 0.5510\n",
      "Epoch 348/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5780 - mse: 0.5703 - val_loss: 0.5567 - val_mse: 0.5490\n",
      "Epoch 349/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5750 - mse: 0.5673 - val_loss: 0.5583 - val_mse: 0.5506\n",
      "Epoch 350/2000\n",
      "269/277 [============================>.] - ETA: 0s - loss: 0.5700 - mse: 0.5623\n",
      "Epoch 00350: saving model to Regression_Model/hepg2.mle.linear-0350.ckpt\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5733 - mse: 0.5656 - val_loss: 0.5590 - val_mse: 0.5512\n",
      "Epoch 351/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5719 - mse: 0.5642 - val_loss: 0.5586 - val_mse: 0.5509\n",
      "Epoch 352/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5808 - mse: 0.5731 - val_loss: 0.5698 - val_mse: 0.5621\n",
      "Epoch 353/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5784 - mse: 0.5707 - val_loss: 0.5547 - val_mse: 0.5470\n",
      "Epoch 354/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5700 - mse: 0.5623 - val_loss: 0.5533 - val_mse: 0.5456\n",
      "Epoch 355/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5770 - mse: 0.5693 - val_loss: 0.5664 - val_mse: 0.5587\n",
      "Epoch 356/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5779 - mse: 0.5702 - val_loss: 0.5578 - val_mse: 0.5501\n",
      "Epoch 357/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5759 - mse: 0.5683 - val_loss: 0.5567 - val_mse: 0.5490\n",
      "Epoch 358/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5750 - mse: 0.5673 - val_loss: 0.5581 - val_mse: 0.5504\n",
      "Epoch 359/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5719 - mse: 0.5642 - val_loss: 0.5583 - val_mse: 0.5506\n",
      "Epoch 360/2000\n",
      "265/277 [===========================>..] - ETA: 0s - loss: 0.5796 - mse: 0.5719\n",
      "Epoch 00360: saving model to Regression_Model/hepg2.mle.linear-0360.ckpt\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5784 - mse: 0.5707 - val_loss: 0.5595 - val_mse: 0.5518\n",
      "Epoch 361/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5832 - mse: 0.5755 - val_loss: 0.5587 - val_mse: 0.5510\n",
      "Epoch 362/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5727 - mse: 0.5650 - val_loss: 0.5624 - val_mse: 0.5547\n",
      "Epoch 363/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5710 - mse: 0.5633 - val_loss: 0.5578 - val_mse: 0.5501\n",
      "Epoch 364/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5731 - mse: 0.5655 - val_loss: 0.5563 - val_mse: 0.5486\n",
      "Epoch 365/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5758 - mse: 0.5681 - val_loss: 0.5551 - val_mse: 0.5474\n",
      "Epoch 366/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5781 - mse: 0.5704 - val_loss: 0.5645 - val_mse: 0.5568\n",
      "Epoch 367/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5741 - mse: 0.5664 - val_loss: 0.5553 - val_mse: 0.5477\n",
      "Epoch 368/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5783 - mse: 0.5707 - val_loss: 0.5548 - val_mse: 0.5471\n",
      "Epoch 369/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5752 - mse: 0.5675 - val_loss: 0.5548 - val_mse: 0.5471\n",
      "Epoch 370/2000\n",
      "264/277 [===========================>..] - ETA: 0s - loss: 0.5681 - mse: 0.5605\n",
      "Epoch 00370: saving model to Regression_Model/hepg2.mle.linear-0370.ckpt\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5671 - mse: 0.5595 - val_loss: 0.5524 - val_mse: 0.5448\n",
      "Epoch 371/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5693 - mse: 0.5617 - val_loss: 0.5589 - val_mse: 0.5513\n",
      "Epoch 372/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5699 - mse: 0.5622 - val_loss: 0.5576 - val_mse: 0.5500\n",
      "Epoch 373/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5814 - mse: 0.5738 - val_loss: 0.5535 - val_mse: 0.5459\n",
      "Epoch 374/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5695 - mse: 0.5619 - val_loss: 0.5525 - val_mse: 0.5449\n",
      "Epoch 375/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5773 - mse: 0.5696 - val_loss: 0.5633 - val_mse: 0.5556\n",
      "Epoch 376/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5716 - mse: 0.5639 - val_loss: 0.5591 - val_mse: 0.5515\n",
      "Epoch 377/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5798 - mse: 0.5722 - val_loss: 0.5576 - val_mse: 0.5499\n",
      "Epoch 378/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5760 - mse: 0.5683 - val_loss: 0.5556 - val_mse: 0.5479\n",
      "Epoch 379/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5807 - mse: 0.5730 - val_loss: 0.5626 - val_mse: 0.5550\n",
      "Epoch 380/2000\n",
      "269/277 [============================>.] - ETA: 0s - loss: 0.5843 - mse: 0.5767\n",
      "Epoch 00380: saving model to Regression_Model/hepg2.mle.linear-0380.ckpt\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5827 - mse: 0.5750 - val_loss: 0.5582 - val_mse: 0.5506\n",
      "Epoch 381/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5732 - mse: 0.5656 - val_loss: 0.5546 - val_mse: 0.5470\n",
      "Epoch 382/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5751 - mse: 0.5675 - val_loss: 0.5553 - val_mse: 0.5477\n",
      "Epoch 383/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5657 - mse: 0.5581 - val_loss: 0.5586 - val_mse: 0.5510\n",
      "Epoch 384/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5777 - mse: 0.5701 - val_loss: 0.5563 - val_mse: 0.5487\n",
      "Epoch 385/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5680 - mse: 0.5603 - val_loss: 0.5553 - val_mse: 0.5477\n",
      "Epoch 386/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5760 - mse: 0.5684 - val_loss: 0.5557 - val_mse: 0.5481\n",
      "Epoch 387/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5738 - mse: 0.5662 - val_loss: 0.5564 - val_mse: 0.5488\n",
      "Epoch 388/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5725 - mse: 0.5649 - val_loss: 0.5564 - val_mse: 0.5488\n",
      "Epoch 389/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5755 - mse: 0.5678 - val_loss: 0.5561 - val_mse: 0.5484\n",
      "Epoch 390/2000\n",
      "270/277 [============================>.] - ETA: 0s - loss: 0.5648 - mse: 0.5572\n",
      "Epoch 00390: saving model to Regression_Model/hepg2.mle.linear-0390.ckpt\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5666 - mse: 0.5590 - val_loss: 0.5546 - val_mse: 0.5470\n",
      "Epoch 391/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5734 - mse: 0.5658 - val_loss: 0.5609 - val_mse: 0.5533\n",
      "Epoch 392/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5713 - mse: 0.5637 - val_loss: 0.5540 - val_mse: 0.5464\n",
      "Epoch 393/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5733 - mse: 0.5657 - val_loss: 0.5578 - val_mse: 0.5502\n",
      "Epoch 394/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5671 - mse: 0.5595 - val_loss: 0.5539 - val_mse: 0.5463\n",
      "Epoch 395/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5814 - mse: 0.5738 - val_loss: 0.5550 - val_mse: 0.5474\n",
      "Epoch 396/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5701 - mse: 0.5625 - val_loss: 0.5551 - val_mse: 0.5475\n",
      "Epoch 397/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5654 - mse: 0.5578 - val_loss: 0.5528 - val_mse: 0.5452\n",
      "Epoch 398/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5751 - mse: 0.5675 - val_loss: 0.5539 - val_mse: 0.5463\n",
      "Epoch 399/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5777 - mse: 0.5701 - val_loss: 0.5549 - val_mse: 0.5473\n",
      "Epoch 400/2000\n",
      "276/277 [============================>.] - ETA: 0s - loss: 0.5718 - mse: 0.5642\n",
      "Epoch 00400: saving model to Regression_Model/hepg2.mle.linear-0400.ckpt\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5710 - mse: 0.5634 - val_loss: 0.5549 - val_mse: 0.5473\n",
      "Epoch 401/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5733 - mse: 0.5658 - val_loss: 0.5630 - val_mse: 0.5554\n",
      "Epoch 402/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5792 - mse: 0.5716 - val_loss: 0.5588 - val_mse: 0.5513\n",
      "Epoch 403/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5659 - mse: 0.5583 - val_loss: 0.5552 - val_mse: 0.5476\n",
      "Epoch 404/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5736 - mse: 0.5660 - val_loss: 0.5534 - val_mse: 0.5459\n",
      "Epoch 405/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5710 - mse: 0.5634 - val_loss: 0.5547 - val_mse: 0.5471\n",
      "Epoch 406/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5665 - mse: 0.5589 - val_loss: 0.5540 - val_mse: 0.5464\n",
      "Epoch 407/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5678 - mse: 0.5602 - val_loss: 0.5581 - val_mse: 0.5505\n",
      "Epoch 408/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5737 - mse: 0.5661 - val_loss: 0.5581 - val_mse: 0.5505\n",
      "Epoch 409/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5688 - mse: 0.5613 - val_loss: 0.5533 - val_mse: 0.5458\n",
      "Epoch 410/2000\n",
      "266/277 [===========================>..] - ETA: 0s - loss: 0.5773 - mse: 0.5698\n",
      "Epoch 00410: saving model to Regression_Model/hepg2.mle.linear-0410.ckpt\n",
      "277/277 [==============================] - 2s 7ms/step - loss: 0.5733 - mse: 0.5657 - val_loss: 0.5577 - val_mse: 0.5501\n",
      "Epoch 411/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5686 - mse: 0.5611 - val_loss: 0.5586 - val_mse: 0.5510\n",
      "Epoch 412/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5683 - mse: 0.5607 - val_loss: 0.5555 - val_mse: 0.5479\n",
      "Epoch 413/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5764 - mse: 0.5689 - val_loss: 0.5588 - val_mse: 0.5513\n",
      "Epoch 414/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5712 - mse: 0.5636 - val_loss: 0.5554 - val_mse: 0.5479\n",
      "Epoch 415/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5767 - mse: 0.5692 - val_loss: 0.5568 - val_mse: 0.5493\n",
      "Epoch 416/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5701 - mse: 0.5625 - val_loss: 0.5583 - val_mse: 0.5508\n",
      "Epoch 417/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5662 - mse: 0.5586 - val_loss: 0.5579 - val_mse: 0.5503\n",
      "Epoch 418/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5693 - mse: 0.5618 - val_loss: 0.5542 - val_mse: 0.5467\n",
      "Epoch 419/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5631 - mse: 0.5556 - val_loss: 0.5536 - val_mse: 0.5460\n",
      "Epoch 420/2000\n",
      "272/277 [============================>.] - ETA: 0s - loss: 0.5720 - mse: 0.5645\n",
      "Epoch 00420: saving model to Regression_Model/hepg2.mle.linear-0420.ckpt\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5740 - mse: 0.5665 - val_loss: 0.5563 - val_mse: 0.5488\n",
      "Epoch 421/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5670 - mse: 0.5594 - val_loss: 0.5564 - val_mse: 0.5489\n",
      "Epoch 422/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5764 - mse: 0.5688 - val_loss: 0.5662 - val_mse: 0.5586\n",
      "Epoch 423/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5737 - mse: 0.5661 - val_loss: 0.5572 - val_mse: 0.5496\n",
      "Epoch 424/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5729 - mse: 0.5654 - val_loss: 0.5580 - val_mse: 0.5505\n",
      "Epoch 425/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5721 - mse: 0.5646 - val_loss: 0.5554 - val_mse: 0.5479\n",
      "Epoch 426/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5688 - mse: 0.5613 - val_loss: 0.5559 - val_mse: 0.5483\n",
      "Epoch 427/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5744 - mse: 0.5669 - val_loss: 0.5574 - val_mse: 0.5499\n",
      "Epoch 428/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5690 - mse: 0.5615 - val_loss: 0.5536 - val_mse: 0.5460\n",
      "Epoch 429/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5709 - mse: 0.5634 - val_loss: 0.5518 - val_mse: 0.5443\n",
      "Epoch 430/2000\n",
      "274/277 [============================>.] - ETA: 0s - loss: 0.5643 - mse: 0.5567\n",
      "Epoch 00430: saving model to Regression_Model/hepg2.mle.linear-0430.ckpt\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5652 - mse: 0.5577 - val_loss: 0.5593 - val_mse: 0.5518\n",
      "Epoch 431/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5755 - mse: 0.5680 - val_loss: 0.5543 - val_mse: 0.5467\n",
      "Epoch 432/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5718 - mse: 0.5643 - val_loss: 0.5539 - val_mse: 0.5464\n",
      "Epoch 433/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5715 - mse: 0.5640 - val_loss: 0.5559 - val_mse: 0.5483\n",
      "Epoch 434/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5659 - mse: 0.5584 - val_loss: 0.5544 - val_mse: 0.5469\n",
      "Epoch 435/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5645 - mse: 0.5569 - val_loss: 0.5577 - val_mse: 0.5502\n",
      "Epoch 436/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5742 - mse: 0.5666 - val_loss: 0.5532 - val_mse: 0.5457\n",
      "Epoch 437/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5744 - mse: 0.5669 - val_loss: 0.5569 - val_mse: 0.5494\n",
      "Epoch 438/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5760 - mse: 0.5685 - val_loss: 0.5572 - val_mse: 0.5497\n",
      "Epoch 439/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5770 - mse: 0.5695 - val_loss: 0.5540 - val_mse: 0.5465\n",
      "Epoch 440/2000\n",
      "276/277 [============================>.] - ETA: 0s - loss: 0.5702 - mse: 0.5627\n",
      "Epoch 00440: saving model to Regression_Model/hepg2.mle.linear-0440.ckpt\n",
      "277/277 [==============================] - 2s 6ms/step - loss: 0.5696 - mse: 0.5621 - val_loss: 0.5570 - val_mse: 0.5495\n",
      "Epoch 441/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5629 - mse: 0.5554 - val_loss: 0.5577 - val_mse: 0.5502\n",
      "Epoch 442/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5725 - mse: 0.5650 - val_loss: 0.5562 - val_mse: 0.5487\n",
      "Epoch 443/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5759 - mse: 0.5684 - val_loss: 0.5580 - val_mse: 0.5505\n",
      "Epoch 444/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5731 - mse: 0.5656 - val_loss: 0.5568 - val_mse: 0.5493\n",
      "Epoch 445/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5757 - mse: 0.5682 - val_loss: 0.5585 - val_mse: 0.5510\n",
      "Epoch 446/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5721 - mse: 0.5646 - val_loss: 0.5580 - val_mse: 0.5506\n",
      "Epoch 447/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5683 - mse: 0.5608 - val_loss: 0.5565 - val_mse: 0.5491\n",
      "Epoch 448/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5742 - mse: 0.5667 - val_loss: 0.5599 - val_mse: 0.5525\n",
      "Epoch 449/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5756 - mse: 0.5681 - val_loss: 0.5563 - val_mse: 0.5488\n",
      "Epoch 450/2000\n",
      "271/277 [============================>.] - ETA: 0s - loss: 0.5797 - mse: 0.5722\n",
      "Epoch 00450: saving model to Regression_Model/hepg2.mle.linear-0450.ckpt\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5799 - mse: 0.5724 - val_loss: 0.5563 - val_mse: 0.5488\n",
      "Epoch 451/2000\n",
      "277/277 [==============================] - 2s 5ms/step - loss: 0.5728 - mse: 0.5653 - val_loss: 0.5610 - val_mse: 0.5535\n",
      "Epoch 452/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5610 - mse: 0.5535 - val_loss: 0.5552 - val_mse: 0.5477\n",
      "Epoch 453/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5723 - mse: 0.5648 - val_loss: 0.5581 - val_mse: 0.5506\n",
      "Epoch 454/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5747 - mse: 0.5672 - val_loss: 0.5559 - val_mse: 0.5485\n",
      "Epoch 455/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5693 - mse: 0.5619 - val_loss: 0.5606 - val_mse: 0.5531\n",
      "Epoch 456/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5681 - mse: 0.5606 - val_loss: 0.5588 - val_mse: 0.5513\n",
      "Epoch 457/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5706 - mse: 0.5631 - val_loss: 0.5641 - val_mse: 0.5566\n",
      "Epoch 458/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5714 - mse: 0.5639 - val_loss: 0.5571 - val_mse: 0.5496\n",
      "Epoch 459/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5713 - mse: 0.5638 - val_loss: 0.5570 - val_mse: 0.5495\n",
      "Epoch 460/2000\n",
      "267/277 [===========================>..] - ETA: 0s - loss: 0.5683 - mse: 0.5608\n",
      "Epoch 00460: saving model to Regression_Model/hepg2.mle.linear-0460.ckpt\n",
      "277/277 [==============================] - 2s 6ms/step - loss: 0.5683 - mse: 0.5608 - val_loss: 0.5535 - val_mse: 0.5461\n",
      "Epoch 461/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5733 - mse: 0.5659 - val_loss: 0.5653 - val_mse: 0.5579\n",
      "Epoch 462/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5736 - mse: 0.5662 - val_loss: 0.5588 - val_mse: 0.5514\n",
      "Epoch 463/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5693 - mse: 0.5618 - val_loss: 0.5535 - val_mse: 0.5460\n",
      "Epoch 464/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5714 - mse: 0.5640 - val_loss: 0.5571 - val_mse: 0.5496\n",
      "Epoch 465/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5684 - mse: 0.5610 - val_loss: 0.5551 - val_mse: 0.5476\n",
      "Epoch 466/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5702 - mse: 0.5628 - val_loss: 0.5595 - val_mse: 0.5520\n",
      "Epoch 467/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5626 - mse: 0.5552 - val_loss: 0.5631 - val_mse: 0.5556\n",
      "Epoch 468/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5796 - mse: 0.5721 - val_loss: 0.5567 - val_mse: 0.5492\n",
      "Epoch 469/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5672 - mse: 0.5597 - val_loss: 0.5584 - val_mse: 0.5510\n",
      "Epoch 470/2000\n",
      "271/277 [============================>.] - ETA: 0s - loss: 0.5736 - mse: 0.5661\n",
      "Epoch 00470: saving model to Regression_Model/hepg2.mle.linear-0470.ckpt\n",
      "277/277 [==============================] - 2s 7ms/step - loss: 0.5742 - mse: 0.5668 - val_loss: 0.5558 - val_mse: 0.5484\n",
      "Epoch 471/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5648 - mse: 0.5574 - val_loss: 0.5580 - val_mse: 0.5506\n",
      "Epoch 472/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5780 - mse: 0.5706 - val_loss: 0.5537 - val_mse: 0.5463\n",
      "Epoch 473/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5659 - mse: 0.5584 - val_loss: 0.5570 - val_mse: 0.5495\n",
      "Epoch 474/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5694 - mse: 0.5619 - val_loss: 0.5565 - val_mse: 0.5491\n",
      "Epoch 475/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5700 - mse: 0.5626 - val_loss: 0.5564 - val_mse: 0.5489\n",
      "Epoch 476/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5702 - mse: 0.5628 - val_loss: 0.5538 - val_mse: 0.5464\n",
      "Epoch 477/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5761 - mse: 0.5686 - val_loss: 0.5583 - val_mse: 0.5509\n",
      "Epoch 478/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5641 - mse: 0.5567 - val_loss: 0.5556 - val_mse: 0.5482\n",
      "Epoch 479/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5659 - mse: 0.5585 - val_loss: 0.5621 - val_mse: 0.5547\n",
      "Epoch 480/2000\n",
      "276/277 [============================>.] - ETA: 0s - loss: 0.5655 - mse: 0.5581\n",
      "Epoch 00480: saving model to Regression_Model/hepg2.mle.linear-0480.ckpt\n",
      "277/277 [==============================] - 2s 6ms/step - loss: 0.5668 - mse: 0.5594 - val_loss: 0.5536 - val_mse: 0.5462\n",
      "Epoch 481/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5664 - mse: 0.5590 - val_loss: 0.5555 - val_mse: 0.5481\n",
      "Epoch 482/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5585 - mse: 0.5511 - val_loss: 0.5566 - val_mse: 0.5492\n",
      "Epoch 483/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5697 - mse: 0.5623 - val_loss: 0.5560 - val_mse: 0.5486\n",
      "Epoch 484/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5636 - mse: 0.5562 - val_loss: 0.5612 - val_mse: 0.5537\n",
      "Epoch 485/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5694 - mse: 0.5620 - val_loss: 0.5548 - val_mse: 0.5474\n",
      "Epoch 486/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5667 - mse: 0.5593 - val_loss: 0.5573 - val_mse: 0.5498\n",
      "Epoch 487/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5710 - mse: 0.5636 - val_loss: 0.5602 - val_mse: 0.5528\n",
      "Epoch 488/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5679 - mse: 0.5605 - val_loss: 0.5573 - val_mse: 0.5499\n",
      "Epoch 489/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5689 - mse: 0.5615 - val_loss: 0.5571 - val_mse: 0.5497\n",
      "Epoch 490/2000\n",
      "275/277 [============================>.] - ETA: 0s - loss: 0.5684 - mse: 0.5610\n",
      "Epoch 00490: saving model to Regression_Model/hepg2.mle.linear-0490.ckpt\n",
      "277/277 [==============================] - 2s 7ms/step - loss: 0.5671 - mse: 0.5597 - val_loss: 0.5539 - val_mse: 0.5465\n",
      "Epoch 491/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5702 - mse: 0.5628 - val_loss: 0.5561 - val_mse: 0.5487\n",
      "Epoch 492/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5664 - mse: 0.5590 - val_loss: 0.5606 - val_mse: 0.5532\n",
      "Epoch 493/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5707 - mse: 0.5633 - val_loss: 0.5578 - val_mse: 0.5504\n",
      "Epoch 494/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5716 - mse: 0.5642 - val_loss: 0.5650 - val_mse: 0.5576\n",
      "Epoch 495/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5675 - mse: 0.5602 - val_loss: 0.5558 - val_mse: 0.5484\n",
      "Epoch 496/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5732 - mse: 0.5658 - val_loss: 0.5588 - val_mse: 0.5514\n",
      "Epoch 497/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5725 - mse: 0.5651 - val_loss: 0.5573 - val_mse: 0.5499\n",
      "Epoch 498/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5706 - mse: 0.5632 - val_loss: 0.5535 - val_mse: 0.5461\n",
      "Epoch 499/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5726 - mse: 0.5652 - val_loss: 0.5568 - val_mse: 0.5494\n",
      "Epoch 500/2000\n",
      "267/277 [===========================>..] - ETA: 0s - loss: 0.5696 - mse: 0.5622\n",
      "Epoch 00500: saving model to Regression_Model/hepg2.mle.linear-0500.ckpt\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5669 - mse: 0.5595 - val_loss: 0.5565 - val_mse: 0.5491\n",
      "Epoch 501/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5702 - mse: 0.5628 - val_loss: 0.5565 - val_mse: 0.5491\n",
      "Epoch 502/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5661 - mse: 0.5587 - val_loss: 0.5581 - val_mse: 0.5508\n",
      "Epoch 503/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5655 - mse: 0.5581 - val_loss: 0.5587 - val_mse: 0.5513\n",
      "Epoch 504/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5599 - mse: 0.5525 - val_loss: 0.5539 - val_mse: 0.5465\n",
      "Epoch 505/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5721 - mse: 0.5648 - val_loss: 0.5616 - val_mse: 0.5542\n",
      "Epoch 506/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5721 - mse: 0.5647 - val_loss: 0.5584 - val_mse: 0.5510\n",
      "Epoch 507/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5688 - mse: 0.5614 - val_loss: 0.5565 - val_mse: 0.5492\n",
      "Epoch 508/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5660 - mse: 0.5586 - val_loss: 0.5542 - val_mse: 0.5468\n",
      "Epoch 509/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5638 - mse: 0.5564 - val_loss: 0.5564 - val_mse: 0.5490\n",
      "Epoch 510/2000\n",
      "264/277 [===========================>..] - ETA: 0s - loss: 0.5612 - mse: 0.5539\n",
      "Epoch 00510: saving model to Regression_Model/hepg2.mle.linear-0510.ckpt\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5622 - mse: 0.5548 - val_loss: 0.5547 - val_mse: 0.5474\n",
      "Epoch 511/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5724 - mse: 0.5651 - val_loss: 0.5553 - val_mse: 0.5479\n",
      "Epoch 512/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5683 - mse: 0.5609 - val_loss: 0.5546 - val_mse: 0.5472\n",
      "Epoch 513/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5741 - mse: 0.5668 - val_loss: 0.5555 - val_mse: 0.5481\n",
      "Epoch 514/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5723 - mse: 0.5650 - val_loss: 0.5569 - val_mse: 0.5495\n",
      "Epoch 515/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5656 - mse: 0.5582 - val_loss: 0.5568 - val_mse: 0.5495\n",
      "Epoch 516/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5605 - mse: 0.5532 - val_loss: 0.5561 - val_mse: 0.5487\n",
      "Epoch 517/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5688 - mse: 0.5615 - val_loss: 0.5617 - val_mse: 0.5544\n",
      "Epoch 518/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5735 - mse: 0.5662 - val_loss: 0.5564 - val_mse: 0.5491\n",
      "Epoch 519/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5696 - mse: 0.5623 - val_loss: 0.5562 - val_mse: 0.5489\n",
      "Epoch 520/2000\n",
      "276/277 [============================>.] - ETA: 0s - loss: 0.5638 - mse: 0.5564\n",
      "Epoch 00520: saving model to Regression_Model/hepg2.mle.linear-0520.ckpt\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5643 - mse: 0.5570 - val_loss: 0.5571 - val_mse: 0.5498\n",
      "Epoch 521/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5654 - mse: 0.5580 - val_loss: 0.5531 - val_mse: 0.5457\n",
      "Epoch 522/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5696 - mse: 0.5623 - val_loss: 0.5527 - val_mse: 0.5454\n",
      "Epoch 523/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5642 - mse: 0.5569 - val_loss: 0.5555 - val_mse: 0.5482\n",
      "Epoch 524/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5612 - mse: 0.5539 - val_loss: 0.5581 - val_mse: 0.5507\n",
      "Epoch 525/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5754 - mse: 0.5681 - val_loss: 0.5580 - val_mse: 0.5507\n",
      "Epoch 526/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5667 - mse: 0.5594 - val_loss: 0.5569 - val_mse: 0.5495\n",
      "Epoch 527/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5634 - mse: 0.5561 - val_loss: 0.5534 - val_mse: 0.5461\n",
      "Epoch 528/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5655 - mse: 0.5582 - val_loss: 0.5548 - val_mse: 0.5475\n",
      "Epoch 529/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5616 - mse: 0.5543 - val_loss: 0.5553 - val_mse: 0.5480\n",
      "Epoch 530/2000\n",
      "267/277 [===========================>..] - ETA: 0s - loss: 0.5745 - mse: 0.5672\n",
      "Epoch 00530: saving model to Regression_Model/hepg2.mle.linear-0530.ckpt\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5762 - mse: 0.5688 - val_loss: 0.5556 - val_mse: 0.5483\n",
      "Epoch 531/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5666 - mse: 0.5593 - val_loss: 0.5574 - val_mse: 0.5501\n",
      "Epoch 532/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5672 - mse: 0.5599 - val_loss: 0.5594 - val_mse: 0.5520\n",
      "Epoch 533/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5675 - mse: 0.5602 - val_loss: 0.5595 - val_mse: 0.5522\n",
      "Epoch 534/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5695 - mse: 0.5621 - val_loss: 0.5579 - val_mse: 0.5506\n",
      "Epoch 535/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5678 - mse: 0.5605 - val_loss: 0.5546 - val_mse: 0.5473\n",
      "Epoch 536/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5632 - mse: 0.5558 - val_loss: 0.5631 - val_mse: 0.5557\n",
      "Epoch 537/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5772 - mse: 0.5699 - val_loss: 0.5615 - val_mse: 0.5542\n",
      "Epoch 538/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5649 - mse: 0.5575 - val_loss: 0.5562 - val_mse: 0.5489\n",
      "Epoch 539/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5651 - mse: 0.5578 - val_loss: 0.5607 - val_mse: 0.5534\n",
      "Epoch 540/2000\n",
      "270/277 [============================>.] - ETA: 0s - loss: 0.5641 - mse: 0.5567\n",
      "Epoch 00540: saving model to Regression_Model/hepg2.mle.linear-0540.ckpt\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5668 - mse: 0.5595 - val_loss: 0.5556 - val_mse: 0.5483\n",
      "Epoch 541/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5713 - mse: 0.5640 - val_loss: 0.5589 - val_mse: 0.5516\n",
      "Epoch 542/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5682 - mse: 0.5609 - val_loss: 0.5556 - val_mse: 0.5483\n",
      "Epoch 543/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5700 - mse: 0.5627 - val_loss: 0.5538 - val_mse: 0.5465\n",
      "Epoch 544/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5659 - mse: 0.5586 - val_loss: 0.5588 - val_mse: 0.5515\n",
      "Epoch 545/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5621 - mse: 0.5548 - val_loss: 0.5575 - val_mse: 0.5502\n",
      "Epoch 546/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5676 - mse: 0.5603 - val_loss: 0.5571 - val_mse: 0.5498\n",
      "Epoch 547/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5718 - mse: 0.5646 - val_loss: 0.5568 - val_mse: 0.5495\n",
      "Epoch 548/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5629 - mse: 0.5556 - val_loss: 0.5609 - val_mse: 0.5536\n",
      "Epoch 549/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5745 - mse: 0.5672 - val_loss: 0.5587 - val_mse: 0.5515\n",
      "Epoch 550/2000\n",
      "268/277 [============================>.] - ETA: 0s - loss: 0.5702 - mse: 0.5630\n",
      "Epoch 00550: saving model to Regression_Model/hepg2.mle.linear-0550.ckpt\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5711 - mse: 0.5638 - val_loss: 0.5548 - val_mse: 0.5475\n",
      "Epoch 551/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5644 - mse: 0.5571 - val_loss: 0.5593 - val_mse: 0.5520\n",
      "Epoch 552/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5693 - mse: 0.5620 - val_loss: 0.5573 - val_mse: 0.5500\n",
      "Epoch 553/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5685 - mse: 0.5612 - val_loss: 0.5562 - val_mse: 0.5490\n",
      "Epoch 554/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5638 - mse: 0.5565 - val_loss: 0.5543 - val_mse: 0.5470\n",
      "Epoch 555/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5710 - mse: 0.5637 - val_loss: 0.5554 - val_mse: 0.5481\n",
      "Epoch 556/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5679 - mse: 0.5606 - val_loss: 0.5533 - val_mse: 0.5460\n",
      "Epoch 557/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5662 - mse: 0.5589 - val_loss: 0.5549 - val_mse: 0.5476\n",
      "Epoch 558/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5667 - mse: 0.5594 - val_loss: 0.5561 - val_mse: 0.5488\n",
      "Epoch 559/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5645 - mse: 0.5572 - val_loss: 0.5534 - val_mse: 0.5462\n",
      "Epoch 560/2000\n",
      "269/277 [============================>.] - ETA: 0s - loss: 0.5666 - mse: 0.5593\n",
      "Epoch 00560: saving model to Regression_Model/hepg2.mle.linear-0560.ckpt\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5658 - mse: 0.5585 - val_loss: 0.5558 - val_mse: 0.5485\n",
      "Epoch 561/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5762 - mse: 0.5689 - val_loss: 0.5564 - val_mse: 0.5491\n",
      "Epoch 562/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5692 - mse: 0.5619 - val_loss: 0.5548 - val_mse: 0.5476\n",
      "Epoch 563/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5737 - mse: 0.5664 - val_loss: 0.5559 - val_mse: 0.5487\n",
      "Epoch 564/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5616 - mse: 0.5544 - val_loss: 0.5537 - val_mse: 0.5464\n",
      "Epoch 565/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5678 - mse: 0.5605 - val_loss: 0.5540 - val_mse: 0.5467\n",
      "Epoch 566/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5698 - mse: 0.5626 - val_loss: 0.5577 - val_mse: 0.5505\n",
      "Epoch 567/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5663 - mse: 0.5591 - val_loss: 0.5590 - val_mse: 0.5517\n",
      "Epoch 568/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5663 - mse: 0.5590 - val_loss: 0.5542 - val_mse: 0.5469\n",
      "Epoch 569/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5660 - mse: 0.5588 - val_loss: 0.5536 - val_mse: 0.5464\n",
      "Epoch 570/2000\n",
      "270/277 [============================>.] - ETA: 0s - loss: 0.5671 - mse: 0.5599\n",
      "Epoch 00570: saving model to Regression_Model/hepg2.mle.linear-0570.ckpt\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5666 - mse: 0.5594 - val_loss: 0.5537 - val_mse: 0.5464\n",
      "Epoch 571/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5622 - mse: 0.5550 - val_loss: 0.5556 - val_mse: 0.5483\n",
      "Epoch 572/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5675 - mse: 0.5602 - val_loss: 0.5525 - val_mse: 0.5452\n",
      "Epoch 573/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5665 - mse: 0.5593 - val_loss: 0.5548 - val_mse: 0.5476\n",
      "Epoch 574/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5600 - mse: 0.5527 - val_loss: 0.5579 - val_mse: 0.5506\n",
      "Epoch 575/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5593 - mse: 0.5521 - val_loss: 0.5547 - val_mse: 0.5474\n",
      "Epoch 576/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5635 - mse: 0.5563 - val_loss: 0.5555 - val_mse: 0.5483\n",
      "Epoch 577/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5649 - mse: 0.5576 - val_loss: 0.5544 - val_mse: 0.5471\n",
      "Epoch 578/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5631 - mse: 0.5559 - val_loss: 0.5606 - val_mse: 0.5534\n",
      "Epoch 579/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5607 - mse: 0.5534 - val_loss: 0.5589 - val_mse: 0.5517\n",
      "Epoch 580/2000\n",
      "273/277 [============================>.] - ETA: 0s - loss: 0.5698 - mse: 0.5626\n",
      "Epoch 00580: saving model to Regression_Model/hepg2.mle.linear-0580.ckpt\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5699 - mse: 0.5627 - val_loss: 0.5605 - val_mse: 0.5532\n",
      "Epoch 581/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5647 - mse: 0.5575 - val_loss: 0.5559 - val_mse: 0.5486\n",
      "Epoch 582/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5644 - mse: 0.5571 - val_loss: 0.5573 - val_mse: 0.5500\n",
      "Epoch 583/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5593 - mse: 0.5521 - val_loss: 0.5555 - val_mse: 0.5483\n",
      "Epoch 584/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5678 - mse: 0.5606 - val_loss: 0.5541 - val_mse: 0.5469\n",
      "Epoch 585/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5654 - mse: 0.5582 - val_loss: 0.5553 - val_mse: 0.5481\n",
      "Epoch 586/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5607 - mse: 0.5535 - val_loss: 0.5554 - val_mse: 0.5482\n",
      "Epoch 587/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5624 - mse: 0.5551 - val_loss: 0.5632 - val_mse: 0.5560\n",
      "Epoch 588/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5671 - mse: 0.5599 - val_loss: 0.5554 - val_mse: 0.5481\n",
      "Epoch 589/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5624 - mse: 0.5552 - val_loss: 0.5553 - val_mse: 0.5481\n",
      "Epoch 590/2000\n",
      "272/277 [============================>.] - ETA: 0s - loss: 0.5702 - mse: 0.5630\n",
      "Epoch 00590: saving model to Regression_Model/hepg2.mle.linear-0590.ckpt\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5703 - mse: 0.5631 - val_loss: 0.5583 - val_mse: 0.5511\n",
      "Epoch 591/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5628 - mse: 0.5556 - val_loss: 0.5553 - val_mse: 0.5481\n",
      "Epoch 592/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5603 - mse: 0.5531 - val_loss: 0.5567 - val_mse: 0.5495\n",
      "Epoch 593/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5673 - mse: 0.5601 - val_loss: 0.5611 - val_mse: 0.5539\n",
      "Epoch 594/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5629 - mse: 0.5557 - val_loss: 0.5541 - val_mse: 0.5468\n",
      "Epoch 595/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5672 - mse: 0.5600 - val_loss: 0.5593 - val_mse: 0.5521\n",
      "Epoch 596/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5693 - mse: 0.5621 - val_loss: 0.5607 - val_mse: 0.5535\n",
      "Epoch 597/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5599 - mse: 0.5526 - val_loss: 0.5538 - val_mse: 0.5466\n",
      "Epoch 598/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5617 - mse: 0.5545 - val_loss: 0.5575 - val_mse: 0.5503\n",
      "Epoch 599/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5724 - mse: 0.5652 - val_loss: 0.5580 - val_mse: 0.5508\n",
      "Epoch 600/2000\n",
      "273/277 [============================>.] - ETA: 0s - loss: 0.5572 - mse: 0.5500\n",
      "Epoch 00600: saving model to Regression_Model/hepg2.mle.linear-0600.ckpt\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5580 - mse: 0.5508 - val_loss: 0.5620 - val_mse: 0.5548\n",
      "Epoch 601/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5623 - mse: 0.5551 - val_loss: 0.5575 - val_mse: 0.5503\n",
      "Epoch 602/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5659 - mse: 0.5587 - val_loss: 0.5600 - val_mse: 0.5528\n",
      "Epoch 603/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5622 - mse: 0.5550 - val_loss: 0.5589 - val_mse: 0.5517\n",
      "Epoch 604/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5674 - mse: 0.5602 - val_loss: 0.5573 - val_mse: 0.5501\n",
      "Epoch 605/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5632 - mse: 0.5560 - val_loss: 0.5568 - val_mse: 0.5496\n",
      "Epoch 606/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5625 - mse: 0.5553 - val_loss: 0.5578 - val_mse: 0.5506\n",
      "Epoch 607/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5641 - mse: 0.5569 - val_loss: 0.5541 - val_mse: 0.5469\n",
      "Epoch 608/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5671 - mse: 0.5599 - val_loss: 0.5601 - val_mse: 0.5529\n",
      "Epoch 609/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5633 - mse: 0.5561 - val_loss: 0.5572 - val_mse: 0.5501\n",
      "Epoch 610/2000\n",
      "274/277 [============================>.] - ETA: 0s - loss: 0.5642 - mse: 0.5570\n",
      "Epoch 00610: saving model to Regression_Model/hepg2.mle.linear-0610.ckpt\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5626 - mse: 0.5554 - val_loss: 0.5577 - val_mse: 0.5505\n",
      "Epoch 611/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5654 - mse: 0.5582 - val_loss: 0.5573 - val_mse: 0.5501\n",
      "Epoch 612/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5676 - mse: 0.5604 - val_loss: 0.5555 - val_mse: 0.5483\n",
      "Epoch 613/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5640 - mse: 0.5568 - val_loss: 0.5573 - val_mse: 0.5502\n",
      "Epoch 614/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5717 - mse: 0.5645 - val_loss: 0.5565 - val_mse: 0.5493\n",
      "Epoch 615/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5638 - mse: 0.5566 - val_loss: 0.5571 - val_mse: 0.5499\n",
      "Epoch 616/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5637 - mse: 0.5565 - val_loss: 0.5555 - val_mse: 0.5484\n",
      "Epoch 617/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5670 - mse: 0.5599 - val_loss: 0.5525 - val_mse: 0.5453\n",
      "Epoch 618/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5641 - mse: 0.5570 - val_loss: 0.5551 - val_mse: 0.5480\n",
      "Epoch 619/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5755 - mse: 0.5683 - val_loss: 0.5567 - val_mse: 0.5496\n",
      "Epoch 620/2000\n",
      "265/277 [===========================>..] - ETA: 0s - loss: 0.5786 - mse: 0.5714\n",
      "Epoch 00620: saving model to Regression_Model/hepg2.mle.linear-0620.ckpt\n",
      "277/277 [==============================] - 2s 5ms/step - loss: 0.5775 - mse: 0.5703 - val_loss: 0.5570 - val_mse: 0.5498\n",
      "Epoch 621/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5594 - mse: 0.5522 - val_loss: 0.5588 - val_mse: 0.5517\n",
      "Epoch 622/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5696 - mse: 0.5625 - val_loss: 0.5558 - val_mse: 0.5487\n",
      "Epoch 623/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5670 - mse: 0.5598 - val_loss: 0.5570 - val_mse: 0.5499\n",
      "Epoch 624/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5612 - mse: 0.5540 - val_loss: 0.5543 - val_mse: 0.5471\n",
      "Epoch 625/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5648 - mse: 0.5576 - val_loss: 0.5557 - val_mse: 0.5485\n",
      "Epoch 626/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5712 - mse: 0.5641 - val_loss: 0.5575 - val_mse: 0.5503\n",
      "Epoch 627/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5633 - mse: 0.5562 - val_loss: 0.5564 - val_mse: 0.5493\n",
      "Epoch 628/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5629 - mse: 0.5557 - val_loss: 0.5562 - val_mse: 0.5490\n",
      "Epoch 629/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5655 - mse: 0.5584 - val_loss: 0.5568 - val_mse: 0.5496\n",
      "Epoch 630/2000\n",
      "271/277 [============================>.] - ETA: 0s - loss: 0.5657 - mse: 0.5585\n",
      "Epoch 00630: saving model to Regression_Model/hepg2.mle.linear-0630.ckpt\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5647 - mse: 0.5576 - val_loss: 0.5557 - val_mse: 0.5485\n",
      "Epoch 631/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5666 - mse: 0.5594 - val_loss: 0.5559 - val_mse: 0.5488\n",
      "Epoch 632/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5738 - mse: 0.5666 - val_loss: 0.5555 - val_mse: 0.5484\n",
      "Epoch 633/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5689 - mse: 0.5617 - val_loss: 0.5556 - val_mse: 0.5484\n",
      "Epoch 634/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5660 - mse: 0.5589 - val_loss: 0.5613 - val_mse: 0.5542\n",
      "Epoch 635/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5617 - mse: 0.5546 - val_loss: 0.5571 - val_mse: 0.5500\n",
      "Epoch 636/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5716 - mse: 0.5644 - val_loss: 0.5548 - val_mse: 0.5477\n",
      "Epoch 637/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5606 - mse: 0.5535 - val_loss: 0.5589 - val_mse: 0.5518\n",
      "Epoch 638/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5634 - mse: 0.5563 - val_loss: 0.5563 - val_mse: 0.5492\n",
      "Epoch 639/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5586 - mse: 0.5515 - val_loss: 0.5547 - val_mse: 0.5475\n",
      "Epoch 640/2000\n",
      "271/277 [============================>.] - ETA: 0s - loss: 0.5611 - mse: 0.5539\n",
      "Epoch 00640: saving model to Regression_Model/hepg2.mle.linear-0640.ckpt\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5625 - mse: 0.5554 - val_loss: 0.5567 - val_mse: 0.5496\n",
      "Epoch 641/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5684 - mse: 0.5613 - val_loss: 0.5560 - val_mse: 0.5488\n",
      "Epoch 642/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5662 - mse: 0.5591 - val_loss: 0.5552 - val_mse: 0.5480\n",
      "Epoch 643/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5648 - mse: 0.5577 - val_loss: 0.5581 - val_mse: 0.5510\n",
      "Epoch 644/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5683 - mse: 0.5611 - val_loss: 0.5582 - val_mse: 0.5511\n",
      "Epoch 645/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5685 - mse: 0.5614 - val_loss: 0.5565 - val_mse: 0.5493\n",
      "Epoch 646/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5643 - mse: 0.5572 - val_loss: 0.5558 - val_mse: 0.5486\n",
      "Epoch 647/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5646 - mse: 0.5575 - val_loss: 0.5535 - val_mse: 0.5464\n",
      "Epoch 648/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5607 - mse: 0.5536 - val_loss: 0.5551 - val_mse: 0.5479\n",
      "Epoch 649/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5714 - mse: 0.5643 - val_loss: 0.5554 - val_mse: 0.5482\n",
      "Epoch 650/2000\n",
      "272/277 [============================>.] - ETA: 0s - loss: 0.5626 - mse: 0.5555\n",
      "Epoch 00650: saving model to Regression_Model/hepg2.mle.linear-0650.ckpt\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5602 - mse: 0.5531 - val_loss: 0.5540 - val_mse: 0.5469\n",
      "Epoch 651/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5589 - mse: 0.5518 - val_loss: 0.5589 - val_mse: 0.5518\n",
      "Epoch 652/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5637 - mse: 0.5566 - val_loss: 0.5557 - val_mse: 0.5486\n",
      "Epoch 653/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5620 - mse: 0.5549 - val_loss: 0.5532 - val_mse: 0.5461\n",
      "Epoch 654/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5630 - mse: 0.5559 - val_loss: 0.5550 - val_mse: 0.5479\n",
      "Epoch 655/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5637 - mse: 0.5566 - val_loss: 0.5554 - val_mse: 0.5483\n",
      "Epoch 656/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5695 - mse: 0.5624 - val_loss: 0.5563 - val_mse: 0.5491\n",
      "Epoch 657/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5660 - mse: 0.5589 - val_loss: 0.5545 - val_mse: 0.5474\n",
      "Epoch 658/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5635 - mse: 0.5564 - val_loss: 0.5572 - val_mse: 0.5501\n",
      "Epoch 659/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5626 - mse: 0.5555 - val_loss: 0.5591 - val_mse: 0.5520\n",
      "Epoch 660/2000\n",
      "270/277 [============================>.] - ETA: 0s - loss: 0.5613 - mse: 0.5542\n",
      "Epoch 00660: saving model to Regression_Model/hepg2.mle.linear-0660.ckpt\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5620 - mse: 0.5549 - val_loss: 0.5559 - val_mse: 0.5488\n",
      "Epoch 661/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5647 - mse: 0.5576 - val_loss: 0.5541 - val_mse: 0.5470\n",
      "Epoch 662/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5638 - mse: 0.5567 - val_loss: 0.5560 - val_mse: 0.5489\n",
      "Epoch 663/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5585 - mse: 0.5514 - val_loss: 0.5557 - val_mse: 0.5486\n",
      "Epoch 664/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5528 - mse: 0.5457 - val_loss: 0.5593 - val_mse: 0.5522\n",
      "Epoch 665/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5694 - mse: 0.5623 - val_loss: 0.5587 - val_mse: 0.5516\n",
      "Epoch 666/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5654 - mse: 0.5583 - val_loss: 0.5539 - val_mse: 0.5468\n",
      "Epoch 667/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5586 - mse: 0.5515 - val_loss: 0.5619 - val_mse: 0.5548\n",
      "Epoch 668/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5667 - mse: 0.5596 - val_loss: 0.5543 - val_mse: 0.5472\n",
      "Epoch 669/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5567 - mse: 0.5496 - val_loss: 0.5602 - val_mse: 0.5531\n",
      "Epoch 670/2000\n",
      "270/277 [============================>.] - ETA: 0s - loss: 0.5644 - mse: 0.5573\n",
      "Epoch 00670: saving model to Regression_Model/hepg2.mle.linear-0670.ckpt\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5632 - mse: 0.5561 - val_loss: 0.5544 - val_mse: 0.5474\n",
      "Epoch 671/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5656 - mse: 0.5585 - val_loss: 0.5554 - val_mse: 0.5483\n",
      "Epoch 672/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5612 - mse: 0.5541 - val_loss: 0.5580 - val_mse: 0.5509\n",
      "Epoch 673/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5607 - mse: 0.5536 - val_loss: 0.5535 - val_mse: 0.5464\n",
      "Epoch 674/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5671 - mse: 0.5600 - val_loss: 0.5556 - val_mse: 0.5485\n",
      "Epoch 675/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5611 - mse: 0.5540 - val_loss: 0.5567 - val_mse: 0.5497\n",
      "Epoch 676/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5573 - mse: 0.5502 - val_loss: 0.5545 - val_mse: 0.5474\n",
      "Epoch 677/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5627 - mse: 0.5556 - val_loss: 0.5554 - val_mse: 0.5483\n",
      "Epoch 678/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5613 - mse: 0.5542 - val_loss: 0.5559 - val_mse: 0.5488\n",
      "Epoch 679/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5594 - mse: 0.5523 - val_loss: 0.5554 - val_mse: 0.5483\n",
      "Epoch 680/2000\n",
      "276/277 [============================>.] - ETA: 0s - loss: 0.5705 - mse: 0.5634\n",
      "Epoch 00680: saving model to Regression_Model/hepg2.mle.linear-0680.ckpt\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5705 - mse: 0.5634 - val_loss: 0.5576 - val_mse: 0.5505\n",
      "Epoch 681/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5575 - mse: 0.5504 - val_loss: 0.5618 - val_mse: 0.5547\n",
      "Epoch 682/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5590 - mse: 0.5519 - val_loss: 0.5571 - val_mse: 0.5501\n",
      "Epoch 683/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5607 - mse: 0.5536 - val_loss: 0.5561 - val_mse: 0.5491\n",
      "Epoch 684/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5627 - mse: 0.5556 - val_loss: 0.5566 - val_mse: 0.5495\n",
      "Epoch 685/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5587 - mse: 0.5517 - val_loss: 0.5558 - val_mse: 0.5488\n",
      "Epoch 686/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5579 - mse: 0.5509 - val_loss: 0.5546 - val_mse: 0.5475\n",
      "Epoch 687/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5660 - mse: 0.5590 - val_loss: 0.5574 - val_mse: 0.5503\n",
      "Epoch 688/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5674 - mse: 0.5603 - val_loss: 0.5573 - val_mse: 0.5502\n",
      "Epoch 689/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5602 - mse: 0.5531 - val_loss: 0.5565 - val_mse: 0.5494\n",
      "Epoch 690/2000\n",
      "271/277 [============================>.] - ETA: 0s - loss: 0.5636 - mse: 0.5565\n",
      "Epoch 00690: saving model to Regression_Model/hepg2.mle.linear-0690.ckpt\n",
      "277/277 [==============================] - 2s 6ms/step - loss: 0.5622 - mse: 0.5551 - val_loss: 0.5534 - val_mse: 0.5463\n",
      "Epoch 691/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5659 - mse: 0.5588 - val_loss: 0.5552 - val_mse: 0.5482\n",
      "Epoch 692/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5564 - mse: 0.5494 - val_loss: 0.5576 - val_mse: 0.5506\n",
      "Epoch 693/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5673 - mse: 0.5602 - val_loss: 0.5614 - val_mse: 0.5544\n",
      "Epoch 694/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5614 - mse: 0.5544 - val_loss: 0.5577 - val_mse: 0.5507\n",
      "Epoch 695/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5641 - mse: 0.5570 - val_loss: 0.5577 - val_mse: 0.5506\n",
      "Epoch 696/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5703 - mse: 0.5633 - val_loss: 0.5568 - val_mse: 0.5497\n",
      "Epoch 697/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5584 - mse: 0.5514 - val_loss: 0.5579 - val_mse: 0.5509\n",
      "Epoch 698/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5594 - mse: 0.5523 - val_loss: 0.5579 - val_mse: 0.5508\n",
      "Epoch 699/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5635 - mse: 0.5564 - val_loss: 0.5587 - val_mse: 0.5517\n",
      "Epoch 700/2000\n",
      "268/277 [============================>.] - ETA: 0s - loss: 0.5683 - mse: 0.5612\n",
      "Epoch 00700: saving model to Regression_Model/hepg2.mle.linear-0700.ckpt\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5688 - mse: 0.5617 - val_loss: 0.5617 - val_mse: 0.5547\n",
      "Epoch 701/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5586 - mse: 0.5516 - val_loss: 0.5563 - val_mse: 0.5493\n",
      "Epoch 702/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5588 - mse: 0.5518 - val_loss: 0.5592 - val_mse: 0.5522\n",
      "Epoch 703/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5665 - mse: 0.5594 - val_loss: 0.5553 - val_mse: 0.5483\n",
      "Epoch 704/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5637 - mse: 0.5566 - val_loss: 0.5555 - val_mse: 0.5484\n",
      "Epoch 705/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5603 - mse: 0.5533 - val_loss: 0.5575 - val_mse: 0.5505\n",
      "Epoch 706/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5642 - mse: 0.5572 - val_loss: 0.5560 - val_mse: 0.5490\n",
      "Epoch 707/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5620 - mse: 0.5550 - val_loss: 0.5568 - val_mse: 0.5497\n",
      "Epoch 708/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5616 - mse: 0.5546 - val_loss: 0.5552 - val_mse: 0.5482\n",
      "Epoch 709/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5648 - mse: 0.5577 - val_loss: 0.5568 - val_mse: 0.5498\n",
      "Epoch 710/2000\n",
      "271/277 [============================>.] - ETA: 0s - loss: 0.5567 - mse: 0.5497\n",
      "Epoch 00710: saving model to Regression_Model/hepg2.mle.linear-0710.ckpt\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5600 - mse: 0.5530 - val_loss: 0.5579 - val_mse: 0.5508\n",
      "Epoch 711/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5609 - mse: 0.5539 - val_loss: 0.5588 - val_mse: 0.5518\n",
      "Epoch 712/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5681 - mse: 0.5611 - val_loss: 0.5602 - val_mse: 0.5532\n",
      "Epoch 713/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5681 - mse: 0.5611 - val_loss: 0.5558 - val_mse: 0.5487\n",
      "Epoch 714/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5647 - mse: 0.5577 - val_loss: 0.5555 - val_mse: 0.5484\n",
      "Epoch 715/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5606 - mse: 0.5536 - val_loss: 0.5550 - val_mse: 0.5480\n",
      "Epoch 716/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5599 - mse: 0.5529 - val_loss: 0.5587 - val_mse: 0.5517\n",
      "Epoch 717/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5572 - mse: 0.5502 - val_loss: 0.5558 - val_mse: 0.5487\n",
      "Epoch 718/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5618 - mse: 0.5548 - val_loss: 0.5547 - val_mse: 0.5477\n",
      "Epoch 719/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5638 - mse: 0.5568 - val_loss: 0.5556 - val_mse: 0.5486\n",
      "Epoch 720/2000\n",
      "275/277 [============================>.] - ETA: 0s - loss: 0.5599 - mse: 0.5529\n",
      "Epoch 00720: saving model to Regression_Model/hepg2.mle.linear-0720.ckpt\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5594 - mse: 0.5524 - val_loss: 0.5562 - val_mse: 0.5492\n",
      "Epoch 721/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5660 - mse: 0.5590 - val_loss: 0.5567 - val_mse: 0.5497\n",
      "Epoch 722/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5622 - mse: 0.5552 - val_loss: 0.5571 - val_mse: 0.5501\n",
      "Epoch 723/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5632 - mse: 0.5562 - val_loss: 0.5540 - val_mse: 0.5470\n",
      "Epoch 724/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5589 - mse: 0.5519 - val_loss: 0.5555 - val_mse: 0.5485\n",
      "Epoch 725/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5672 - mse: 0.5601 - val_loss: 0.5570 - val_mse: 0.5500\n",
      "Epoch 726/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5661 - mse: 0.5591 - val_loss: 0.5562 - val_mse: 0.5492\n",
      "Epoch 727/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5670 - mse: 0.5599 - val_loss: 0.5544 - val_mse: 0.5473\n",
      "Epoch 728/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5625 - mse: 0.5555 - val_loss: 0.5553 - val_mse: 0.5483\n",
      "Epoch 729/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5571 - mse: 0.5501 - val_loss: 0.5594 - val_mse: 0.5524\n",
      "Epoch 730/2000\n",
      "276/277 [============================>.] - ETA: 0s - loss: 0.5646 - mse: 0.5576\n",
      "Epoch 00730: saving model to Regression_Model/hepg2.mle.linear-0730.ckpt\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5642 - mse: 0.5572 - val_loss: 0.5549 - val_mse: 0.5479\n",
      "Epoch 731/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5600 - mse: 0.5530 - val_loss: 0.5540 - val_mse: 0.5470\n",
      "Epoch 732/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5622 - mse: 0.5552 - val_loss: 0.5536 - val_mse: 0.5466\n",
      "Epoch 733/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5589 - mse: 0.5519 - val_loss: 0.5575 - val_mse: 0.5505\n",
      "Epoch 734/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5626 - mse: 0.5556 - val_loss: 0.5567 - val_mse: 0.5497\n",
      "Epoch 735/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5575 - mse: 0.5505 - val_loss: 0.5571 - val_mse: 0.5501\n",
      "Epoch 736/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5602 - mse: 0.5532 - val_loss: 0.5550 - val_mse: 0.5480\n",
      "Epoch 737/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5631 - mse: 0.5561 - val_loss: 0.5536 - val_mse: 0.5466\n",
      "Epoch 738/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5642 - mse: 0.5572 - val_loss: 0.5544 - val_mse: 0.5474\n",
      "Epoch 739/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5637 - mse: 0.5567 - val_loss: 0.5571 - val_mse: 0.5501\n",
      "Epoch 740/2000\n",
      "273/277 [============================>.] - ETA: 0s - loss: 0.5629 - mse: 0.5559\n",
      "Epoch 00740: saving model to Regression_Model/hepg2.mle.linear-0740.ckpt\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5617 - mse: 0.5547 - val_loss: 0.5595 - val_mse: 0.5525\n",
      "Epoch 741/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5619 - mse: 0.5549 - val_loss: 0.5540 - val_mse: 0.5470\n",
      "Epoch 742/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5636 - mse: 0.5566 - val_loss: 0.5539 - val_mse: 0.5469\n",
      "Epoch 743/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5556 - mse: 0.5486 - val_loss: 0.5534 - val_mse: 0.5464\n",
      "Epoch 744/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5557 - mse: 0.5487 - val_loss: 0.5555 - val_mse: 0.5486\n",
      "Epoch 745/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5584 - mse: 0.5514 - val_loss: 0.5545 - val_mse: 0.5475\n",
      "Epoch 746/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5578 - mse: 0.5509 - val_loss: 0.5542 - val_mse: 0.5472\n",
      "Epoch 747/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5610 - mse: 0.5541 - val_loss: 0.5564 - val_mse: 0.5494\n",
      "Epoch 748/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5626 - mse: 0.5556 - val_loss: 0.5537 - val_mse: 0.5468\n",
      "Epoch 749/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5604 - mse: 0.5535 - val_loss: 0.5556 - val_mse: 0.5486\n",
      "Epoch 750/2000\n",
      "274/277 [============================>.] - ETA: 0s - loss: 0.5590 - mse: 0.5520\n",
      "Epoch 00750: saving model to Regression_Model/hepg2.mle.linear-0750.ckpt\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5600 - mse: 0.5531 - val_loss: 0.5549 - val_mse: 0.5479\n",
      "Epoch 751/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5584 - mse: 0.5514 - val_loss: 0.5526 - val_mse: 0.5456\n",
      "Epoch 752/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5578 - mse: 0.5508 - val_loss: 0.5543 - val_mse: 0.5473\n",
      "Epoch 753/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5577 - mse: 0.5507 - val_loss: 0.5556 - val_mse: 0.5486\n",
      "Epoch 754/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5666 - mse: 0.5597 - val_loss: 0.5534 - val_mse: 0.5464\n",
      "Epoch 755/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5602 - mse: 0.5532 - val_loss: 0.5550 - val_mse: 0.5480\n",
      "Epoch 756/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5645 - mse: 0.5575 - val_loss: 0.5560 - val_mse: 0.5491\n",
      "Epoch 757/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5678 - mse: 0.5608 - val_loss: 0.5528 - val_mse: 0.5459\n",
      "Epoch 758/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5543 - mse: 0.5473 - val_loss: 0.5539 - val_mse: 0.5469\n",
      "Epoch 759/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5580 - mse: 0.5511 - val_loss: 0.5563 - val_mse: 0.5494\n",
      "Epoch 760/2000\n",
      "270/277 [============================>.] - ETA: 0s - loss: 0.5583 - mse: 0.5513\n",
      "Epoch 00760: saving model to Regression_Model/hepg2.mle.linear-0760.ckpt\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5589 - mse: 0.5519 - val_loss: 0.5576 - val_mse: 0.5507\n",
      "Epoch 761/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5653 - mse: 0.5584 - val_loss: 0.5554 - val_mse: 0.5484\n",
      "Epoch 762/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5575 - mse: 0.5506 - val_loss: 0.5554 - val_mse: 0.5485\n",
      "Epoch 763/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5692 - mse: 0.5622 - val_loss: 0.5581 - val_mse: 0.5512\n",
      "Epoch 764/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5647 - mse: 0.5578 - val_loss: 0.5544 - val_mse: 0.5475\n",
      "Epoch 765/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5616 - mse: 0.5546 - val_loss: 0.5562 - val_mse: 0.5492\n",
      "Epoch 766/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5600 - mse: 0.5530 - val_loss: 0.5584 - val_mse: 0.5514\n",
      "Epoch 767/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5662 - mse: 0.5593 - val_loss: 0.5560 - val_mse: 0.5490\n",
      "Epoch 768/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5627 - mse: 0.5557 - val_loss: 0.5576 - val_mse: 0.5506\n",
      "Epoch 769/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5625 - mse: 0.5556 - val_loss: 0.5585 - val_mse: 0.5515\n",
      "Epoch 770/2000\n",
      "271/277 [============================>.] - ETA: 0s - loss: 0.5574 - mse: 0.5504\n",
      "Epoch 00770: saving model to Regression_Model/hepg2.mle.linear-0770.ckpt\n",
      "277/277 [==============================] - 2s 6ms/step - loss: 0.5573 - mse: 0.5503 - val_loss: 0.5567 - val_mse: 0.5498\n",
      "Epoch 771/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5650 - mse: 0.5580 - val_loss: 0.5558 - val_mse: 0.5489\n",
      "Epoch 772/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5613 - mse: 0.5543 - val_loss: 0.5546 - val_mse: 0.5477\n",
      "Epoch 773/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5613 - mse: 0.5544 - val_loss: 0.5612 - val_mse: 0.5542\n",
      "Epoch 774/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5605 - mse: 0.5535 - val_loss: 0.5542 - val_mse: 0.5473\n",
      "Epoch 775/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5637 - mse: 0.5568 - val_loss: 0.5558 - val_mse: 0.5489\n",
      "Epoch 776/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5598 - mse: 0.5529 - val_loss: 0.5605 - val_mse: 0.5536\n",
      "Epoch 777/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5608 - mse: 0.5539 - val_loss: 0.5546 - val_mse: 0.5476\n",
      "Epoch 778/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5572 - mse: 0.5503 - val_loss: 0.5551 - val_mse: 0.5481\n",
      "Epoch 779/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5623 - mse: 0.5554 - val_loss: 0.5540 - val_mse: 0.5471\n",
      "Epoch 780/2000\n",
      "266/277 [===========================>..] - ETA: 0s - loss: 0.5554 - mse: 0.5485\n",
      "Epoch 00780: saving model to Regression_Model/hepg2.mle.linear-0780.ckpt\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5539 - mse: 0.5470 - val_loss: 0.5553 - val_mse: 0.5484\n",
      "Epoch 781/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5587 - mse: 0.5517 - val_loss: 0.5572 - val_mse: 0.5503\n",
      "Epoch 782/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5575 - mse: 0.5506 - val_loss: 0.5551 - val_mse: 0.5481\n",
      "Epoch 783/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5634 - mse: 0.5565 - val_loss: 0.5560 - val_mse: 0.5491\n",
      "Epoch 784/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5626 - mse: 0.5556 - val_loss: 0.5577 - val_mse: 0.5508\n",
      "Epoch 785/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5652 - mse: 0.5583 - val_loss: 0.5553 - val_mse: 0.5484\n",
      "Epoch 786/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5592 - mse: 0.5523 - val_loss: 0.5571 - val_mse: 0.5502\n",
      "Epoch 787/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5615 - mse: 0.5546 - val_loss: 0.5548 - val_mse: 0.5478\n",
      "Epoch 788/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5580 - mse: 0.5511 - val_loss: 0.5539 - val_mse: 0.5470\n",
      "Epoch 789/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5645 - mse: 0.5576 - val_loss: 0.5543 - val_mse: 0.5473\n",
      "Epoch 790/2000\n",
      "268/277 [============================>.] - ETA: 0s - loss: 0.5633 - mse: 0.5564\n",
      "Epoch 00790: saving model to Regression_Model/hepg2.mle.linear-0790.ckpt\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5633 - mse: 0.5563 - val_loss: 0.5540 - val_mse: 0.5471\n",
      "Epoch 791/2000\n",
      "277/277 [==============================] - 2s 6ms/step - loss: 0.5615 - mse: 0.5545 - val_loss: 0.5568 - val_mse: 0.5499\n",
      "Epoch 792/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5608 - mse: 0.5539 - val_loss: 0.5560 - val_mse: 0.5491\n",
      "Epoch 793/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5588 - mse: 0.5519 - val_loss: 0.5546 - val_mse: 0.5477\n",
      "Epoch 794/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5564 - mse: 0.5495 - val_loss: 0.5566 - val_mse: 0.5497\n",
      "Epoch 795/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5600 - mse: 0.5531 - val_loss: 0.5544 - val_mse: 0.5475\n",
      "Epoch 796/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5610 - mse: 0.5541 - val_loss: 0.5597 - val_mse: 0.5528\n",
      "Epoch 797/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5589 - mse: 0.5520 - val_loss: 0.5565 - val_mse: 0.5496\n",
      "Epoch 798/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5656 - mse: 0.5586 - val_loss: 0.5554 - val_mse: 0.5485\n",
      "Epoch 799/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5536 - mse: 0.5467 - val_loss: 0.5560 - val_mse: 0.5491\n",
      "Epoch 800/2000\n",
      "273/277 [============================>.] - ETA: 0s - loss: 0.5565 - mse: 0.5496\n",
      "Epoch 00800: saving model to Regression_Model/hepg2.mle.linear-0800.ckpt\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5566 - mse: 0.5497 - val_loss: 0.5556 - val_mse: 0.5487\n",
      "Epoch 801/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5555 - mse: 0.5486 - val_loss: 0.5551 - val_mse: 0.5482\n",
      "Epoch 802/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5614 - mse: 0.5545 - val_loss: 0.5569 - val_mse: 0.5500\n",
      "Epoch 803/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5569 - mse: 0.5500 - val_loss: 0.5580 - val_mse: 0.5511\n",
      "Epoch 804/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5594 - mse: 0.5525 - val_loss: 0.5561 - val_mse: 0.5492\n",
      "Epoch 805/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5545 - mse: 0.5476 - val_loss: 0.5563 - val_mse: 0.5494\n",
      "Epoch 806/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5612 - mse: 0.5543 - val_loss: 0.5557 - val_mse: 0.5488\n",
      "Epoch 807/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5610 - mse: 0.5541 - val_loss: 0.5545 - val_mse: 0.5476\n",
      "Epoch 808/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5544 - mse: 0.5475 - val_loss: 0.5567 - val_mse: 0.5498\n",
      "Epoch 809/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5618 - mse: 0.5549 - val_loss: 0.5560 - val_mse: 0.5492\n",
      "Epoch 810/2000\n",
      "275/277 [============================>.] - ETA: 0s - loss: 0.5647 - mse: 0.5578\n",
      "Epoch 00810: saving model to Regression_Model/hepg2.mle.linear-0810.ckpt\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5654 - mse: 0.5585 - val_loss: 0.5583 - val_mse: 0.5514\n",
      "Epoch 811/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5521 - mse: 0.5452 - val_loss: 0.5550 - val_mse: 0.5481\n",
      "Epoch 812/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5638 - mse: 0.5569 - val_loss: 0.5583 - val_mse: 0.5515\n",
      "Epoch 813/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5593 - mse: 0.5524 - val_loss: 0.5570 - val_mse: 0.5501\n",
      "Epoch 814/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5558 - mse: 0.5489 - val_loss: 0.5574 - val_mse: 0.5505\n",
      "Epoch 815/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5602 - mse: 0.5533 - val_loss: 0.5563 - val_mse: 0.5494\n",
      "Epoch 816/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5545 - mse: 0.5476 - val_loss: 0.5562 - val_mse: 0.5493\n",
      "Epoch 817/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5680 - mse: 0.5611 - val_loss: 0.5543 - val_mse: 0.5474\n",
      "Epoch 818/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5621 - mse: 0.5552 - val_loss: 0.5567 - val_mse: 0.5498\n",
      "Epoch 819/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5547 - mse: 0.5478 - val_loss: 0.5547 - val_mse: 0.5479\n",
      "Epoch 820/2000\n",
      "270/277 [============================>.] - ETA: 0s - loss: 0.5649 - mse: 0.5581\n",
      "Epoch 00820: saving model to Regression_Model/hepg2.mle.linear-0820.ckpt\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5656 - mse: 0.5587 - val_loss: 0.5566 - val_mse: 0.5497\n",
      "Epoch 821/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5571 - mse: 0.5502 - val_loss: 0.5562 - val_mse: 0.5494\n",
      "Epoch 822/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5561 - mse: 0.5492 - val_loss: 0.5564 - val_mse: 0.5495\n",
      "Epoch 823/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5587 - mse: 0.5518 - val_loss: 0.5584 - val_mse: 0.5515\n",
      "Epoch 824/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5625 - mse: 0.5556 - val_loss: 0.5549 - val_mse: 0.5480\n",
      "Epoch 825/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5610 - mse: 0.5542 - val_loss: 0.5552 - val_mse: 0.5483\n",
      "Epoch 826/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5639 - mse: 0.5571 - val_loss: 0.5555 - val_mse: 0.5487\n",
      "Epoch 827/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5579 - mse: 0.5510 - val_loss: 0.5578 - val_mse: 0.5509\n",
      "Epoch 828/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5619 - mse: 0.5550 - val_loss: 0.5597 - val_mse: 0.5528\n",
      "Epoch 829/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5673 - mse: 0.5604 - val_loss: 0.5570 - val_mse: 0.5501\n",
      "Epoch 830/2000\n",
      "272/277 [============================>.] - ETA: 0s - loss: 0.5588 - mse: 0.5519\n",
      "Epoch 00830: saving model to Regression_Model/hepg2.mle.linear-0830.ckpt\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5613 - mse: 0.5544 - val_loss: 0.5549 - val_mse: 0.5480\n",
      "Epoch 831/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5610 - mse: 0.5541 - val_loss: 0.5576 - val_mse: 0.5507\n",
      "Epoch 832/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5719 - mse: 0.5651 - val_loss: 0.5575 - val_mse: 0.5506\n",
      "Epoch 833/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5557 - mse: 0.5488 - val_loss: 0.5546 - val_mse: 0.5478\n",
      "Epoch 834/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5588 - mse: 0.5520 - val_loss: 0.5567 - val_mse: 0.5498\n",
      "Epoch 835/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5580 - mse: 0.5511 - val_loss: 0.5551 - val_mse: 0.5482\n",
      "Epoch 836/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5533 - mse: 0.5464 - val_loss: 0.5558 - val_mse: 0.5489\n",
      "Epoch 837/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5590 - mse: 0.5522 - val_loss: 0.5556 - val_mse: 0.5488\n",
      "Epoch 838/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5634 - mse: 0.5565 - val_loss: 0.5565 - val_mse: 0.5496\n",
      "Epoch 839/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5581 - mse: 0.5512 - val_loss: 0.5546 - val_mse: 0.5478\n",
      "Epoch 840/2000\n",
      "265/277 [===========================>..] - ETA: 0s - loss: 0.5509 - mse: 0.5441\n",
      "Epoch 00840: saving model to Regression_Model/hepg2.mle.linear-0840.ckpt\n",
      "277/277 [==============================] - 2s 6ms/step - loss: 0.5520 - mse: 0.5451 - val_loss: 0.5561 - val_mse: 0.5493\n",
      "Epoch 841/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5529 - mse: 0.5461 - val_loss: 0.5576 - val_mse: 0.5508\n",
      "Epoch 842/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5623 - mse: 0.5555 - val_loss: 0.5566 - val_mse: 0.5498\n",
      "Epoch 843/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5597 - mse: 0.5529 - val_loss: 0.5574 - val_mse: 0.5505\n",
      "Epoch 844/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5606 - mse: 0.5538 - val_loss: 0.5551 - val_mse: 0.5482\n",
      "Epoch 845/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5643 - mse: 0.5575 - val_loss: 0.5552 - val_mse: 0.5484\n",
      "Epoch 846/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5644 - mse: 0.5576 - val_loss: 0.5549 - val_mse: 0.5480\n",
      "Epoch 847/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5611 - mse: 0.5542 - val_loss: 0.5559 - val_mse: 0.5490\n",
      "Epoch 848/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5592 - mse: 0.5524 - val_loss: 0.5559 - val_mse: 0.5491\n",
      "Epoch 849/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5596 - mse: 0.5527 - val_loss: 0.5574 - val_mse: 0.5505\n",
      "Epoch 850/2000\n",
      "272/277 [============================>.] - ETA: 0s - loss: 0.5622 - mse: 0.5554\n",
      "Epoch 00850: saving model to Regression_Model/hepg2.mle.linear-0850.ckpt\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5618 - mse: 0.5549 - val_loss: 0.5566 - val_mse: 0.5497\n",
      "Epoch 851/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5567 - mse: 0.5498 - val_loss: 0.5563 - val_mse: 0.5495\n",
      "Epoch 852/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5527 - mse: 0.5458 - val_loss: 0.5605 - val_mse: 0.5537\n",
      "Epoch 853/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5615 - mse: 0.5546 - val_loss: 0.5540 - val_mse: 0.5472\n",
      "Epoch 854/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5603 - mse: 0.5535 - val_loss: 0.5549 - val_mse: 0.5480\n",
      "Epoch 855/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5604 - mse: 0.5535 - val_loss: 0.5571 - val_mse: 0.5502\n",
      "Epoch 856/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5493 - mse: 0.5424 - val_loss: 0.5577 - val_mse: 0.5509\n",
      "Epoch 857/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5543 - mse: 0.5475 - val_loss: 0.5562 - val_mse: 0.5494\n",
      "Epoch 858/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5625 - mse: 0.5557 - val_loss: 0.5546 - val_mse: 0.5478\n",
      "Epoch 859/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5606 - mse: 0.5538 - val_loss: 0.5560 - val_mse: 0.5492\n",
      "Epoch 860/2000\n",
      "272/277 [============================>.] - ETA: 0s - loss: 0.5579 - mse: 0.5511\n",
      "Epoch 00860: saving model to Regression_Model/hepg2.mle.linear-0860.ckpt\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5576 - mse: 0.5507 - val_loss: 0.5548 - val_mse: 0.5480\n",
      "Epoch 861/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5590 - mse: 0.5521 - val_loss: 0.5580 - val_mse: 0.5512\n",
      "Epoch 862/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5679 - mse: 0.5611 - val_loss: 0.5577 - val_mse: 0.5509\n",
      "Epoch 863/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5612 - mse: 0.5544 - val_loss: 0.5540 - val_mse: 0.5471\n",
      "Epoch 864/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5599 - mse: 0.5530 - val_loss: 0.5558 - val_mse: 0.5490\n",
      "Epoch 865/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5598 - mse: 0.5530 - val_loss: 0.5540 - val_mse: 0.5472\n",
      "Epoch 866/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5579 - mse: 0.5511 - val_loss: 0.5561 - val_mse: 0.5492\n",
      "Epoch 867/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5609 - mse: 0.5541 - val_loss: 0.5585 - val_mse: 0.5517\n",
      "Epoch 868/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5620 - mse: 0.5552 - val_loss: 0.5546 - val_mse: 0.5478\n",
      "Epoch 869/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5635 - mse: 0.5567 - val_loss: 0.5538 - val_mse: 0.5469\n",
      "Epoch 870/2000\n",
      "264/277 [===========================>..] - ETA: 0s - loss: 0.5600 - mse: 0.5532\n",
      "Epoch 00870: saving model to Regression_Model/hepg2.mle.linear-0870.ckpt\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5612 - mse: 0.5544 - val_loss: 0.5558 - val_mse: 0.5490\n",
      "Epoch 871/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5552 - mse: 0.5483 - val_loss: 0.5533 - val_mse: 0.5465\n",
      "Epoch 872/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5519 - mse: 0.5451 - val_loss: 0.5540 - val_mse: 0.5472\n",
      "Epoch 873/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5584 - mse: 0.5516 - val_loss: 0.5545 - val_mse: 0.5477\n",
      "Epoch 874/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5521 - mse: 0.5453 - val_loss: 0.5558 - val_mse: 0.5490\n",
      "Epoch 875/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5556 - mse: 0.5488 - val_loss: 0.5543 - val_mse: 0.5475\n",
      "Epoch 876/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5598 - mse: 0.5530 - val_loss: 0.5571 - val_mse: 0.5503\n",
      "Epoch 877/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5553 - mse: 0.5485 - val_loss: 0.5560 - val_mse: 0.5492\n",
      "Epoch 878/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5627 - mse: 0.5559 - val_loss: 0.5531 - val_mse: 0.5463\n",
      "Epoch 879/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5595 - mse: 0.5527 - val_loss: 0.5528 - val_mse: 0.5460\n",
      "Epoch 880/2000\n",
      "274/277 [============================>.] - ETA: 0s - loss: 0.5570 - mse: 0.5502\n",
      "Epoch 00880: saving model to Regression_Model/hepg2.mle.linear-0880.ckpt\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5558 - mse: 0.5490 - val_loss: 0.5548 - val_mse: 0.5480\n",
      "Epoch 881/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5599 - mse: 0.5531 - val_loss: 0.5529 - val_mse: 0.5461\n",
      "Epoch 882/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5627 - mse: 0.5559 - val_loss: 0.5558 - val_mse: 0.5490\n",
      "Epoch 883/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5615 - mse: 0.5546 - val_loss: 0.5559 - val_mse: 0.5491\n",
      "Epoch 884/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5671 - mse: 0.5603 - val_loss: 0.5552 - val_mse: 0.5484\n",
      "Epoch 885/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5623 - mse: 0.5555 - val_loss: 0.5547 - val_mse: 0.5479\n",
      "Epoch 886/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5601 - mse: 0.5533 - val_loss: 0.5547 - val_mse: 0.5479\n",
      "Epoch 887/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5601 - mse: 0.5533 - val_loss: 0.5533 - val_mse: 0.5465\n",
      "Epoch 888/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5591 - mse: 0.5523 - val_loss: 0.5531 - val_mse: 0.5463\n",
      "Epoch 889/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5543 - mse: 0.5475 - val_loss: 0.5553 - val_mse: 0.5485\n",
      "Epoch 890/2000\n",
      "272/277 [============================>.] - ETA: 0s - loss: 0.5636 - mse: 0.5568\n",
      "Epoch 00890: saving model to Regression_Model/hepg2.mle.linear-0890.ckpt\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5633 - mse: 0.5565 - val_loss: 0.5542 - val_mse: 0.5474\n",
      "Epoch 891/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5618 - mse: 0.5550 - val_loss: 0.5535 - val_mse: 0.5467\n",
      "Epoch 892/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5600 - mse: 0.5532 - val_loss: 0.5548 - val_mse: 0.5480\n",
      "Epoch 893/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5615 - mse: 0.5547 - val_loss: 0.5560 - val_mse: 0.5492\n",
      "Epoch 894/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5626 - mse: 0.5558 - val_loss: 0.5543 - val_mse: 0.5475\n",
      "Epoch 895/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5648 - mse: 0.5580 - val_loss: 0.5567 - val_mse: 0.5499\n",
      "Epoch 896/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5618 - mse: 0.5550 - val_loss: 0.5574 - val_mse: 0.5506\n",
      "Epoch 897/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5561 - mse: 0.5493 - val_loss: 0.5557 - val_mse: 0.5489\n",
      "Epoch 898/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5624 - mse: 0.5556 - val_loss: 0.5552 - val_mse: 0.5484\n",
      "Epoch 899/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5583 - mse: 0.5515 - val_loss: 0.5584 - val_mse: 0.5516\n",
      "Epoch 900/2000\n",
      "276/277 [============================>.] - ETA: 0s - loss: 0.5518 - mse: 0.5450\n",
      "Epoch 00900: saving model to Regression_Model/hepg2.mle.linear-0900.ckpt\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5516 - mse: 0.5448 - val_loss: 0.5551 - val_mse: 0.5483\n",
      "Epoch 901/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5563 - mse: 0.5495 - val_loss: 0.5565 - val_mse: 0.5497\n",
      "Epoch 902/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5611 - mse: 0.5543 - val_loss: 0.5567 - val_mse: 0.5500\n",
      "Epoch 903/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5564 - mse: 0.5496 - val_loss: 0.5543 - val_mse: 0.5475\n",
      "Epoch 904/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5646 - mse: 0.5579 - val_loss: 0.5618 - val_mse: 0.5550\n",
      "Epoch 905/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5579 - mse: 0.5511 - val_loss: 0.5545 - val_mse: 0.5477\n",
      "Epoch 906/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5535 - mse: 0.5467 - val_loss: 0.5548 - val_mse: 0.5480\n",
      "Epoch 907/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5596 - mse: 0.5528 - val_loss: 0.5549 - val_mse: 0.5481\n",
      "Epoch 908/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5584 - mse: 0.5517 - val_loss: 0.5547 - val_mse: 0.5479\n",
      "Epoch 909/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5595 - mse: 0.5527 - val_loss: 0.5558 - val_mse: 0.5490\n",
      "Epoch 910/2000\n",
      "276/277 [============================>.] - ETA: 0s - loss: 0.5575 - mse: 0.5508\n",
      "Epoch 00910: saving model to Regression_Model/hepg2.mle.linear-0910.ckpt\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5578 - mse: 0.5511 - val_loss: 0.5560 - val_mse: 0.5492\n",
      "Epoch 911/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5594 - mse: 0.5526 - val_loss: 0.5558 - val_mse: 0.5491\n",
      "Epoch 912/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5540 - mse: 0.5472 - val_loss: 0.5551 - val_mse: 0.5483\n",
      "Epoch 913/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5549 - mse: 0.5481 - val_loss: 0.5560 - val_mse: 0.5493\n",
      "Epoch 914/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5547 - mse: 0.5480 - val_loss: 0.5552 - val_mse: 0.5484\n",
      "Epoch 915/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5609 - mse: 0.5542 - val_loss: 0.5562 - val_mse: 0.5494\n",
      "Epoch 916/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5524 - mse: 0.5457 - val_loss: 0.5552 - val_mse: 0.5484\n",
      "Epoch 917/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5547 - mse: 0.5480 - val_loss: 0.5556 - val_mse: 0.5489\n",
      "Epoch 918/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5563 - mse: 0.5495 - val_loss: 0.5554 - val_mse: 0.5486\n",
      "Epoch 919/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5582 - mse: 0.5514 - val_loss: 0.5554 - val_mse: 0.5486\n",
      "Epoch 920/2000\n",
      "269/277 [============================>.] - ETA: 0s - loss: 0.5519 - mse: 0.5451\n",
      "Epoch 00920: saving model to Regression_Model/hepg2.mle.linear-0920.ckpt\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5547 - mse: 0.5479 - val_loss: 0.5548 - val_mse: 0.5481\n",
      "Epoch 921/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5568 - mse: 0.5500 - val_loss: 0.5549 - val_mse: 0.5482\n",
      "Epoch 922/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5546 - mse: 0.5479 - val_loss: 0.5562 - val_mse: 0.5495\n",
      "Epoch 923/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5559 - mse: 0.5492 - val_loss: 0.5552 - val_mse: 0.5485\n",
      "Epoch 924/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5577 - mse: 0.5509 - val_loss: 0.5550 - val_mse: 0.5482\n",
      "Epoch 925/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5597 - mse: 0.5530 - val_loss: 0.5549 - val_mse: 0.5481\n",
      "Epoch 926/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5613 - mse: 0.5546 - val_loss: 0.5582 - val_mse: 0.5515\n",
      "Epoch 927/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5531 - mse: 0.5463 - val_loss: 0.5543 - val_mse: 0.5475\n",
      "Epoch 928/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5521 - mse: 0.5453 - val_loss: 0.5538 - val_mse: 0.5470\n",
      "Epoch 929/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5602 - mse: 0.5534 - val_loss: 0.5545 - val_mse: 0.5477\n",
      "Epoch 930/2000\n",
      "269/277 [============================>.] - ETA: 0s - loss: 0.5544 - mse: 0.5476\n",
      "Epoch 00930: saving model to Regression_Model/hepg2.mle.linear-0930.ckpt\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5568 - mse: 0.5500 - val_loss: 0.5545 - val_mse: 0.5478\n",
      "Epoch 931/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5571 - mse: 0.5504 - val_loss: 0.5545 - val_mse: 0.5478\n",
      "Epoch 932/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5498 - mse: 0.5430 - val_loss: 0.5553 - val_mse: 0.5486\n",
      "Epoch 933/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5674 - mse: 0.5606 - val_loss: 0.5621 - val_mse: 0.5554\n",
      "Epoch 934/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5592 - mse: 0.5525 - val_loss: 0.5549 - val_mse: 0.5482\n",
      "Epoch 935/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5648 - mse: 0.5581 - val_loss: 0.5551 - val_mse: 0.5483\n",
      "Epoch 936/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5520 - mse: 0.5452 - val_loss: 0.5549 - val_mse: 0.5481\n",
      "Epoch 937/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5605 - mse: 0.5538 - val_loss: 0.5568 - val_mse: 0.5501\n",
      "Epoch 938/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5491 - mse: 0.5424 - val_loss: 0.5548 - val_mse: 0.5481\n",
      "Epoch 939/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5587 - mse: 0.5520 - val_loss: 0.5541 - val_mse: 0.5473\n",
      "Epoch 940/2000\n",
      "268/277 [============================>.] - ETA: 0s - loss: 0.5549 - mse: 0.5481\n",
      "Epoch 00940: saving model to Regression_Model/hepg2.mle.linear-0940.ckpt\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5530 - mse: 0.5463 - val_loss: 0.5553 - val_mse: 0.5486\n",
      "Epoch 941/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5516 - mse: 0.5448 - val_loss: 0.5534 - val_mse: 0.5467\n",
      "Epoch 942/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5546 - mse: 0.5479 - val_loss: 0.5550 - val_mse: 0.5482\n",
      "Epoch 943/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5497 - mse: 0.5430 - val_loss: 0.5555 - val_mse: 0.5487\n",
      "Epoch 944/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5577 - mse: 0.5509 - val_loss: 0.5548 - val_mse: 0.5480\n",
      "Epoch 945/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5645 - mse: 0.5577 - val_loss: 0.5552 - val_mse: 0.5485\n",
      "Epoch 946/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5587 - mse: 0.5520 - val_loss: 0.5559 - val_mse: 0.5492\n",
      "Epoch 947/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5532 - mse: 0.5464 - val_loss: 0.5550 - val_mse: 0.5483\n",
      "Epoch 948/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5543 - mse: 0.5476 - val_loss: 0.5548 - val_mse: 0.5481\n",
      "Epoch 949/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5539 - mse: 0.5471 - val_loss: 0.5579 - val_mse: 0.5512\n",
      "Epoch 950/2000\n",
      "270/277 [============================>.] - ETA: 0s - loss: 0.5577 - mse: 0.5510\n",
      "Epoch 00950: saving model to Regression_Model/hepg2.mle.linear-0950.ckpt\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5561 - mse: 0.5493 - val_loss: 0.5532 - val_mse: 0.5464\n",
      "Epoch 951/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5596 - mse: 0.5529 - val_loss: 0.5538 - val_mse: 0.5471\n",
      "Epoch 952/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5645 - mse: 0.5577 - val_loss: 0.5539 - val_mse: 0.5472\n",
      "Epoch 953/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5542 - mse: 0.5474 - val_loss: 0.5562 - val_mse: 0.5494\n",
      "Epoch 954/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5612 - mse: 0.5544 - val_loss: 0.5557 - val_mse: 0.5490\n",
      "Epoch 955/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5555 - mse: 0.5487 - val_loss: 0.5554 - val_mse: 0.5486\n",
      "Epoch 956/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5554 - mse: 0.5487 - val_loss: 0.5533 - val_mse: 0.5466\n",
      "Epoch 957/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5614 - mse: 0.5547 - val_loss: 0.5563 - val_mse: 0.5496\n",
      "Epoch 958/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5596 - mse: 0.5529 - val_loss: 0.5545 - val_mse: 0.5478\n",
      "Epoch 959/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5555 - mse: 0.5488 - val_loss: 0.5547 - val_mse: 0.5480\n",
      "Epoch 960/2000\n",
      "266/277 [===========================>..] - ETA: 0s - loss: 0.5533 - mse: 0.5465\n",
      "Epoch 00960: saving model to Regression_Model/hepg2.mle.linear-0960.ckpt\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5513 - mse: 0.5445 - val_loss: 0.5556 - val_mse: 0.5489\n",
      "Epoch 961/2000\n",
      "277/277 [==============================] - 2s 5ms/step - loss: 0.5551 - mse: 0.5484 - val_loss: 0.5551 - val_mse: 0.5484\n",
      "Epoch 962/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5559 - mse: 0.5492 - val_loss: 0.5540 - val_mse: 0.5473\n",
      "Epoch 963/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5546 - mse: 0.5478 - val_loss: 0.5525 - val_mse: 0.5458\n",
      "Epoch 964/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5527 - mse: 0.5459 - val_loss: 0.5525 - val_mse: 0.5458\n",
      "Epoch 965/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5557 - mse: 0.5490 - val_loss: 0.5552 - val_mse: 0.5485\n",
      "Epoch 966/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5533 - mse: 0.5466 - val_loss: 0.5546 - val_mse: 0.5478\n",
      "Epoch 967/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5518 - mse: 0.5450 - val_loss: 0.5555 - val_mse: 0.5488\n",
      "Epoch 968/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5609 - mse: 0.5542 - val_loss: 0.5556 - val_mse: 0.5489\n",
      "Epoch 969/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5579 - mse: 0.5512 - val_loss: 0.5548 - val_mse: 0.5481\n",
      "Epoch 970/2000\n",
      "273/277 [============================>.] - ETA: 0s - loss: 0.5508 - mse: 0.5441\n",
      "Epoch 00970: saving model to Regression_Model/hepg2.mle.linear-0970.ckpt\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5520 - mse: 0.5452 - val_loss: 0.5565 - val_mse: 0.5498\n",
      "Epoch 971/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5549 - mse: 0.5482 - val_loss: 0.5568 - val_mse: 0.5501\n",
      "Epoch 972/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5546 - mse: 0.5479 - val_loss: 0.5542 - val_mse: 0.5475\n",
      "Epoch 973/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5539 - mse: 0.5472 - val_loss: 0.5553 - val_mse: 0.5486\n",
      "Epoch 974/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5530 - mse: 0.5463 - val_loss: 0.5540 - val_mse: 0.5473\n",
      "Epoch 975/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5649 - mse: 0.5582 - val_loss: 0.5554 - val_mse: 0.5487\n",
      "Epoch 976/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5583 - mse: 0.5516 - val_loss: 0.5540 - val_mse: 0.5473\n",
      "Epoch 977/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5499 - mse: 0.5432 - val_loss: 0.5579 - val_mse: 0.5512\n",
      "Epoch 978/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5590 - mse: 0.5523 - val_loss: 0.5569 - val_mse: 0.5502\n",
      "Epoch 979/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5585 - mse: 0.5518 - val_loss: 0.5532 - val_mse: 0.5465\n",
      "Epoch 980/2000\n",
      "270/277 [============================>.] - ETA: 0s - loss: 0.5497 - mse: 0.5430\n",
      "Epoch 00980: saving model to Regression_Model/hepg2.mle.linear-0980.ckpt\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5486 - mse: 0.5419 - val_loss: 0.5548 - val_mse: 0.5481\n",
      "Epoch 981/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5572 - mse: 0.5505 - val_loss: 0.5543 - val_mse: 0.5476\n",
      "Epoch 982/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5580 - mse: 0.5513 - val_loss: 0.5558 - val_mse: 0.5491\n",
      "Epoch 983/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5555 - mse: 0.5488 - val_loss: 0.5584 - val_mse: 0.5517\n",
      "Epoch 984/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5570 - mse: 0.5503 - val_loss: 0.5567 - val_mse: 0.5500\n",
      "Epoch 985/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5603 - mse: 0.5536 - val_loss: 0.5567 - val_mse: 0.5500\n",
      "Epoch 986/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5581 - mse: 0.5514 - val_loss: 0.5552 - val_mse: 0.5485\n",
      "Epoch 987/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5518 - mse: 0.5451 - val_loss: 0.5543 - val_mse: 0.5476\n",
      "Epoch 988/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5528 - mse: 0.5461 - val_loss: 0.5598 - val_mse: 0.5531\n",
      "Epoch 989/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5655 - mse: 0.5588 - val_loss: 0.5575 - val_mse: 0.5508\n",
      "Epoch 990/2000\n",
      "263/277 [===========================>..] - ETA: 0s - loss: 0.5566 - mse: 0.5499\n",
      "Epoch 00990: saving model to Regression_Model/hepg2.mle.linear-0990.ckpt\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5571 - mse: 0.5504 - val_loss: 0.5551 - val_mse: 0.5484\n",
      "Epoch 991/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5576 - mse: 0.5509 - val_loss: 0.5540 - val_mse: 0.5473\n",
      "Epoch 992/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5486 - mse: 0.5420 - val_loss: 0.5535 - val_mse: 0.5469\n",
      "Epoch 993/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5542 - mse: 0.5475 - val_loss: 0.5544 - val_mse: 0.5477\n",
      "Epoch 994/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5604 - mse: 0.5538 - val_loss: 0.5568 - val_mse: 0.5501\n",
      "Epoch 995/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5566 - mse: 0.5499 - val_loss: 0.5563 - val_mse: 0.5496\n",
      "Epoch 996/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5562 - mse: 0.5496 - val_loss: 0.5567 - val_mse: 0.5500\n",
      "Epoch 997/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5560 - mse: 0.5493 - val_loss: 0.5561 - val_mse: 0.5494\n",
      "Epoch 998/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5562 - mse: 0.5495 - val_loss: 0.5546 - val_mse: 0.5479\n",
      "Epoch 999/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5591 - mse: 0.5524 - val_loss: 0.5538 - val_mse: 0.5471\n",
      "Epoch 1000/2000\n",
      "273/277 [============================>.] - ETA: 0s - loss: 0.5589 - mse: 0.5523\n",
      "Epoch 01000: saving model to Regression_Model/hepg2.mle.linear-1000.ckpt\n",
      "277/277 [==============================] - 2s 7ms/step - loss: 0.5602 - mse: 0.5535 - val_loss: 0.5552 - val_mse: 0.5485\n",
      "Epoch 1001/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5567 - mse: 0.5500 - val_loss: 0.5560 - val_mse: 0.5493\n",
      "Epoch 1002/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5548 - mse: 0.5482 - val_loss: 0.5570 - val_mse: 0.5504\n",
      "Epoch 1003/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5486 - mse: 0.5419 - val_loss: 0.5560 - val_mse: 0.5493\n",
      "Epoch 1004/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5569 - mse: 0.5502 - val_loss: 0.5546 - val_mse: 0.5480\n",
      "Epoch 1005/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5564 - mse: 0.5497 - val_loss: 0.5554 - val_mse: 0.5487\n",
      "Epoch 1006/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5565 - mse: 0.5499 - val_loss: 0.5552 - val_mse: 0.5486\n",
      "Epoch 1007/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5512 - mse: 0.5445 - val_loss: 0.5551 - val_mse: 0.5484\n",
      "Epoch 1008/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5517 - mse: 0.5450 - val_loss: 0.5543 - val_mse: 0.5476\n",
      "Epoch 1009/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5607 - mse: 0.5540 - val_loss: 0.5539 - val_mse: 0.5472\n",
      "Epoch 1010/2000\n",
      "269/277 [============================>.] - ETA: 0s - loss: 0.5473 - mse: 0.5406\n",
      "Epoch 01010: saving model to Regression_Model/hepg2.mle.linear-1010.ckpt\n",
      "277/277 [==============================] - 2s 7ms/step - loss: 0.5465 - mse: 0.5398 - val_loss: 0.5552 - val_mse: 0.5485\n",
      "Epoch 1011/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5531 - mse: 0.5464 - val_loss: 0.5546 - val_mse: 0.5479\n",
      "Epoch 1012/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5602 - mse: 0.5536 - val_loss: 0.5541 - val_mse: 0.5474\n",
      "Epoch 1013/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5614 - mse: 0.5547 - val_loss: 0.5543 - val_mse: 0.5476\n",
      "Epoch 1014/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5633 - mse: 0.5567 - val_loss: 0.5552 - val_mse: 0.5485\n",
      "Epoch 1015/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5551 - mse: 0.5485 - val_loss: 0.5577 - val_mse: 0.5511\n",
      "Epoch 1016/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5567 - mse: 0.5500 - val_loss: 0.5538 - val_mse: 0.5471\n",
      "Epoch 1017/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5590 - mse: 0.5523 - val_loss: 0.5572 - val_mse: 0.5506\n",
      "Epoch 1018/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5608 - mse: 0.5541 - val_loss: 0.5554 - val_mse: 0.5487\n",
      "Epoch 1019/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5535 - mse: 0.5469 - val_loss: 0.5538 - val_mse: 0.5471\n",
      "Epoch 1020/2000\n",
      "274/277 [============================>.] - ETA: 0s - loss: 0.5604 - mse: 0.5538\n",
      "Epoch 01020: saving model to Regression_Model/hepg2.mle.linear-1020.ckpt\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5600 - mse: 0.5534 - val_loss: 0.5550 - val_mse: 0.5483\n",
      "Epoch 1021/2000\n",
      "277/277 [==============================] - 2s 6ms/step - loss: 0.5562 - mse: 0.5495 - val_loss: 0.5554 - val_mse: 0.5488\n",
      "Epoch 1022/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5565 - mse: 0.5498 - val_loss: 0.5541 - val_mse: 0.5474\n",
      "Epoch 1023/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5574 - mse: 0.5508 - val_loss: 0.5545 - val_mse: 0.5478\n",
      "Epoch 1024/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5482 - mse: 0.5415 - val_loss: 0.5534 - val_mse: 0.5467\n",
      "Epoch 1025/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5523 - mse: 0.5456 - val_loss: 0.5552 - val_mse: 0.5485\n",
      "Epoch 1026/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5527 - mse: 0.5461 - val_loss: 0.5588 - val_mse: 0.5521\n",
      "Epoch 1027/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5602 - mse: 0.5535 - val_loss: 0.5546 - val_mse: 0.5480\n",
      "Epoch 1028/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5571 - mse: 0.5504 - val_loss: 0.5560 - val_mse: 0.5494\n",
      "Epoch 1029/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5545 - mse: 0.5478 - val_loss: 0.5549 - val_mse: 0.5482\n",
      "Epoch 1030/2000\n",
      "275/277 [============================>.] - ETA: 0s - loss: 0.5617 - mse: 0.5551\n",
      "Epoch 01030: saving model to Regression_Model/hepg2.mle.linear-1030.ckpt\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5609 - mse: 0.5543 - val_loss: 0.5534 - val_mse: 0.5467\n",
      "Epoch 1031/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5480 - mse: 0.5413 - val_loss: 0.5544 - val_mse: 0.5478\n",
      "Epoch 1032/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5620 - mse: 0.5553 - val_loss: 0.5564 - val_mse: 0.5497\n",
      "Epoch 1033/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5554 - mse: 0.5487 - val_loss: 0.5540 - val_mse: 0.5474\n",
      "Epoch 1034/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5545 - mse: 0.5479 - val_loss: 0.5561 - val_mse: 0.5494\n",
      "Epoch 1035/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5590 - mse: 0.5524 - val_loss: 0.5557 - val_mse: 0.5491\n",
      "Epoch 1036/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5620 - mse: 0.5554 - val_loss: 0.5547 - val_mse: 0.5480\n",
      "Epoch 1037/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5578 - mse: 0.5512 - val_loss: 0.5540 - val_mse: 0.5474\n",
      "Epoch 1038/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5580 - mse: 0.5513 - val_loss: 0.5533 - val_mse: 0.5467\n",
      "Epoch 1039/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5523 - mse: 0.5457 - val_loss: 0.5532 - val_mse: 0.5466\n",
      "Epoch 1040/2000\n",
      "266/277 [===========================>..] - ETA: 0s - loss: 0.5581 - mse: 0.5515\n",
      "Epoch 01040: saving model to Regression_Model/hepg2.mle.linear-1040.ckpt\n",
      "277/277 [==============================] - 2s 6ms/step - loss: 0.5571 - mse: 0.5504 - val_loss: 0.5560 - val_mse: 0.5493\n",
      "Epoch 1041/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5485 - mse: 0.5419 - val_loss: 0.5554 - val_mse: 0.5488\n",
      "Epoch 1042/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5524 - mse: 0.5458 - val_loss: 0.5540 - val_mse: 0.5474\n",
      "Epoch 1043/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5562 - mse: 0.5496 - val_loss: 0.5563 - val_mse: 0.5497\n",
      "Epoch 1044/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5605 - mse: 0.5538 - val_loss: 0.5542 - val_mse: 0.5476\n",
      "Epoch 1045/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5494 - mse: 0.5428 - val_loss: 0.5539 - val_mse: 0.5472\n",
      "Epoch 1046/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5601 - mse: 0.5535 - val_loss: 0.5538 - val_mse: 0.5471\n",
      "Epoch 1047/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5567 - mse: 0.5501 - val_loss: 0.5563 - val_mse: 0.5497\n",
      "Epoch 1048/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5651 - mse: 0.5584 - val_loss: 0.5545 - val_mse: 0.5478\n",
      "Epoch 1049/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5467 - mse: 0.5401 - val_loss: 0.5554 - val_mse: 0.5488\n",
      "Epoch 1050/2000\n",
      "271/277 [============================>.] - ETA: 0s - loss: 0.5593 - mse: 0.5527\n",
      "Epoch 01050: saving model to Regression_Model/hepg2.mle.linear-1050.ckpt\n",
      "277/277 [==============================] - 2s 7ms/step - loss: 0.5618 - mse: 0.5552 - val_loss: 0.5544 - val_mse: 0.5478\n",
      "Epoch 1051/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5551 - mse: 0.5485 - val_loss: 0.5559 - val_mse: 0.5493\n",
      "Epoch 1052/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5564 - mse: 0.5498 - val_loss: 0.5558 - val_mse: 0.5492\n",
      "Epoch 1053/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5513 - mse: 0.5447 - val_loss: 0.5555 - val_mse: 0.5489\n",
      "Epoch 1054/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5535 - mse: 0.5469 - val_loss: 0.5538 - val_mse: 0.5472\n",
      "Epoch 1055/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5561 - mse: 0.5495 - val_loss: 0.5561 - val_mse: 0.5495\n",
      "Epoch 1056/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5563 - mse: 0.5497 - val_loss: 0.5597 - val_mse: 0.5530\n",
      "Epoch 1057/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5612 - mse: 0.5546 - val_loss: 0.5561 - val_mse: 0.5494\n",
      "Epoch 1058/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5546 - mse: 0.5480 - val_loss: 0.5550 - val_mse: 0.5483\n",
      "Epoch 1059/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5533 - mse: 0.5467 - val_loss: 0.5557 - val_mse: 0.5491\n",
      "Epoch 1060/2000\n",
      "269/277 [============================>.] - ETA: 0s - loss: 0.5602 - mse: 0.5536\n",
      "Epoch 01060: saving model to Regression_Model/hepg2.mle.linear-1060.ckpt\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5599 - mse: 0.5533 - val_loss: 0.5554 - val_mse: 0.5488\n",
      "Epoch 1061/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5582 - mse: 0.5516 - val_loss: 0.5550 - val_mse: 0.5483\n",
      "Epoch 1062/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5631 - mse: 0.5565 - val_loss: 0.5558 - val_mse: 0.5492\n",
      "Epoch 1063/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5597 - mse: 0.5531 - val_loss: 0.5549 - val_mse: 0.5483\n",
      "Epoch 1064/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5537 - mse: 0.5471 - val_loss: 0.5552 - val_mse: 0.5486\n",
      "Epoch 1065/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5556 - mse: 0.5490 - val_loss: 0.5544 - val_mse: 0.5478\n",
      "Epoch 1066/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5550 - mse: 0.5484 - val_loss: 0.5549 - val_mse: 0.5483\n",
      "Epoch 1067/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5572 - mse: 0.5506 - val_loss: 0.5549 - val_mse: 0.5483\n",
      "Epoch 1068/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5507 - mse: 0.5441 - val_loss: 0.5557 - val_mse: 0.5490\n",
      "Epoch 1069/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5578 - mse: 0.5512 - val_loss: 0.5550 - val_mse: 0.5484\n",
      "Epoch 1070/2000\n",
      "274/277 [============================>.] - ETA: 0s - loss: 0.5494 - mse: 0.5428\n",
      "Epoch 01070: saving model to Regression_Model/hepg2.mle.linear-1070.ckpt\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5491 - mse: 0.5424 - val_loss: 0.5535 - val_mse: 0.5469\n",
      "Epoch 1071/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5524 - mse: 0.5458 - val_loss: 0.5550 - val_mse: 0.5484\n",
      "Epoch 1072/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5516 - mse: 0.5450 - val_loss: 0.5568 - val_mse: 0.5502\n",
      "Epoch 1073/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5575 - mse: 0.5509 - val_loss: 0.5569 - val_mse: 0.5502\n",
      "Epoch 1074/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5622 - mse: 0.5556 - val_loss: 0.5560 - val_mse: 0.5494\n",
      "Epoch 1075/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5619 - mse: 0.5553 - val_loss: 0.5536 - val_mse: 0.5470\n",
      "Epoch 1076/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5550 - mse: 0.5484 - val_loss: 0.5561 - val_mse: 0.5495\n",
      "Epoch 1077/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5559 - mse: 0.5493 - val_loss: 0.5553 - val_mse: 0.5487\n",
      "Epoch 1078/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5630 - mse: 0.5564 - val_loss: 0.5557 - val_mse: 0.5491\n",
      "Epoch 1079/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5524 - mse: 0.5458 - val_loss: 0.5545 - val_mse: 0.5478\n",
      "Epoch 1080/2000\n",
      "274/277 [============================>.] - ETA: 0s - loss: 0.5591 - mse: 0.5524\n",
      "Epoch 01080: saving model to Regression_Model/hepg2.mle.linear-1080.ckpt\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5597 - mse: 0.5530 - val_loss: 0.5576 - val_mse: 0.5509\n",
      "Epoch 1081/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5535 - mse: 0.5469 - val_loss: 0.5555 - val_mse: 0.5489\n",
      "Epoch 1082/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5477 - mse: 0.5411 - val_loss: 0.5557 - val_mse: 0.5491\n",
      "Epoch 1083/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5580 - mse: 0.5514 - val_loss: 0.5550 - val_mse: 0.5484\n",
      "Epoch 1084/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5590 - mse: 0.5524 - val_loss: 0.5561 - val_mse: 0.5494\n",
      "Epoch 1085/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5533 - mse: 0.5467 - val_loss: 0.5544 - val_mse: 0.5478\n",
      "Epoch 1086/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5614 - mse: 0.5548 - val_loss: 0.5553 - val_mse: 0.5487\n",
      "Epoch 1087/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5500 - mse: 0.5434 - val_loss: 0.5554 - val_mse: 0.5488\n",
      "Epoch 1088/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5444 - mse: 0.5378 - val_loss: 0.5549 - val_mse: 0.5483\n",
      "Epoch 1089/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5533 - mse: 0.5467 - val_loss: 0.5540 - val_mse: 0.5474\n",
      "Epoch 1090/2000\n",
      "264/277 [===========================>..] - ETA: 0s - loss: 0.5488 - mse: 0.5422\n",
      "Epoch 01090: saving model to Regression_Model/hepg2.mle.linear-1090.ckpt\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5483 - mse: 0.5416 - val_loss: 0.5551 - val_mse: 0.5485\n",
      "Epoch 1091/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5498 - mse: 0.5432 - val_loss: 0.5539 - val_mse: 0.5473\n",
      "Epoch 1092/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5578 - mse: 0.5512 - val_loss: 0.5533 - val_mse: 0.5467\n",
      "Epoch 1093/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5527 - mse: 0.5461 - val_loss: 0.5539 - val_mse: 0.5473\n",
      "Epoch 1094/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5517 - mse: 0.5452 - val_loss: 0.5557 - val_mse: 0.5491\n",
      "Epoch 1095/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5539 - mse: 0.5473 - val_loss: 0.5544 - val_mse: 0.5478\n",
      "Epoch 1096/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5485 - mse: 0.5419 - val_loss: 0.5549 - val_mse: 0.5483\n",
      "Epoch 1097/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5523 - mse: 0.5458 - val_loss: 0.5534 - val_mse: 0.5468\n",
      "Epoch 1098/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5528 - mse: 0.5462 - val_loss: 0.5536 - val_mse: 0.5470\n",
      "Epoch 1099/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5597 - mse: 0.5531 - val_loss: 0.5543 - val_mse: 0.5477\n",
      "Epoch 1100/2000\n",
      "266/277 [===========================>..] - ETA: 0s - loss: 0.5511 - mse: 0.5445\n",
      "Epoch 01100: saving model to Regression_Model/hepg2.mle.linear-1100.ckpt\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5478 - mse: 0.5412 - val_loss: 0.5548 - val_mse: 0.5482\n",
      "Epoch 1101/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5590 - mse: 0.5524 - val_loss: 0.5575 - val_mse: 0.5509\n",
      "Epoch 1102/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5522 - mse: 0.5456 - val_loss: 0.5547 - val_mse: 0.5482\n",
      "Epoch 1103/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5626 - mse: 0.5560 - val_loss: 0.5540 - val_mse: 0.5474\n",
      "Epoch 1104/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5538 - mse: 0.5472 - val_loss: 0.5540 - val_mse: 0.5474\n",
      "Epoch 1105/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5559 - mse: 0.5493 - val_loss: 0.5538 - val_mse: 0.5472\n",
      "Epoch 1106/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5490 - mse: 0.5424 - val_loss: 0.5544 - val_mse: 0.5478\n",
      "Epoch 1107/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5610 - mse: 0.5544 - val_loss: 0.5544 - val_mse: 0.5478\n",
      "Epoch 1108/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5567 - mse: 0.5502 - val_loss: 0.5555 - val_mse: 0.5489\n",
      "Epoch 1109/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5596 - mse: 0.5530 - val_loss: 0.5534 - val_mse: 0.5468\n",
      "Epoch 1110/2000\n",
      "272/277 [============================>.] - ETA: 0s - loss: 0.5538 - mse: 0.5472\n",
      "Epoch 01110: saving model to Regression_Model/hepg2.mle.linear-1110.ckpt\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5542 - mse: 0.5476 - val_loss: 0.5538 - val_mse: 0.5472\n",
      "Epoch 1111/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5550 - mse: 0.5484 - val_loss: 0.5552 - val_mse: 0.5487\n",
      "Epoch 1112/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5532 - mse: 0.5466 - val_loss: 0.5547 - val_mse: 0.5481\n",
      "Epoch 1113/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5537 - mse: 0.5471 - val_loss: 0.5542 - val_mse: 0.5476\n",
      "Epoch 1114/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5503 - mse: 0.5437 - val_loss: 0.5544 - val_mse: 0.5478\n",
      "Epoch 1115/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5576 - mse: 0.5510 - val_loss: 0.5542 - val_mse: 0.5476\n",
      "Epoch 1116/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5532 - mse: 0.5466 - val_loss: 0.5535 - val_mse: 0.5469\n",
      "Epoch 1117/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5590 - mse: 0.5524 - val_loss: 0.5567 - val_mse: 0.5502\n",
      "Epoch 1118/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5579 - mse: 0.5513 - val_loss: 0.5544 - val_mse: 0.5478\n",
      "Epoch 1119/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5451 - mse: 0.5386 - val_loss: 0.5541 - val_mse: 0.5475\n",
      "Epoch 1120/2000\n",
      "276/277 [============================>.] - ETA: 0s - loss: 0.5490 - mse: 0.5424\n",
      "Epoch 01120: saving model to Regression_Model/hepg2.mle.linear-1120.ckpt\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5497 - mse: 0.5432 - val_loss: 0.5539 - val_mse: 0.5473\n",
      "Epoch 1121/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5489 - mse: 0.5423 - val_loss: 0.5553 - val_mse: 0.5488\n",
      "Epoch 1122/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5538 - mse: 0.5473 - val_loss: 0.5554 - val_mse: 0.5489\n",
      "Epoch 1123/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5567 - mse: 0.5501 - val_loss: 0.5545 - val_mse: 0.5479\n",
      "Epoch 1124/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5482 - mse: 0.5416 - val_loss: 0.5553 - val_mse: 0.5487\n",
      "Epoch 1125/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5573 - mse: 0.5508 - val_loss: 0.5553 - val_mse: 0.5487\n",
      "Epoch 1126/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5528 - mse: 0.5462 - val_loss: 0.5560 - val_mse: 0.5494\n",
      "Epoch 1127/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5511 - mse: 0.5445 - val_loss: 0.5543 - val_mse: 0.5478\n",
      "Epoch 1128/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5637 - mse: 0.5571 - val_loss: 0.5584 - val_mse: 0.5519\n",
      "Epoch 1129/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5565 - mse: 0.5499 - val_loss: 0.5552 - val_mse: 0.5487\n",
      "Epoch 1130/2000\n",
      "276/277 [============================>.] - ETA: 0s - loss: 0.5614 - mse: 0.5548\n",
      "Epoch 01130: saving model to Regression_Model/hepg2.mle.linear-1130.ckpt\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5624 - mse: 0.5558 - val_loss: 0.5538 - val_mse: 0.5472\n",
      "Epoch 1131/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5555 - mse: 0.5489 - val_loss: 0.5547 - val_mse: 0.5481\n",
      "Epoch 1132/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5598 - mse: 0.5532 - val_loss: 0.5547 - val_mse: 0.5481\n",
      "Epoch 1133/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5541 - mse: 0.5475 - val_loss: 0.5533 - val_mse: 0.5468\n",
      "Epoch 1134/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5537 - mse: 0.5472 - val_loss: 0.5545 - val_mse: 0.5479\n",
      "Epoch 1135/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5559 - mse: 0.5494 - val_loss: 0.5537 - val_mse: 0.5472\n",
      "Epoch 1136/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5525 - mse: 0.5459 - val_loss: 0.5535 - val_mse: 0.5469\n",
      "Epoch 1137/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5543 - mse: 0.5478 - val_loss: 0.5557 - val_mse: 0.5491\n",
      "Epoch 1138/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5588 - mse: 0.5522 - val_loss: 0.5571 - val_mse: 0.5505\n",
      "Epoch 1139/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5568 - mse: 0.5502 - val_loss: 0.5540 - val_mse: 0.5475\n",
      "Epoch 1140/2000\n",
      "274/277 [============================>.] - ETA: 0s - loss: 0.5551 - mse: 0.5485\n",
      "Epoch 01140: saving model to Regression_Model/hepg2.mle.linear-1140.ckpt\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5537 - mse: 0.5472 - val_loss: 0.5541 - val_mse: 0.5475\n",
      "Epoch 1141/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5592 - mse: 0.5526 - val_loss: 0.5534 - val_mse: 0.5468\n",
      "Epoch 1142/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5531 - mse: 0.5466 - val_loss: 0.5537 - val_mse: 0.5472\n",
      "Epoch 1143/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5546 - mse: 0.5480 - val_loss: 0.5583 - val_mse: 0.5518\n",
      "Epoch 1144/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5572 - mse: 0.5506 - val_loss: 0.5530 - val_mse: 0.5464\n",
      "Epoch 1145/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5569 - mse: 0.5503 - val_loss: 0.5544 - val_mse: 0.5478\n",
      "Epoch 1146/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5519 - mse: 0.5453 - val_loss: 0.5538 - val_mse: 0.5473\n",
      "Epoch 1147/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5538 - mse: 0.5472 - val_loss: 0.5547 - val_mse: 0.5482\n",
      "Epoch 1148/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5557 - mse: 0.5492 - val_loss: 0.5533 - val_mse: 0.5467\n",
      "Epoch 1149/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5524 - mse: 0.5459 - val_loss: 0.5555 - val_mse: 0.5489\n",
      "Epoch 1150/2000\n",
      "270/277 [============================>.] - ETA: 0s - loss: 0.5539 - mse: 0.5474\n",
      "Epoch 01150: saving model to Regression_Model/hepg2.mle.linear-1150.ckpt\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5533 - mse: 0.5467 - val_loss: 0.5547 - val_mse: 0.5482\n",
      "Epoch 1151/2000\n",
      "277/277 [==============================] - 2s 6ms/step - loss: 0.5545 - mse: 0.5479 - val_loss: 0.5538 - val_mse: 0.5473\n",
      "Epoch 1152/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5531 - mse: 0.5466 - val_loss: 0.5537 - val_mse: 0.5472\n",
      "Epoch 1153/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5485 - mse: 0.5419 - val_loss: 0.5535 - val_mse: 0.5470\n",
      "Epoch 1154/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5533 - mse: 0.5468 - val_loss: 0.5538 - val_mse: 0.5473\n",
      "Epoch 1155/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5575 - mse: 0.5509 - val_loss: 0.5547 - val_mse: 0.5481\n",
      "Epoch 1156/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5540 - mse: 0.5474 - val_loss: 0.5548 - val_mse: 0.5483\n",
      "Epoch 1157/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5514 - mse: 0.5449 - val_loss: 0.5566 - val_mse: 0.5500\n",
      "Epoch 1158/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5477 - mse: 0.5412 - val_loss: 0.5546 - val_mse: 0.5480\n",
      "Epoch 1159/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5577 - mse: 0.5511 - val_loss: 0.5564 - val_mse: 0.5499\n",
      "Epoch 1160/2000\n",
      "270/277 [============================>.] - ETA: 0s - loss: 0.5500 - mse: 0.5435\n",
      "Epoch 01160: saving model to Regression_Model/hepg2.mle.linear-1160.ckpt\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5522 - mse: 0.5457 - val_loss: 0.5542 - val_mse: 0.5477\n",
      "Epoch 1161/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5532 - mse: 0.5466 - val_loss: 0.5540 - val_mse: 0.5474\n",
      "Epoch 1162/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5526 - mse: 0.5460 - val_loss: 0.5546 - val_mse: 0.5481\n",
      "Epoch 1163/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5597 - mse: 0.5532 - val_loss: 0.5549 - val_mse: 0.5483\n",
      "Epoch 1164/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5506 - mse: 0.5441 - val_loss: 0.5562 - val_mse: 0.5496\n",
      "Epoch 1165/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5574 - mse: 0.5509 - val_loss: 0.5546 - val_mse: 0.5481\n",
      "Epoch 1166/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5602 - mse: 0.5537 - val_loss: 0.5547 - val_mse: 0.5481\n",
      "Epoch 1167/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5531 - mse: 0.5466 - val_loss: 0.5545 - val_mse: 0.5480\n",
      "Epoch 1168/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5583 - mse: 0.5518 - val_loss: 0.5544 - val_mse: 0.5479\n",
      "Epoch 1169/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5567 - mse: 0.5501 - val_loss: 0.5563 - val_mse: 0.5498\n",
      "Epoch 1170/2000\n",
      "273/277 [============================>.] - ETA: 0s - loss: 0.5518 - mse: 0.5452\n",
      "Epoch 01170: saving model to Regression_Model/hepg2.mle.linear-1170.ckpt\n",
      "277/277 [==============================] - 2s 7ms/step - loss: 0.5523 - mse: 0.5458 - val_loss: 0.5549 - val_mse: 0.5484\n",
      "Epoch 1171/2000\n",
      "277/277 [==============================] - 2s 5ms/step - loss: 0.5601 - mse: 0.5535 - val_loss: 0.5559 - val_mse: 0.5493\n",
      "Epoch 1172/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5583 - mse: 0.5517 - val_loss: 0.5560 - val_mse: 0.5495\n",
      "Epoch 1173/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5572 - mse: 0.5507 - val_loss: 0.5560 - val_mse: 0.5494\n",
      "Epoch 1174/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5557 - mse: 0.5492 - val_loss: 0.5544 - val_mse: 0.5479\n",
      "Epoch 1175/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5539 - mse: 0.5474 - val_loss: 0.5548 - val_mse: 0.5482\n",
      "Epoch 1176/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5534 - mse: 0.5469 - val_loss: 0.5551 - val_mse: 0.5485\n",
      "Epoch 1177/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5528 - mse: 0.5462 - val_loss: 0.5545 - val_mse: 0.5480\n",
      "Epoch 1178/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5479 - mse: 0.5414 - val_loss: 0.5545 - val_mse: 0.5480\n",
      "Epoch 1179/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5502 - mse: 0.5436 - val_loss: 0.5542 - val_mse: 0.5477\n",
      "Epoch 1180/2000\n",
      "271/277 [============================>.] - ETA: 0s - loss: 0.5584 - mse: 0.5519\n",
      "Epoch 01180: saving model to Regression_Model/hepg2.mle.linear-1180.ckpt\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5570 - mse: 0.5505 - val_loss: 0.5540 - val_mse: 0.5475\n",
      "Epoch 1181/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5535 - mse: 0.5470 - val_loss: 0.5537 - val_mse: 0.5472\n",
      "Epoch 1182/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5505 - mse: 0.5440 - val_loss: 0.5534 - val_mse: 0.5468\n",
      "Epoch 1183/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5589 - mse: 0.5523 - val_loss: 0.5549 - val_mse: 0.5483\n",
      "Epoch 1184/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5584 - mse: 0.5519 - val_loss: 0.5575 - val_mse: 0.5510\n",
      "Epoch 1185/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5481 - mse: 0.5416 - val_loss: 0.5552 - val_mse: 0.5487\n",
      "Epoch 1186/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5607 - mse: 0.5542 - val_loss: 0.5543 - val_mse: 0.5478\n",
      "Epoch 1187/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5531 - mse: 0.5466 - val_loss: 0.5552 - val_mse: 0.5487\n",
      "Epoch 1188/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5527 - mse: 0.5461 - val_loss: 0.5537 - val_mse: 0.5472\n",
      "Epoch 1189/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5554 - mse: 0.5489 - val_loss: 0.5550 - val_mse: 0.5485\n",
      "Epoch 1190/2000\n",
      "269/277 [============================>.] - ETA: 0s - loss: 0.5531 - mse: 0.5465\n",
      "Epoch 01190: saving model to Regression_Model/hepg2.mle.linear-1190.ckpt\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5526 - mse: 0.5461 - val_loss: 0.5545 - val_mse: 0.5480\n",
      "Epoch 1191/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5599 - mse: 0.5533 - val_loss: 0.5570 - val_mse: 0.5505\n",
      "Epoch 1192/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5596 - mse: 0.5531 - val_loss: 0.5557 - val_mse: 0.5491\n",
      "Epoch 1193/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5588 - mse: 0.5523 - val_loss: 0.5544 - val_mse: 0.5479\n",
      "Epoch 1194/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5534 - mse: 0.5468 - val_loss: 0.5549 - val_mse: 0.5484\n",
      "Epoch 1195/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5583 - mse: 0.5517 - val_loss: 0.5542 - val_mse: 0.5477\n",
      "Epoch 1196/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5559 - mse: 0.5494 - val_loss: 0.5538 - val_mse: 0.5472\n",
      "Epoch 1197/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5494 - mse: 0.5429 - val_loss: 0.5532 - val_mse: 0.5467\n",
      "Epoch 1198/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5443 - mse: 0.5378 - val_loss: 0.5544 - val_mse: 0.5478\n",
      "Epoch 1199/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5466 - mse: 0.5401 - val_loss: 0.5544 - val_mse: 0.5479\n",
      "Epoch 1200/2000\n",
      "274/277 [============================>.] - ETA: 0s - loss: 0.5511 - mse: 0.5446\n",
      "Epoch 01200: saving model to Regression_Model/hepg2.mle.linear-1200.ckpt\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5505 - mse: 0.5440 - val_loss: 0.5541 - val_mse: 0.5476\n",
      "Epoch 1201/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5486 - mse: 0.5421 - val_loss: 0.5548 - val_mse: 0.5483\n",
      "Epoch 1202/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5560 - mse: 0.5495 - val_loss: 0.5535 - val_mse: 0.5470\n",
      "Epoch 1203/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5546 - mse: 0.5481 - val_loss: 0.5540 - val_mse: 0.5475\n",
      "Epoch 1204/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5508 - mse: 0.5442 - val_loss: 0.5552 - val_mse: 0.5487\n",
      "Epoch 1205/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5538 - mse: 0.5473 - val_loss: 0.5550 - val_mse: 0.5485\n",
      "Epoch 1206/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5521 - mse: 0.5456 - val_loss: 0.5543 - val_mse: 0.5478\n",
      "Epoch 1207/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5546 - mse: 0.5481 - val_loss: 0.5529 - val_mse: 0.5464\n",
      "Epoch 1208/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5597 - mse: 0.5532 - val_loss: 0.5557 - val_mse: 0.5491\n",
      "Epoch 1209/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5541 - mse: 0.5476 - val_loss: 0.5532 - val_mse: 0.5467\n",
      "Epoch 1210/2000\n",
      "275/277 [============================>.] - ETA: 0s - loss: 0.5553 - mse: 0.5487\n",
      "Epoch 01210: saving model to Regression_Model/hepg2.mle.linear-1210.ckpt\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5544 - mse: 0.5479 - val_loss: 0.5548 - val_mse: 0.5482\n",
      "Epoch 1211/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5556 - mse: 0.5491 - val_loss: 0.5553 - val_mse: 0.5488\n",
      "Epoch 1212/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5515 - mse: 0.5450 - val_loss: 0.5536 - val_mse: 0.5471\n",
      "Epoch 1213/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5439 - mse: 0.5373 - val_loss: 0.5566 - val_mse: 0.5501\n",
      "Epoch 1214/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5547 - mse: 0.5481 - val_loss: 0.5540 - val_mse: 0.5475\n",
      "Epoch 1215/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5579 - mse: 0.5514 - val_loss: 0.5552 - val_mse: 0.5487\n",
      "Epoch 1216/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5527 - mse: 0.5462 - val_loss: 0.5554 - val_mse: 0.5489\n",
      "Epoch 1217/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5615 - mse: 0.5550 - val_loss: 0.5562 - val_mse: 0.5497\n",
      "Epoch 1218/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5537 - mse: 0.5472 - val_loss: 0.5548 - val_mse: 0.5483\n",
      "Epoch 1219/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5578 - mse: 0.5513 - val_loss: 0.5560 - val_mse: 0.5495\n",
      "Epoch 1220/2000\n",
      "267/277 [===========================>..] - ETA: 0s - loss: 0.5533 - mse: 0.5468\n",
      "Epoch 01220: saving model to Regression_Model/hepg2.mle.linear-1220.ckpt\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5542 - mse: 0.5477 - val_loss: 0.5541 - val_mse: 0.5476\n",
      "Epoch 1221/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5536 - mse: 0.5471 - val_loss: 0.5540 - val_mse: 0.5475\n",
      "Epoch 1222/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5517 - mse: 0.5452 - val_loss: 0.5553 - val_mse: 0.5488\n",
      "Epoch 1223/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5541 - mse: 0.5476 - val_loss: 0.5553 - val_mse: 0.5488\n",
      "Epoch 1224/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5533 - mse: 0.5468 - val_loss: 0.5546 - val_mse: 0.5481\n",
      "Epoch 1225/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5586 - mse: 0.5521 - val_loss: 0.5549 - val_mse: 0.5484\n",
      "Epoch 1226/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5570 - mse: 0.5505 - val_loss: 0.5550 - val_mse: 0.5485\n",
      "Epoch 1227/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5633 - mse: 0.5568 - val_loss: 0.5567 - val_mse: 0.5502\n",
      "Epoch 1228/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5529 - mse: 0.5464 - val_loss: 0.5548 - val_mse: 0.5483\n",
      "Epoch 1229/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5491 - mse: 0.5426 - val_loss: 0.5547 - val_mse: 0.5482\n",
      "Epoch 1230/2000\n",
      "275/277 [============================>.] - ETA: 0s - loss: 0.5514 - mse: 0.5449\n",
      "Epoch 01230: saving model to Regression_Model/hepg2.mle.linear-1230.ckpt\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5507 - mse: 0.5442 - val_loss: 0.5545 - val_mse: 0.5480\n",
      "Epoch 1231/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5525 - mse: 0.5460 - val_loss: 0.5548 - val_mse: 0.5483\n",
      "Epoch 1232/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5527 - mse: 0.5462 - val_loss: 0.5556 - val_mse: 0.5492\n",
      "Epoch 1233/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5598 - mse: 0.5533 - val_loss: 0.5553 - val_mse: 0.5488\n",
      "Epoch 1234/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5555 - mse: 0.5490 - val_loss: 0.5551 - val_mse: 0.5486\n",
      "Epoch 1235/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5529 - mse: 0.5464 - val_loss: 0.5554 - val_mse: 0.5489\n",
      "Epoch 1236/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5477 - mse: 0.5412 - val_loss: 0.5543 - val_mse: 0.5478\n",
      "Epoch 1237/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5535 - mse: 0.5470 - val_loss: 0.5553 - val_mse: 0.5488\n",
      "Epoch 1238/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5524 - mse: 0.5459 - val_loss: 0.5536 - val_mse: 0.5472\n",
      "Epoch 1239/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5552 - mse: 0.5487 - val_loss: 0.5547 - val_mse: 0.5482\n",
      "Epoch 1240/2000\n",
      "268/277 [============================>.] - ETA: 0s - loss: 0.5558 - mse: 0.5493\n",
      "Epoch 01240: saving model to Regression_Model/hepg2.mle.linear-1240.ckpt\n",
      "277/277 [==============================] - 2s 5ms/step - loss: 0.5546 - mse: 0.5481 - val_loss: 0.5552 - val_mse: 0.5487\n",
      "Epoch 1241/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5557 - mse: 0.5492 - val_loss: 0.5553 - val_mse: 0.5488\n",
      "Epoch 1242/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5460 - mse: 0.5395 - val_loss: 0.5537 - val_mse: 0.5472\n",
      "Epoch 1243/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5596 - mse: 0.5531 - val_loss: 0.5551 - val_mse: 0.5486\n",
      "Epoch 1244/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5558 - mse: 0.5493 - val_loss: 0.5546 - val_mse: 0.5481\n",
      "Epoch 1245/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5576 - mse: 0.5511 - val_loss: 0.5549 - val_mse: 0.5484\n",
      "Epoch 1246/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5520 - mse: 0.5455 - val_loss: 0.5556 - val_mse: 0.5491\n",
      "Epoch 1247/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5532 - mse: 0.5468 - val_loss: 0.5564 - val_mse: 0.5500\n",
      "Epoch 1248/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5563 - mse: 0.5499 - val_loss: 0.5556 - val_mse: 0.5491\n",
      "Epoch 1249/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5525 - mse: 0.5460 - val_loss: 0.5555 - val_mse: 0.5490\n",
      "Epoch 1250/2000\n",
      "266/277 [===========================>..] - ETA: 0s - loss: 0.5472 - mse: 0.5407\n",
      "Epoch 01250: saving model to Regression_Model/hepg2.mle.linear-1250.ckpt\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5504 - mse: 0.5439 - val_loss: 0.5544 - val_mse: 0.5479\n",
      "Epoch 1251/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5519 - mse: 0.5454 - val_loss: 0.5560 - val_mse: 0.5495\n",
      "Epoch 1252/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5519 - mse: 0.5454 - val_loss: 0.5552 - val_mse: 0.5488\n",
      "Epoch 1253/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5561 - mse: 0.5496 - val_loss: 0.5552 - val_mse: 0.5487\n",
      "Epoch 1254/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5540 - mse: 0.5475 - val_loss: 0.5560 - val_mse: 0.5495\n",
      "Epoch 1255/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5466 - mse: 0.5401 - val_loss: 0.5556 - val_mse: 0.5491\n",
      "Epoch 1256/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5541 - mse: 0.5476 - val_loss: 0.5563 - val_mse: 0.5498\n",
      "Epoch 1257/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5523 - mse: 0.5459 - val_loss: 0.5559 - val_mse: 0.5494\n",
      "Epoch 1258/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5485 - mse: 0.5420 - val_loss: 0.5544 - val_mse: 0.5480\n",
      "Epoch 1259/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5492 - mse: 0.5428 - val_loss: 0.5555 - val_mse: 0.5491\n",
      "Epoch 1260/2000\n",
      "273/277 [============================>.] - ETA: 0s - loss: 0.5508 - mse: 0.5443\n",
      "Epoch 01260: saving model to Regression_Model/hepg2.mle.linear-1260.ckpt\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5501 - mse: 0.5437 - val_loss: 0.5551 - val_mse: 0.5486\n",
      "Epoch 1261/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5518 - mse: 0.5453 - val_loss: 0.5540 - val_mse: 0.5475\n",
      "Epoch 1262/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5542 - mse: 0.5477 - val_loss: 0.5541 - val_mse: 0.5476\n",
      "Epoch 1263/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5573 - mse: 0.5508 - val_loss: 0.5552 - val_mse: 0.5488\n",
      "Epoch 1264/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5543 - mse: 0.5478 - val_loss: 0.5565 - val_mse: 0.5500\n",
      "Epoch 1265/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5546 - mse: 0.5481 - val_loss: 0.5542 - val_mse: 0.5477\n",
      "Epoch 1266/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5489 - mse: 0.5424 - val_loss: 0.5570 - val_mse: 0.5506\n",
      "Epoch 1267/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5535 - mse: 0.5471 - val_loss: 0.5545 - val_mse: 0.5480\n",
      "Epoch 1268/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5560 - mse: 0.5496 - val_loss: 0.5557 - val_mse: 0.5492\n",
      "Epoch 1269/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5479 - mse: 0.5414 - val_loss: 0.5549 - val_mse: 0.5484\n",
      "Epoch 1270/2000\n",
      "274/277 [============================>.] - ETA: 0s - loss: 0.5523 - mse: 0.5458\n",
      "Epoch 01270: saving model to Regression_Model/hepg2.mle.linear-1270.ckpt\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5521 - mse: 0.5456 - val_loss: 0.5552 - val_mse: 0.5487\n",
      "Epoch 1271/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5579 - mse: 0.5515 - val_loss: 0.5551 - val_mse: 0.5486\n",
      "Epoch 1272/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5519 - mse: 0.5454 - val_loss: 0.5568 - val_mse: 0.5504\n",
      "Epoch 1273/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5481 - mse: 0.5416 - val_loss: 0.5575 - val_mse: 0.5511\n",
      "Epoch 1274/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5504 - mse: 0.5439 - val_loss: 0.5551 - val_mse: 0.5487\n",
      "Epoch 1275/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5544 - mse: 0.5479 - val_loss: 0.5550 - val_mse: 0.5485\n",
      "Epoch 1276/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5488 - mse: 0.5424 - val_loss: 0.5544 - val_mse: 0.5479\n",
      "Epoch 1277/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5597 - mse: 0.5532 - val_loss: 0.5546 - val_mse: 0.5481\n",
      "Epoch 1278/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5547 - mse: 0.5483 - val_loss: 0.5547 - val_mse: 0.5482\n",
      "Epoch 1279/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5561 - mse: 0.5496 - val_loss: 0.5558 - val_mse: 0.5494\n",
      "Epoch 1280/2000\n",
      "274/277 [============================>.] - ETA: 0s - loss: 0.5466 - mse: 0.5402\n",
      "Epoch 01280: saving model to Regression_Model/hepg2.mle.linear-1280.ckpt\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5456 - mse: 0.5391 - val_loss: 0.5549 - val_mse: 0.5484\n",
      "Epoch 1281/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5537 - mse: 0.5472 - val_loss: 0.5553 - val_mse: 0.5488\n",
      "Epoch 1282/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5456 - mse: 0.5392 - val_loss: 0.5546 - val_mse: 0.5481\n",
      "Epoch 1283/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5541 - mse: 0.5477 - val_loss: 0.5558 - val_mse: 0.5494\n",
      "Epoch 1284/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5458 - mse: 0.5394 - val_loss: 0.5540 - val_mse: 0.5476\n",
      "Epoch 1285/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5546 - mse: 0.5482 - val_loss: 0.5551 - val_mse: 0.5487\n",
      "Epoch 1286/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5472 - mse: 0.5407 - val_loss: 0.5542 - val_mse: 0.5477\n",
      "Epoch 1287/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5561 - mse: 0.5497 - val_loss: 0.5550 - val_mse: 0.5485\n",
      "Epoch 1288/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5502 - mse: 0.5438 - val_loss: 0.5545 - val_mse: 0.5480\n",
      "Epoch 1289/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5499 - mse: 0.5434 - val_loss: 0.5559 - val_mse: 0.5495\n",
      "Epoch 1290/2000\n",
      "266/277 [===========================>..] - ETA: 0s - loss: 0.5504 - mse: 0.5439\n",
      "Epoch 01290: saving model to Regression_Model/hepg2.mle.linear-1290.ckpt\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5514 - mse: 0.5450 - val_loss: 0.5545 - val_mse: 0.5480\n",
      "Epoch 1291/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5499 - mse: 0.5434 - val_loss: 0.5546 - val_mse: 0.5482\n",
      "Epoch 1292/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5531 - mse: 0.5467 - val_loss: 0.5556 - val_mse: 0.5491\n",
      "Epoch 1293/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5606 - mse: 0.5542 - val_loss: 0.5561 - val_mse: 0.5497\n",
      "Epoch 1294/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5489 - mse: 0.5424 - val_loss: 0.5541 - val_mse: 0.5476\n",
      "Epoch 1295/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5514 - mse: 0.5450 - val_loss: 0.5549 - val_mse: 0.5485\n",
      "Epoch 1296/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5585 - mse: 0.5520 - val_loss: 0.5550 - val_mse: 0.5485\n",
      "Epoch 1297/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5572 - mse: 0.5507 - val_loss: 0.5541 - val_mse: 0.5477\n",
      "Epoch 1298/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5526 - mse: 0.5461 - val_loss: 0.5544 - val_mse: 0.5480\n",
      "Epoch 1299/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5524 - mse: 0.5459 - val_loss: 0.5537 - val_mse: 0.5473\n",
      "Epoch 1300/2000\n",
      "265/277 [===========================>..] - ETA: 0s - loss: 0.5555 - mse: 0.5490\n",
      "Epoch 01300: saving model to Regression_Model/hepg2.mle.linear-1300.ckpt\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5564 - mse: 0.5500 - val_loss: 0.5537 - val_mse: 0.5472\n",
      "Epoch 1301/2000\n",
      "277/277 [==============================] - 2s 6ms/step - loss: 0.5547 - mse: 0.5483 - val_loss: 0.5541 - val_mse: 0.5476\n",
      "Epoch 1302/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5571 - mse: 0.5507 - val_loss: 0.5545 - val_mse: 0.5481\n",
      "Epoch 1303/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5536 - mse: 0.5471 - val_loss: 0.5545 - val_mse: 0.5481\n",
      "Epoch 1304/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5456 - mse: 0.5391 - val_loss: 0.5535 - val_mse: 0.5470\n",
      "Epoch 1305/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5546 - mse: 0.5481 - val_loss: 0.5551 - val_mse: 0.5487\n",
      "Epoch 1306/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5518 - mse: 0.5454 - val_loss: 0.5531 - val_mse: 0.5467\n",
      "Epoch 1307/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5550 - mse: 0.5485 - val_loss: 0.5554 - val_mse: 0.5489\n",
      "Epoch 1308/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5562 - mse: 0.5498 - val_loss: 0.5555 - val_mse: 0.5491\n",
      "Epoch 1309/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5548 - mse: 0.5483 - val_loss: 0.5534 - val_mse: 0.5469\n",
      "Epoch 1310/2000\n",
      "276/277 [============================>.] - ETA: 0s - loss: 0.5476 - mse: 0.5412\n",
      "Epoch 01310: saving model to Regression_Model/hepg2.mle.linear-1310.ckpt\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5487 - mse: 0.5422 - val_loss: 0.5553 - val_mse: 0.5488\n",
      "Epoch 1311/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5520 - mse: 0.5456 - val_loss: 0.5547 - val_mse: 0.5482\n",
      "Epoch 1312/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5549 - mse: 0.5485 - val_loss: 0.5548 - val_mse: 0.5484\n",
      "Epoch 1313/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5495 - mse: 0.5431 - val_loss: 0.5539 - val_mse: 0.5475\n",
      "Epoch 1314/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5535 - mse: 0.5471 - val_loss: 0.5546 - val_mse: 0.5481\n",
      "Epoch 1315/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5553 - mse: 0.5489 - val_loss: 0.5542 - val_mse: 0.5478\n",
      "Epoch 1316/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5536 - mse: 0.5472 - val_loss: 0.5555 - val_mse: 0.5491\n",
      "Epoch 1317/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5566 - mse: 0.5501 - val_loss: 0.5552 - val_mse: 0.5488\n",
      "Epoch 1318/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5452 - mse: 0.5388 - val_loss: 0.5558 - val_mse: 0.5494\n",
      "Epoch 1319/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5562 - mse: 0.5497 - val_loss: 0.5555 - val_mse: 0.5490\n",
      "Epoch 1320/2000\n",
      "266/277 [===========================>..] - ETA: 0s - loss: 0.5611 - mse: 0.5546\n",
      "Epoch 01320: saving model to Regression_Model/hepg2.mle.linear-1320.ckpt\n",
      "277/277 [==============================] - 2s 6ms/step - loss: 0.5584 - mse: 0.5519 - val_loss: 0.5554 - val_mse: 0.5490\n",
      "Epoch 1321/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5572 - mse: 0.5507 - val_loss: 0.5569 - val_mse: 0.5505\n",
      "Epoch 1322/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5550 - mse: 0.5486 - val_loss: 0.5553 - val_mse: 0.5489\n",
      "Epoch 1323/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5569 - mse: 0.5504 - val_loss: 0.5554 - val_mse: 0.5489\n",
      "Epoch 1324/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5597 - mse: 0.5533 - val_loss: 0.5557 - val_mse: 0.5492\n",
      "Epoch 1325/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5479 - mse: 0.5415 - val_loss: 0.5547 - val_mse: 0.5483\n",
      "Epoch 1326/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5521 - mse: 0.5457 - val_loss: 0.5552 - val_mse: 0.5487\n",
      "Epoch 1327/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5600 - mse: 0.5536 - val_loss: 0.5553 - val_mse: 0.5489\n",
      "Epoch 1328/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5486 - mse: 0.5422 - val_loss: 0.5546 - val_mse: 0.5482\n",
      "Epoch 1329/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5512 - mse: 0.5447 - val_loss: 0.5547 - val_mse: 0.5482\n",
      "Epoch 1330/2000\n",
      "272/277 [============================>.] - ETA: 0s - loss: 0.5569 - mse: 0.5505\n",
      "Epoch 01330: saving model to Regression_Model/hepg2.mle.linear-1330.ckpt\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5567 - mse: 0.5502 - val_loss: 0.5562 - val_mse: 0.5497\n",
      "Epoch 1331/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5522 - mse: 0.5457 - val_loss: 0.5542 - val_mse: 0.5477\n",
      "Epoch 1332/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5558 - mse: 0.5493 - val_loss: 0.5557 - val_mse: 0.5492\n",
      "Epoch 1333/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5521 - mse: 0.5456 - val_loss: 0.5554 - val_mse: 0.5490\n",
      "Epoch 1334/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5488 - mse: 0.5423 - val_loss: 0.5572 - val_mse: 0.5507\n",
      "Epoch 1335/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5543 - mse: 0.5478 - val_loss: 0.5555 - val_mse: 0.5491\n",
      "Epoch 1336/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5526 - mse: 0.5462 - val_loss: 0.5549 - val_mse: 0.5484\n",
      "Epoch 1337/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5435 - mse: 0.5370 - val_loss: 0.5554 - val_mse: 0.5489\n",
      "Epoch 1338/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5442 - mse: 0.5378 - val_loss: 0.5550 - val_mse: 0.5486\n",
      "Epoch 1339/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5551 - mse: 0.5486 - val_loss: 0.5555 - val_mse: 0.5491\n",
      "Epoch 1340/2000\n",
      "268/277 [============================>.] - ETA: 0s - loss: 0.5589 - mse: 0.5524\n",
      "Epoch 01340: saving model to Regression_Model/hepg2.mle.linear-1340.ckpt\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5579 - mse: 0.5514 - val_loss: 0.5554 - val_mse: 0.5489\n",
      "Epoch 1341/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5538 - mse: 0.5473 - val_loss: 0.5555 - val_mse: 0.5491\n",
      "Epoch 1342/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5568 - mse: 0.5503 - val_loss: 0.5564 - val_mse: 0.5499\n",
      "Epoch 1343/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5529 - mse: 0.5465 - val_loss: 0.5548 - val_mse: 0.5483\n",
      "Epoch 1344/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5533 - mse: 0.5469 - val_loss: 0.5568 - val_mse: 0.5504\n",
      "Epoch 1345/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5508 - mse: 0.5443 - val_loss: 0.5556 - val_mse: 0.5492\n",
      "Epoch 1346/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5630 - mse: 0.5566 - val_loss: 0.5562 - val_mse: 0.5497\n",
      "Epoch 1347/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5520 - mse: 0.5456 - val_loss: 0.5550 - val_mse: 0.5486\n",
      "Epoch 1348/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5525 - mse: 0.5460 - val_loss: 0.5563 - val_mse: 0.5499\n",
      "Epoch 1349/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5498 - mse: 0.5434 - val_loss: 0.5570 - val_mse: 0.5506\n",
      "Epoch 1350/2000\n",
      "270/277 [============================>.] - ETA: 0s - loss: 0.5478 - mse: 0.5413\n",
      "Epoch 01350: saving model to Regression_Model/hepg2.mle.linear-1350.ckpt\n",
      "277/277 [==============================] - 2s 6ms/step - loss: 0.5481 - mse: 0.5417 - val_loss: 0.5559 - val_mse: 0.5495\n",
      "Epoch 1351/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5523 - mse: 0.5459 - val_loss: 0.5547 - val_mse: 0.5483\n",
      "Epoch 1352/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5479 - mse: 0.5415 - val_loss: 0.5557 - val_mse: 0.5493\n",
      "Epoch 1353/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5523 - mse: 0.5459 - val_loss: 0.5557 - val_mse: 0.5493\n",
      "Epoch 1354/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5494 - mse: 0.5430 - val_loss: 0.5556 - val_mse: 0.5491\n",
      "Epoch 1355/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5529 - mse: 0.5465 - val_loss: 0.5543 - val_mse: 0.5478\n",
      "Epoch 1356/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5510 - mse: 0.5446 - val_loss: 0.5544 - val_mse: 0.5479\n",
      "Epoch 1357/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5564 - mse: 0.5500 - val_loss: 0.5549 - val_mse: 0.5485\n",
      "Epoch 1358/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5576 - mse: 0.5512 - val_loss: 0.5543 - val_mse: 0.5479\n",
      "Epoch 1359/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5548 - mse: 0.5484 - val_loss: 0.5544 - val_mse: 0.5479\n",
      "Epoch 1360/2000\n",
      "268/277 [============================>.] - ETA: 0s - loss: 0.5605 - mse: 0.5540\n",
      "Epoch 01360: saving model to Regression_Model/hepg2.mle.linear-1360.ckpt\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5605 - mse: 0.5540 - val_loss: 0.5548 - val_mse: 0.5484\n",
      "Epoch 1361/2000\n",
      "277/277 [==============================] - 2s 5ms/step - loss: 0.5500 - mse: 0.5436 - val_loss: 0.5555 - val_mse: 0.5491\n",
      "Epoch 1362/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5478 - mse: 0.5414 - val_loss: 0.5550 - val_mse: 0.5486\n",
      "Epoch 1363/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5444 - mse: 0.5380 - val_loss: 0.5553 - val_mse: 0.5489\n",
      "Epoch 1364/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5487 - mse: 0.5423 - val_loss: 0.5539 - val_mse: 0.5475\n",
      "Epoch 1365/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5546 - mse: 0.5481 - val_loss: 0.5544 - val_mse: 0.5480\n",
      "Epoch 1366/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5533 - mse: 0.5469 - val_loss: 0.5545 - val_mse: 0.5481\n",
      "Epoch 1367/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5496 - mse: 0.5432 - val_loss: 0.5561 - val_mse: 0.5497\n",
      "Epoch 1368/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5557 - mse: 0.5493 - val_loss: 0.5558 - val_mse: 0.5493\n",
      "Epoch 1369/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5543 - mse: 0.5479 - val_loss: 0.5548 - val_mse: 0.5484\n",
      "Epoch 1370/2000\n",
      "263/277 [===========================>..] - ETA: 0s - loss: 0.5612 - mse: 0.5548\n",
      "Epoch 01370: saving model to Regression_Model/hepg2.mle.linear-1370.ckpt\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5588 - mse: 0.5523 - val_loss: 0.5548 - val_mse: 0.5484\n",
      "Epoch 1371/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5563 - mse: 0.5499 - val_loss: 0.5549 - val_mse: 0.5485\n",
      "Epoch 1372/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5517 - mse: 0.5453 - val_loss: 0.5556 - val_mse: 0.5492\n",
      "Epoch 1373/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5468 - mse: 0.5404 - val_loss: 0.5550 - val_mse: 0.5486\n",
      "Epoch 1374/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5486 - mse: 0.5422 - val_loss: 0.5560 - val_mse: 0.5496\n",
      "Epoch 1375/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5517 - mse: 0.5452 - val_loss: 0.5555 - val_mse: 0.5490\n",
      "Epoch 1376/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5527 - mse: 0.5462 - val_loss: 0.5554 - val_mse: 0.5490\n",
      "Epoch 1377/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5481 - mse: 0.5417 - val_loss: 0.5545 - val_mse: 0.5481\n",
      "Epoch 1378/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5454 - mse: 0.5390 - val_loss: 0.5561 - val_mse: 0.5497\n",
      "Epoch 1379/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5545 - mse: 0.5481 - val_loss: 0.5546 - val_mse: 0.5482\n",
      "Epoch 1380/2000\n",
      "272/277 [============================>.] - ETA: 0s - loss: 0.5540 - mse: 0.5476\n",
      "Epoch 01380: saving model to Regression_Model/hepg2.mle.linear-1380.ckpt\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5557 - mse: 0.5493 - val_loss: 0.5553 - val_mse: 0.5489\n",
      "Epoch 1381/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5546 - mse: 0.5482 - val_loss: 0.5547 - val_mse: 0.5483\n",
      "Epoch 1382/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5576 - mse: 0.5512 - val_loss: 0.5561 - val_mse: 0.5497\n",
      "Epoch 1383/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5469 - mse: 0.5405 - val_loss: 0.5553 - val_mse: 0.5489\n",
      "Epoch 1384/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5546 - mse: 0.5482 - val_loss: 0.5554 - val_mse: 0.5490\n",
      "Epoch 1385/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5429 - mse: 0.5365 - val_loss: 0.5550 - val_mse: 0.5486\n",
      "Epoch 1386/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5509 - mse: 0.5445 - val_loss: 0.5556 - val_mse: 0.5492\n",
      "Epoch 1387/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5436 - mse: 0.5372 - val_loss: 0.5549 - val_mse: 0.5485\n",
      "Epoch 1388/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5495 - mse: 0.5431 - val_loss: 0.5554 - val_mse: 0.5490\n",
      "Epoch 1389/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5439 - mse: 0.5375 - val_loss: 0.5549 - val_mse: 0.5485\n",
      "Epoch 1390/2000\n",
      "273/277 [============================>.] - ETA: 0s - loss: 0.5475 - mse: 0.5411\n",
      "Epoch 01390: saving model to Regression_Model/hepg2.mle.linear-1390.ckpt\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5468 - mse: 0.5404 - val_loss: 0.5545 - val_mse: 0.5481\n",
      "Epoch 1391/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5547 - mse: 0.5483 - val_loss: 0.5551 - val_mse: 0.5487\n",
      "Epoch 1392/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5561 - mse: 0.5497 - val_loss: 0.5558 - val_mse: 0.5494\n",
      "Epoch 1393/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5561 - mse: 0.5497 - val_loss: 0.5546 - val_mse: 0.5482\n",
      "Epoch 1394/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5501 - mse: 0.5437 - val_loss: 0.5543 - val_mse: 0.5479\n",
      "Epoch 1395/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5543 - mse: 0.5479 - val_loss: 0.5541 - val_mse: 0.5477\n",
      "Epoch 1396/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5456 - mse: 0.5392 - val_loss: 0.5543 - val_mse: 0.5479\n",
      "Epoch 1397/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5495 - mse: 0.5431 - val_loss: 0.5544 - val_mse: 0.5480\n",
      "Epoch 1398/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5594 - mse: 0.5530 - val_loss: 0.5551 - val_mse: 0.5487\n",
      "Epoch 1399/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5451 - mse: 0.5387 - val_loss: 0.5546 - val_mse: 0.5482\n",
      "Epoch 1400/2000\n",
      "272/277 [============================>.] - ETA: 0s - loss: 0.5524 - mse: 0.5460\n",
      "Epoch 01400: saving model to Regression_Model/hepg2.mle.linear-1400.ckpt\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5502 - mse: 0.5438 - val_loss: 0.5548 - val_mse: 0.5484\n",
      "Epoch 1401/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5540 - mse: 0.5476 - val_loss: 0.5549 - val_mse: 0.5485\n",
      "Epoch 1402/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5633 - mse: 0.5569 - val_loss: 0.5555 - val_mse: 0.5491\n",
      "Epoch 1403/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5467 - mse: 0.5403 - val_loss: 0.5557 - val_mse: 0.5493\n",
      "Epoch 1404/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5478 - mse: 0.5414 - val_loss: 0.5557 - val_mse: 0.5493\n",
      "Epoch 1405/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5555 - mse: 0.5491 - val_loss: 0.5544 - val_mse: 0.5480\n",
      "Epoch 1406/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5506 - mse: 0.5442 - val_loss: 0.5561 - val_mse: 0.5497\n",
      "Epoch 1407/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5522 - mse: 0.5458 - val_loss: 0.5555 - val_mse: 0.5491\n",
      "Epoch 1408/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5547 - mse: 0.5483 - val_loss: 0.5544 - val_mse: 0.5480\n",
      "Epoch 1409/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5502 - mse: 0.5438 - val_loss: 0.5549 - val_mse: 0.5485\n",
      "Epoch 1410/2000\n",
      "270/277 [============================>.] - ETA: 0s - loss: 0.5450 - mse: 0.5386\n",
      "Epoch 01410: saving model to Regression_Model/hepg2.mle.linear-1410.ckpt\n",
      "277/277 [==============================] - 2s 5ms/step - loss: 0.5455 - mse: 0.5392 - val_loss: 0.5553 - val_mse: 0.5489\n",
      "Epoch 1411/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5496 - mse: 0.5432 - val_loss: 0.5558 - val_mse: 0.5494\n",
      "Epoch 1412/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5568 - mse: 0.5504 - val_loss: 0.5551 - val_mse: 0.5487\n",
      "Epoch 1413/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5502 - mse: 0.5438 - val_loss: 0.5557 - val_mse: 0.5493\n",
      "Epoch 1414/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5527 - mse: 0.5463 - val_loss: 0.5547 - val_mse: 0.5483\n",
      "Epoch 1415/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5486 - mse: 0.5422 - val_loss: 0.5550 - val_mse: 0.5486\n",
      "Epoch 1416/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5520 - mse: 0.5456 - val_loss: 0.5555 - val_mse: 0.5491\n",
      "Epoch 1417/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5503 - mse: 0.5439 - val_loss: 0.5541 - val_mse: 0.5477\n",
      "Epoch 1418/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5545 - mse: 0.5482 - val_loss: 0.5549 - val_mse: 0.5485\n",
      "Epoch 1419/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5512 - mse: 0.5448 - val_loss: 0.5563 - val_mse: 0.5499\n",
      "Epoch 1420/2000\n",
      "273/277 [============================>.] - ETA: 0s - loss: 0.5492 - mse: 0.5428\n",
      "Epoch 01420: saving model to Regression_Model/hepg2.mle.linear-1420.ckpt\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5512 - mse: 0.5448 - val_loss: 0.5548 - val_mse: 0.5484\n",
      "Epoch 1421/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5574 - mse: 0.5510 - val_loss: 0.5547 - val_mse: 0.5483\n",
      "Epoch 1422/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5510 - mse: 0.5446 - val_loss: 0.5552 - val_mse: 0.5488\n",
      "Epoch 1423/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5546 - mse: 0.5483 - val_loss: 0.5545 - val_mse: 0.5481\n",
      "Epoch 1424/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5498 - mse: 0.5434 - val_loss: 0.5552 - val_mse: 0.5488\n",
      "Epoch 1425/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5529 - mse: 0.5465 - val_loss: 0.5545 - val_mse: 0.5481\n",
      "Epoch 1426/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5585 - mse: 0.5521 - val_loss: 0.5546 - val_mse: 0.5482\n",
      "Epoch 1427/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5538 - mse: 0.5474 - val_loss: 0.5554 - val_mse: 0.5490\n",
      "Epoch 1428/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5577 - mse: 0.5513 - val_loss: 0.5545 - val_mse: 0.5481\n",
      "Epoch 1429/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5526 - mse: 0.5462 - val_loss: 0.5545 - val_mse: 0.5481\n",
      "Epoch 1430/2000\n",
      "271/277 [============================>.] - ETA: 0s - loss: 0.5435 - mse: 0.5371\n",
      "Epoch 01430: saving model to Regression_Model/hepg2.mle.linear-1430.ckpt\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5428 - mse: 0.5364 - val_loss: 0.5542 - val_mse: 0.5478\n",
      "Epoch 1431/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5548 - mse: 0.5484 - val_loss: 0.5557 - val_mse: 0.5493\n",
      "Epoch 1432/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5614 - mse: 0.5550 - val_loss: 0.5548 - val_mse: 0.5484\n",
      "Epoch 1433/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5500 - mse: 0.5436 - val_loss: 0.5538 - val_mse: 0.5474\n",
      "Epoch 1434/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5487 - mse: 0.5423 - val_loss: 0.5548 - val_mse: 0.5485\n",
      "Epoch 1435/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5465 - mse: 0.5401 - val_loss: 0.5542 - val_mse: 0.5478\n",
      "Epoch 1436/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5485 - mse: 0.5421 - val_loss: 0.5538 - val_mse: 0.5474\n",
      "Epoch 1437/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5528 - mse: 0.5464 - val_loss: 0.5555 - val_mse: 0.5491\n",
      "Epoch 1438/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5463 - mse: 0.5400 - val_loss: 0.5549 - val_mse: 0.5485\n",
      "Epoch 1439/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5520 - mse: 0.5456 - val_loss: 0.5550 - val_mse: 0.5487\n",
      "Epoch 1440/2000\n",
      "273/277 [============================>.] - ETA: 0s - loss: 0.5487 - mse: 0.5423\n",
      "Epoch 01440: saving model to Regression_Model/hepg2.mle.linear-1440.ckpt\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5483 - mse: 0.5419 - val_loss: 0.5547 - val_mse: 0.5484\n",
      "Epoch 1441/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5504 - mse: 0.5441 - val_loss: 0.5539 - val_mse: 0.5475\n",
      "Epoch 1442/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5567 - mse: 0.5503 - val_loss: 0.5542 - val_mse: 0.5478\n",
      "Epoch 1443/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5482 - mse: 0.5419 - val_loss: 0.5548 - val_mse: 0.5484\n",
      "Epoch 1444/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5513 - mse: 0.5450 - val_loss: 0.5557 - val_mse: 0.5493\n",
      "Epoch 1445/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5534 - mse: 0.5470 - val_loss: 0.5551 - val_mse: 0.5488\n",
      "Epoch 1446/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5558 - mse: 0.5494 - val_loss: 0.5556 - val_mse: 0.5492\n",
      "Epoch 1447/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5536 - mse: 0.5472 - val_loss: 0.5555 - val_mse: 0.5491\n",
      "Epoch 1448/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5461 - mse: 0.5398 - val_loss: 0.5547 - val_mse: 0.5483\n",
      "Epoch 1449/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5547 - mse: 0.5483 - val_loss: 0.5554 - val_mse: 0.5490\n",
      "Epoch 1450/2000\n",
      "269/277 [============================>.] - ETA: 0s - loss: 0.5476 - mse: 0.5413\n",
      "Epoch 01450: saving model to Regression_Model/hepg2.mle.linear-1450.ckpt\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5469 - mse: 0.5405 - val_loss: 0.5559 - val_mse: 0.5495\n",
      "Epoch 1451/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5533 - mse: 0.5470 - val_loss: 0.5553 - val_mse: 0.5489\n",
      "Epoch 1452/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5533 - mse: 0.5469 - val_loss: 0.5547 - val_mse: 0.5484\n",
      "Epoch 1453/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5566 - mse: 0.5503 - val_loss: 0.5564 - val_mse: 0.5500\n",
      "Epoch 1454/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5607 - mse: 0.5543 - val_loss: 0.5559 - val_mse: 0.5495\n",
      "Epoch 1455/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5553 - mse: 0.5489 - val_loss: 0.5555 - val_mse: 0.5491\n",
      "Epoch 1456/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5510 - mse: 0.5446 - val_loss: 0.5546 - val_mse: 0.5482\n",
      "Epoch 1457/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5469 - mse: 0.5405 - val_loss: 0.5556 - val_mse: 0.5492\n",
      "Epoch 1458/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5504 - mse: 0.5440 - val_loss: 0.5554 - val_mse: 0.5491\n",
      "Epoch 1459/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5504 - mse: 0.5441 - val_loss: 0.5559 - val_mse: 0.5495\n",
      "Epoch 1460/2000\n",
      "275/277 [============================>.] - ETA: 0s - loss: 0.5547 - mse: 0.5483\n",
      "Epoch 01460: saving model to Regression_Model/hepg2.mle.linear-1460.ckpt\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5545 - mse: 0.5481 - val_loss: 0.5552 - val_mse: 0.5488\n",
      "Epoch 1461/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5496 - mse: 0.5432 - val_loss: 0.5555 - val_mse: 0.5491\n",
      "Epoch 1462/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5527 - mse: 0.5463 - val_loss: 0.5547 - val_mse: 0.5484\n",
      "Epoch 1463/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5523 - mse: 0.5459 - val_loss: 0.5550 - val_mse: 0.5486\n",
      "Epoch 1464/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5567 - mse: 0.5503 - val_loss: 0.5548 - val_mse: 0.5484\n",
      "Epoch 1465/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5538 - mse: 0.5474 - val_loss: 0.5563 - val_mse: 0.5500\n",
      "Epoch 1466/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5461 - mse: 0.5397 - val_loss: 0.5547 - val_mse: 0.5483\n",
      "Epoch 1467/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5479 - mse: 0.5415 - val_loss: 0.5553 - val_mse: 0.5490\n",
      "Epoch 1468/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5514 - mse: 0.5451 - val_loss: 0.5552 - val_mse: 0.5488\n",
      "Epoch 1469/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5507 - mse: 0.5443 - val_loss: 0.5554 - val_mse: 0.5491\n",
      "Epoch 1470/2000\n",
      "273/277 [============================>.] - ETA: 0s - loss: 0.5436 - mse: 0.5372\n",
      "Epoch 01470: saving model to Regression_Model/hepg2.mle.linear-1470.ckpt\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5414 - mse: 0.5350 - val_loss: 0.5554 - val_mse: 0.5490\n",
      "Epoch 1471/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5495 - mse: 0.5431 - val_loss: 0.5551 - val_mse: 0.5487\n",
      "Epoch 1472/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5519 - mse: 0.5456 - val_loss: 0.5551 - val_mse: 0.5487\n",
      "Epoch 1473/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5525 - mse: 0.5462 - val_loss: 0.5553 - val_mse: 0.5489\n",
      "Epoch 1474/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5598 - mse: 0.5534 - val_loss: 0.5546 - val_mse: 0.5482\n",
      "Epoch 1475/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5571 - mse: 0.5507 - val_loss: 0.5551 - val_mse: 0.5487\n",
      "Epoch 1476/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5502 - mse: 0.5438 - val_loss: 0.5554 - val_mse: 0.5490\n",
      "Epoch 1477/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5554 - mse: 0.5490 - val_loss: 0.5560 - val_mse: 0.5496\n",
      "Epoch 1478/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5499 - mse: 0.5436 - val_loss: 0.5554 - val_mse: 0.5490\n",
      "Epoch 1479/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5608 - mse: 0.5545 - val_loss: 0.5558 - val_mse: 0.5494\n",
      "Epoch 1480/2000\n",
      "265/277 [===========================>..] - ETA: 0s - loss: 0.5522 - mse: 0.5458\n",
      "Epoch 01480: saving model to Regression_Model/hepg2.mle.linear-1480.ckpt\n",
      "277/277 [==============================] - 2s 5ms/step - loss: 0.5501 - mse: 0.5437 - val_loss: 0.5560 - val_mse: 0.5496\n",
      "Epoch 1481/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5545 - mse: 0.5481 - val_loss: 0.5548 - val_mse: 0.5484\n",
      "Epoch 1482/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5547 - mse: 0.5484 - val_loss: 0.5545 - val_mse: 0.5481\n",
      "Epoch 1483/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5521 - mse: 0.5457 - val_loss: 0.5553 - val_mse: 0.5489\n",
      "Epoch 1484/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5485 - mse: 0.5422 - val_loss: 0.5564 - val_mse: 0.5501\n",
      "Epoch 1485/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5514 - mse: 0.5450 - val_loss: 0.5560 - val_mse: 0.5496\n",
      "Epoch 1486/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5437 - mse: 0.5374 - val_loss: 0.5560 - val_mse: 0.5497\n",
      "Epoch 1487/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5582 - mse: 0.5518 - val_loss: 0.5576 - val_mse: 0.5513\n",
      "Epoch 1488/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5562 - mse: 0.5498 - val_loss: 0.5562 - val_mse: 0.5498\n",
      "Epoch 1489/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5548 - mse: 0.5484 - val_loss: 0.5555 - val_mse: 0.5491\n",
      "Epoch 1490/2000\n",
      "265/277 [===========================>..] - ETA: 0s - loss: 0.5458 - mse: 0.5395\n",
      "Epoch 01490: saving model to Regression_Model/hepg2.mle.linear-1490.ckpt\n",
      "277/277 [==============================] - 2s 5ms/step - loss: 0.5462 - mse: 0.5398 - val_loss: 0.5557 - val_mse: 0.5493\n",
      "Epoch 1491/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5424 - mse: 0.5361 - val_loss: 0.5553 - val_mse: 0.5490\n",
      "Epoch 1492/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5504 - mse: 0.5441 - val_loss: 0.5555 - val_mse: 0.5492\n",
      "Epoch 1493/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5534 - mse: 0.5471 - val_loss: 0.5559 - val_mse: 0.5495\n",
      "Epoch 1494/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5522 - mse: 0.5458 - val_loss: 0.5557 - val_mse: 0.5494\n",
      "Epoch 1495/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5572 - mse: 0.5509 - val_loss: 0.5549 - val_mse: 0.5485\n",
      "Epoch 1496/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5577 - mse: 0.5513 - val_loss: 0.5562 - val_mse: 0.5498\n",
      "Epoch 1497/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5529 - mse: 0.5466 - val_loss: 0.5561 - val_mse: 0.5498\n",
      "Epoch 1498/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5571 - mse: 0.5507 - val_loss: 0.5553 - val_mse: 0.5489\n",
      "Epoch 1499/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5520 - mse: 0.5457 - val_loss: 0.5553 - val_mse: 0.5489\n",
      "Epoch 1500/2000\n",
      "266/277 [===========================>..] - ETA: 0s - loss: 0.5510 - mse: 0.5447\n",
      "Epoch 01500: saving model to Regression_Model/hepg2.mle.linear-1500.ckpt\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5521 - mse: 0.5458 - val_loss: 0.5542 - val_mse: 0.5478\n",
      "Epoch 1501/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5470 - mse: 0.5406 - val_loss: 0.5548 - val_mse: 0.5484\n",
      "Epoch 1502/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5485 - mse: 0.5422 - val_loss: 0.5549 - val_mse: 0.5485\n",
      "Epoch 1503/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5530 - mse: 0.5467 - val_loss: 0.5547 - val_mse: 0.5483\n",
      "Epoch 1504/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5538 - mse: 0.5474 - val_loss: 0.5549 - val_mse: 0.5486\n",
      "Epoch 1505/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5510 - mse: 0.5446 - val_loss: 0.5550 - val_mse: 0.5487\n",
      "Epoch 1506/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5447 - mse: 0.5383 - val_loss: 0.5544 - val_mse: 0.5481\n",
      "Epoch 1507/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5577 - mse: 0.5513 - val_loss: 0.5554 - val_mse: 0.5491\n",
      "Epoch 1508/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5564 - mse: 0.5500 - val_loss: 0.5555 - val_mse: 0.5492\n",
      "Epoch 1509/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5544 - mse: 0.5481 - val_loss: 0.5548 - val_mse: 0.5485\n",
      "Epoch 1510/2000\n",
      "270/277 [============================>.] - ETA: 0s - loss: 0.5512 - mse: 0.5449\n",
      "Epoch 01510: saving model to Regression_Model/hepg2.mle.linear-1510.ckpt\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5523 - mse: 0.5459 - val_loss: 0.5552 - val_mse: 0.5488\n",
      "Epoch 1511/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5497 - mse: 0.5434 - val_loss: 0.5553 - val_mse: 0.5489\n",
      "Epoch 1512/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5492 - mse: 0.5428 - val_loss: 0.5549 - val_mse: 0.5486\n",
      "Epoch 1513/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5562 - mse: 0.5499 - val_loss: 0.5552 - val_mse: 0.5489\n",
      "Epoch 1514/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5530 - mse: 0.5467 - val_loss: 0.5547 - val_mse: 0.5484\n",
      "Epoch 1515/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5499 - mse: 0.5435 - val_loss: 0.5543 - val_mse: 0.5480\n",
      "Epoch 1516/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5518 - mse: 0.5455 - val_loss: 0.5551 - val_mse: 0.5487\n",
      "Epoch 1517/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5453 - mse: 0.5389 - val_loss: 0.5541 - val_mse: 0.5477\n",
      "Epoch 1518/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5475 - mse: 0.5412 - val_loss: 0.5542 - val_mse: 0.5479\n",
      "Epoch 1519/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5517 - mse: 0.5454 - val_loss: 0.5550 - val_mse: 0.5487\n",
      "Epoch 1520/2000\n",
      "276/277 [============================>.] - ETA: 0s - loss: 0.5511 - mse: 0.5448\n",
      "Epoch 01520: saving model to Regression_Model/hepg2.mle.linear-1520.ckpt\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5516 - mse: 0.5452 - val_loss: 0.5548 - val_mse: 0.5484\n",
      "Epoch 1521/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5606 - mse: 0.5542 - val_loss: 0.5556 - val_mse: 0.5492\n",
      "Epoch 1522/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5566 - mse: 0.5502 - val_loss: 0.5549 - val_mse: 0.5485\n",
      "Epoch 1523/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5490 - mse: 0.5427 - val_loss: 0.5556 - val_mse: 0.5492\n",
      "Epoch 1524/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5533 - mse: 0.5470 - val_loss: 0.5555 - val_mse: 0.5492\n",
      "Epoch 1525/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5548 - mse: 0.5485 - val_loss: 0.5559 - val_mse: 0.5496\n",
      "Epoch 1526/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5479 - mse: 0.5415 - val_loss: 0.5554 - val_mse: 0.5490\n",
      "Epoch 1527/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5573 - mse: 0.5510 - val_loss: 0.5561 - val_mse: 0.5498\n",
      "Epoch 1528/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5516 - mse: 0.5453 - val_loss: 0.5554 - val_mse: 0.5490\n",
      "Epoch 1529/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5471 - mse: 0.5408 - val_loss: 0.5543 - val_mse: 0.5480\n",
      "Epoch 1530/2000\n",
      "275/277 [============================>.] - ETA: 0s - loss: 0.5495 - mse: 0.5432\n",
      "Epoch 01530: saving model to Regression_Model/hepg2.mle.linear-1530.ckpt\n",
      "277/277 [==============================] - 2s 6ms/step - loss: 0.5493 - mse: 0.5430 - val_loss: 0.5541 - val_mse: 0.5478\n",
      "Epoch 1531/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5447 - mse: 0.5384 - val_loss: 0.5549 - val_mse: 0.5486\n",
      "Epoch 1532/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5519 - mse: 0.5455 - val_loss: 0.5541 - val_mse: 0.5478\n",
      "Epoch 1533/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5527 - mse: 0.5464 - val_loss: 0.5542 - val_mse: 0.5478\n",
      "Epoch 1534/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5527 - mse: 0.5464 - val_loss: 0.5548 - val_mse: 0.5485\n",
      "Epoch 1535/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5493 - mse: 0.5430 - val_loss: 0.5547 - val_mse: 0.5484\n",
      "Epoch 1536/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5500 - mse: 0.5437 - val_loss: 0.5555 - val_mse: 0.5491\n",
      "Epoch 1537/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5489 - mse: 0.5426 - val_loss: 0.5551 - val_mse: 0.5487\n",
      "Epoch 1538/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5553 - mse: 0.5490 - val_loss: 0.5550 - val_mse: 0.5486\n",
      "Epoch 1539/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5532 - mse: 0.5469 - val_loss: 0.5550 - val_mse: 0.5487\n",
      "Epoch 1540/2000\n",
      "266/277 [===========================>..] - ETA: 0s - loss: 0.5482 - mse: 0.5418\n",
      "Epoch 01540: saving model to Regression_Model/hepg2.mle.linear-1540.ckpt\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5500 - mse: 0.5437 - val_loss: 0.5545 - val_mse: 0.5482\n",
      "Epoch 1541/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5528 - mse: 0.5465 - val_loss: 0.5548 - val_mse: 0.5484\n",
      "Epoch 1542/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5436 - mse: 0.5373 - val_loss: 0.5550 - val_mse: 0.5487\n",
      "Epoch 1543/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5475 - mse: 0.5412 - val_loss: 0.5546 - val_mse: 0.5482\n",
      "Epoch 1544/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5564 - mse: 0.5500 - val_loss: 0.5547 - val_mse: 0.5484\n",
      "Epoch 1545/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5529 - mse: 0.5466 - val_loss: 0.5545 - val_mse: 0.5481\n",
      "Epoch 1546/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5545 - mse: 0.5481 - val_loss: 0.5550 - val_mse: 0.5487\n",
      "Epoch 1547/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5494 - mse: 0.5431 - val_loss: 0.5541 - val_mse: 0.5477\n",
      "Epoch 1548/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5502 - mse: 0.5439 - val_loss: 0.5551 - val_mse: 0.5487\n",
      "Epoch 1549/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5511 - mse: 0.5447 - val_loss: 0.5545 - val_mse: 0.5481\n",
      "Epoch 1550/2000\n",
      "275/277 [============================>.] - ETA: 0s - loss: 0.5457 - mse: 0.5393\n",
      "Epoch 01550: saving model to Regression_Model/hepg2.mle.linear-1550.ckpt\n",
      "277/277 [==============================] - 2s 6ms/step - loss: 0.5455 - mse: 0.5391 - val_loss: 0.5555 - val_mse: 0.5491\n",
      "Epoch 1551/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5508 - mse: 0.5445 - val_loss: 0.5565 - val_mse: 0.5502\n",
      "Epoch 1552/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5571 - mse: 0.5508 - val_loss: 0.5557 - val_mse: 0.5494\n",
      "Epoch 1553/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5550 - mse: 0.5487 - val_loss: 0.5554 - val_mse: 0.5490\n",
      "Epoch 1554/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5541 - mse: 0.5478 - val_loss: 0.5558 - val_mse: 0.5494\n",
      "Epoch 1555/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5565 - mse: 0.5502 - val_loss: 0.5558 - val_mse: 0.5494\n",
      "Epoch 1556/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5441 - mse: 0.5378 - val_loss: 0.5546 - val_mse: 0.5483\n",
      "Epoch 1557/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5420 - mse: 0.5357 - val_loss: 0.5540 - val_mse: 0.5476\n",
      "Epoch 1558/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5497 - mse: 0.5433 - val_loss: 0.5553 - val_mse: 0.5489\n",
      "Epoch 1559/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5515 - mse: 0.5452 - val_loss: 0.5544 - val_mse: 0.5480\n",
      "Epoch 1560/2000\n",
      "273/277 [============================>.] - ETA: 0s - loss: 0.5507 - mse: 0.5444\n",
      "Epoch 01560: saving model to Regression_Model/hepg2.mle.linear-1560.ckpt\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5515 - mse: 0.5452 - val_loss: 0.5550 - val_mse: 0.5486\n",
      "Epoch 1561/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5465 - mse: 0.5402 - val_loss: 0.5548 - val_mse: 0.5485\n",
      "Epoch 1562/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5448 - mse: 0.5384 - val_loss: 0.5545 - val_mse: 0.5482\n",
      "Epoch 1563/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5473 - mse: 0.5410 - val_loss: 0.5552 - val_mse: 0.5488\n",
      "Epoch 1564/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5506 - mse: 0.5442 - val_loss: 0.5558 - val_mse: 0.5494\n",
      "Epoch 1565/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5446 - mse: 0.5383 - val_loss: 0.5557 - val_mse: 0.5494\n",
      "Epoch 1566/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5515 - mse: 0.5452 - val_loss: 0.5553 - val_mse: 0.5490\n",
      "Epoch 1567/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5451 - mse: 0.5388 - val_loss: 0.5549 - val_mse: 0.5485\n",
      "Epoch 1568/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5516 - mse: 0.5453 - val_loss: 0.5546 - val_mse: 0.5483\n",
      "Epoch 1569/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5515 - mse: 0.5452 - val_loss: 0.5553 - val_mse: 0.5490\n",
      "Epoch 1570/2000\n",
      "273/277 [============================>.] - ETA: 0s - loss: 0.5531 - mse: 0.5467\n",
      "Epoch 01570: saving model to Regression_Model/hepg2.mle.linear-1570.ckpt\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5547 - mse: 0.5483 - val_loss: 0.5544 - val_mse: 0.5480\n",
      "Epoch 1571/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5463 - mse: 0.5400 - val_loss: 0.5547 - val_mse: 0.5483\n",
      "Epoch 1572/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5478 - mse: 0.5415 - val_loss: 0.5554 - val_mse: 0.5491\n",
      "Epoch 1573/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5566 - mse: 0.5503 - val_loss: 0.5554 - val_mse: 0.5491\n",
      "Epoch 1574/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5466 - mse: 0.5402 - val_loss: 0.5548 - val_mse: 0.5484\n",
      "Epoch 1575/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5485 - mse: 0.5422 - val_loss: 0.5548 - val_mse: 0.5485\n",
      "Epoch 1576/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5446 - mse: 0.5383 - val_loss: 0.5551 - val_mse: 0.5488\n",
      "Epoch 1577/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5473 - mse: 0.5409 - val_loss: 0.5556 - val_mse: 0.5492\n",
      "Epoch 1578/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5452 - mse: 0.5389 - val_loss: 0.5551 - val_mse: 0.5488\n",
      "Epoch 1579/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5545 - mse: 0.5482 - val_loss: 0.5553 - val_mse: 0.5490\n",
      "Epoch 1580/2000\n",
      "267/277 [===========================>..] - ETA: 0s - loss: 0.5526 - mse: 0.5463\n",
      "Epoch 01580: saving model to Regression_Model/hepg2.mle.linear-1580.ckpt\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5517 - mse: 0.5454 - val_loss: 0.5546 - val_mse: 0.5483\n",
      "Epoch 1581/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5525 - mse: 0.5462 - val_loss: 0.5547 - val_mse: 0.5484\n",
      "Epoch 1582/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5573 - mse: 0.5510 - val_loss: 0.5547 - val_mse: 0.5484\n",
      "Epoch 1583/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5580 - mse: 0.5517 - val_loss: 0.5547 - val_mse: 0.5484\n",
      "Epoch 1584/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5517 - mse: 0.5454 - val_loss: 0.5547 - val_mse: 0.5484\n",
      "Epoch 1585/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5478 - mse: 0.5415 - val_loss: 0.5546 - val_mse: 0.5483\n",
      "Epoch 1586/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5553 - mse: 0.5489 - val_loss: 0.5546 - val_mse: 0.5482\n",
      "Epoch 1587/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5488 - mse: 0.5424 - val_loss: 0.5552 - val_mse: 0.5489\n",
      "Epoch 1588/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5450 - mse: 0.5386 - val_loss: 0.5546 - val_mse: 0.5483\n",
      "Epoch 1589/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5569 - mse: 0.5506 - val_loss: 0.5552 - val_mse: 0.5488\n",
      "Epoch 1590/2000\n",
      "263/277 [===========================>..] - ETA: 0s - loss: 0.5470 - mse: 0.5407\n",
      "Epoch 01590: saving model to Regression_Model/hepg2.mle.linear-1590.ckpt\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5512 - mse: 0.5449 - val_loss: 0.5542 - val_mse: 0.5479\n",
      "Epoch 1591/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5541 - mse: 0.5477 - val_loss: 0.5555 - val_mse: 0.5491\n",
      "Epoch 1592/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5489 - mse: 0.5426 - val_loss: 0.5544 - val_mse: 0.5480\n",
      "Epoch 1593/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5538 - mse: 0.5475 - val_loss: 0.5545 - val_mse: 0.5481\n",
      "Epoch 1594/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5581 - mse: 0.5517 - val_loss: 0.5545 - val_mse: 0.5482\n",
      "Epoch 1595/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5464 - mse: 0.5401 - val_loss: 0.5543 - val_mse: 0.5480\n",
      "Epoch 1596/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5486 - mse: 0.5423 - val_loss: 0.5546 - val_mse: 0.5483\n",
      "Epoch 1597/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5494 - mse: 0.5430 - val_loss: 0.5542 - val_mse: 0.5479\n",
      "Epoch 1598/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5467 - mse: 0.5404 - val_loss: 0.5548 - val_mse: 0.5485\n",
      "Epoch 1599/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5534 - mse: 0.5471 - val_loss: 0.5547 - val_mse: 0.5484\n",
      "Epoch 1600/2000\n",
      "267/277 [===========================>..] - ETA: 0s - loss: 0.5560 - mse: 0.5497\n",
      "Epoch 01600: saving model to Regression_Model/hepg2.mle.linear-1600.ckpt\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5531 - mse: 0.5468 - val_loss: 0.5551 - val_mse: 0.5488\n",
      "Epoch 1601/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5517 - mse: 0.5454 - val_loss: 0.5545 - val_mse: 0.5482\n",
      "Epoch 1602/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5491 - mse: 0.5428 - val_loss: 0.5545 - val_mse: 0.5481\n",
      "Epoch 1603/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5505 - mse: 0.5441 - val_loss: 0.5549 - val_mse: 0.5485\n",
      "Epoch 1604/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5512 - mse: 0.5449 - val_loss: 0.5548 - val_mse: 0.5485\n",
      "Epoch 1605/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5533 - mse: 0.5470 - val_loss: 0.5542 - val_mse: 0.5479\n",
      "Epoch 1606/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5560 - mse: 0.5497 - val_loss: 0.5546 - val_mse: 0.5483\n",
      "Epoch 1607/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5503 - mse: 0.5440 - val_loss: 0.5548 - val_mse: 0.5485\n",
      "Epoch 1608/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5520 - mse: 0.5457 - val_loss: 0.5544 - val_mse: 0.5481\n",
      "Epoch 1609/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5507 - mse: 0.5443 - val_loss: 0.5545 - val_mse: 0.5482\n",
      "Epoch 1610/2000\n",
      "272/277 [============================>.] - ETA: 0s - loss: 0.5475 - mse: 0.5411\n",
      "Epoch 01610: saving model to Regression_Model/hepg2.mle.linear-1610.ckpt\n",
      "277/277 [==============================] - 2s 7ms/step - loss: 0.5466 - mse: 0.5402 - val_loss: 0.5547 - val_mse: 0.5483\n",
      "Epoch 1611/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5478 - mse: 0.5415 - val_loss: 0.5553 - val_mse: 0.5490\n",
      "Epoch 1612/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5555 - mse: 0.5491 - val_loss: 0.5557 - val_mse: 0.5494\n",
      "Epoch 1613/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5468 - mse: 0.5404 - val_loss: 0.5543 - val_mse: 0.5480\n",
      "Epoch 1614/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5472 - mse: 0.5408 - val_loss: 0.5547 - val_mse: 0.5484\n",
      "Epoch 1615/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5437 - mse: 0.5374 - val_loss: 0.5541 - val_mse: 0.5477\n",
      "Epoch 1616/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5486 - mse: 0.5423 - val_loss: 0.5552 - val_mse: 0.5489\n",
      "Epoch 1617/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5545 - mse: 0.5481 - val_loss: 0.5545 - val_mse: 0.5482\n",
      "Epoch 1618/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5575 - mse: 0.5512 - val_loss: 0.5546 - val_mse: 0.5483\n",
      "Epoch 1619/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5459 - mse: 0.5396 - val_loss: 0.5541 - val_mse: 0.5478\n",
      "Epoch 1620/2000\n",
      "271/277 [============================>.] - ETA: 0s - loss: 0.5445 - mse: 0.5382\n",
      "Epoch 01620: saving model to Regression_Model/hepg2.mle.linear-1620.ckpt\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5450 - mse: 0.5387 - val_loss: 0.5546 - val_mse: 0.5483\n",
      "Epoch 1621/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5593 - mse: 0.5530 - val_loss: 0.5547 - val_mse: 0.5484\n",
      "Epoch 1622/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5560 - mse: 0.5497 - val_loss: 0.5553 - val_mse: 0.5490\n",
      "Epoch 1623/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5482 - mse: 0.5419 - val_loss: 0.5542 - val_mse: 0.5479\n",
      "Epoch 1624/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5494 - mse: 0.5431 - val_loss: 0.5544 - val_mse: 0.5481\n",
      "Epoch 1625/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5540 - mse: 0.5477 - val_loss: 0.5548 - val_mse: 0.5485\n",
      "Epoch 1626/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5533 - mse: 0.5470 - val_loss: 0.5545 - val_mse: 0.5482\n",
      "Epoch 1627/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5560 - mse: 0.5497 - val_loss: 0.5549 - val_mse: 0.5486\n",
      "Epoch 1628/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5480 - mse: 0.5416 - val_loss: 0.5552 - val_mse: 0.5489\n",
      "Epoch 1629/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5455 - mse: 0.5392 - val_loss: 0.5546 - val_mse: 0.5483\n",
      "Epoch 1630/2000\n",
      "262/277 [===========================>..] - ETA: 0s - loss: 0.5448 - mse: 0.5385\n",
      "Epoch 01630: saving model to Regression_Model/hepg2.mle.linear-1630.ckpt\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5459 - mse: 0.5396 - val_loss: 0.5551 - val_mse: 0.5488\n",
      "Epoch 1631/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5535 - mse: 0.5471 - val_loss: 0.5550 - val_mse: 0.5487\n",
      "Epoch 1632/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5564 - mse: 0.5501 - val_loss: 0.5546 - val_mse: 0.5483\n",
      "Epoch 1633/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5466 - mse: 0.5403 - val_loss: 0.5548 - val_mse: 0.5485\n",
      "Epoch 1634/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5443 - mse: 0.5380 - val_loss: 0.5546 - val_mse: 0.5483\n",
      "Epoch 1635/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5520 - mse: 0.5457 - val_loss: 0.5542 - val_mse: 0.5479\n",
      "Epoch 1636/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5490 - mse: 0.5426 - val_loss: 0.5543 - val_mse: 0.5480\n",
      "Epoch 1637/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5579 - mse: 0.5516 - val_loss: 0.5547 - val_mse: 0.5484\n",
      "Epoch 1638/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5505 - mse: 0.5441 - val_loss: 0.5547 - val_mse: 0.5484\n",
      "Epoch 1639/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5424 - mse: 0.5361 - val_loss: 0.5549 - val_mse: 0.5486\n",
      "Epoch 1640/2000\n",
      "269/277 [============================>.] - ETA: 0s - loss: 0.5499 - mse: 0.5436\n",
      "Epoch 01640: saving model to Regression_Model/hepg2.mle.linear-1640.ckpt\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5509 - mse: 0.5446 - val_loss: 0.5546 - val_mse: 0.5483\n",
      "Epoch 1641/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5443 - mse: 0.5379 - val_loss: 0.5542 - val_mse: 0.5479\n",
      "Epoch 1642/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5570 - mse: 0.5507 - val_loss: 0.5545 - val_mse: 0.5481\n",
      "Epoch 1643/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5517 - mse: 0.5454 - val_loss: 0.5546 - val_mse: 0.5482\n",
      "Epoch 1644/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5530 - mse: 0.5467 - val_loss: 0.5548 - val_mse: 0.5485\n",
      "Epoch 1645/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5528 - mse: 0.5465 - val_loss: 0.5541 - val_mse: 0.5478\n",
      "Epoch 1646/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5512 - mse: 0.5449 - val_loss: 0.5552 - val_mse: 0.5489\n",
      "Epoch 1647/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5458 - mse: 0.5395 - val_loss: 0.5552 - val_mse: 0.5489\n",
      "Epoch 1648/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5520 - mse: 0.5457 - val_loss: 0.5548 - val_mse: 0.5485\n",
      "Epoch 1649/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5589 - mse: 0.5526 - val_loss: 0.5543 - val_mse: 0.5480\n",
      "Epoch 1650/2000\n",
      "268/277 [============================>.] - ETA: 0s - loss: 0.5422 - mse: 0.5359\n",
      "Epoch 01650: saving model to Regression_Model/hepg2.mle.linear-1650.ckpt\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5418 - mse: 0.5355 - val_loss: 0.5545 - val_mse: 0.5481\n",
      "Epoch 1651/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5450 - mse: 0.5387 - val_loss: 0.5545 - val_mse: 0.5482\n",
      "Epoch 1652/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5445 - mse: 0.5382 - val_loss: 0.5543 - val_mse: 0.5480\n",
      "Epoch 1653/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5483 - mse: 0.5420 - val_loss: 0.5546 - val_mse: 0.5483\n",
      "Epoch 1654/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5510 - mse: 0.5447 - val_loss: 0.5544 - val_mse: 0.5481\n",
      "Epoch 1655/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5477 - mse: 0.5414 - val_loss: 0.5542 - val_mse: 0.5479\n",
      "Epoch 1656/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5517 - mse: 0.5454 - val_loss: 0.5546 - val_mse: 0.5483\n",
      "Epoch 1657/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5553 - mse: 0.5490 - val_loss: 0.5549 - val_mse: 0.5486\n",
      "Epoch 1658/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5522 - mse: 0.5459 - val_loss: 0.5547 - val_mse: 0.5484\n",
      "Epoch 1659/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5555 - mse: 0.5492 - val_loss: 0.5558 - val_mse: 0.5495\n",
      "Epoch 1660/2000\n",
      "273/277 [============================>.] - ETA: 0s - loss: 0.5522 - mse: 0.5459\n",
      "Epoch 01660: saving model to Regression_Model/hepg2.mle.linear-1660.ckpt\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5509 - mse: 0.5446 - val_loss: 0.5547 - val_mse: 0.5484\n",
      "Epoch 1661/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5413 - mse: 0.5350 - val_loss: 0.5547 - val_mse: 0.5484\n",
      "Epoch 1662/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5553 - mse: 0.5490 - val_loss: 0.5543 - val_mse: 0.5480\n",
      "Epoch 1663/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5487 - mse: 0.5424 - val_loss: 0.5546 - val_mse: 0.5483\n",
      "Epoch 1664/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5571 - mse: 0.5508 - val_loss: 0.5552 - val_mse: 0.5489\n",
      "Epoch 1665/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5462 - mse: 0.5399 - val_loss: 0.5549 - val_mse: 0.5486\n",
      "Epoch 1666/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5524 - mse: 0.5461 - val_loss: 0.5548 - val_mse: 0.5485\n",
      "Epoch 1667/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5565 - mse: 0.5502 - val_loss: 0.5547 - val_mse: 0.5484\n",
      "Epoch 1668/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5480 - mse: 0.5417 - val_loss: 0.5548 - val_mse: 0.5485\n",
      "Epoch 1669/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5512 - mse: 0.5449 - val_loss: 0.5544 - val_mse: 0.5481\n",
      "Epoch 1670/2000\n",
      "272/277 [============================>.] - ETA: 0s - loss: 0.5571 - mse: 0.5508\n",
      "Epoch 01670: saving model to Regression_Model/hepg2.mle.linear-1670.ckpt\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5585 - mse: 0.5522 - val_loss: 0.5559 - val_mse: 0.5496\n",
      "Epoch 1671/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5465 - mse: 0.5402 - val_loss: 0.5547 - val_mse: 0.5484\n",
      "Epoch 1672/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5492 - mse: 0.5429 - val_loss: 0.5551 - val_mse: 0.5488\n",
      "Epoch 1673/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5425 - mse: 0.5362 - val_loss: 0.5546 - val_mse: 0.5483\n",
      "Epoch 1674/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5506 - mse: 0.5443 - val_loss: 0.5558 - val_mse: 0.5495\n",
      "Epoch 1675/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5439 - mse: 0.5376 - val_loss: 0.5542 - val_mse: 0.5479\n",
      "Epoch 1676/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5495 - mse: 0.5432 - val_loss: 0.5549 - val_mse: 0.5486\n",
      "Epoch 1677/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5566 - mse: 0.5503 - val_loss: 0.5547 - val_mse: 0.5484\n",
      "Epoch 1678/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5593 - mse: 0.5530 - val_loss: 0.5541 - val_mse: 0.5478\n",
      "Epoch 1679/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5465 - mse: 0.5402 - val_loss: 0.5543 - val_mse: 0.5480\n",
      "Epoch 1680/2000\n",
      "275/277 [============================>.] - ETA: 0s - loss: 0.5555 - mse: 0.5492\n",
      "Epoch 01680: saving model to Regression_Model/hepg2.mle.linear-1680.ckpt\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5554 - mse: 0.5491 - val_loss: 0.5551 - val_mse: 0.5488\n",
      "Epoch 1681/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5446 - mse: 0.5383 - val_loss: 0.5541 - val_mse: 0.5478\n",
      "Epoch 1682/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5473 - mse: 0.5410 - val_loss: 0.5536 - val_mse: 0.5473\n",
      "Epoch 1683/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5509 - mse: 0.5446 - val_loss: 0.5537 - val_mse: 0.5474\n",
      "Epoch 1684/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5489 - mse: 0.5426 - val_loss: 0.5540 - val_mse: 0.5477\n",
      "Epoch 1685/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5457 - mse: 0.5394 - val_loss: 0.5542 - val_mse: 0.5479\n",
      "Epoch 1686/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5438 - mse: 0.5375 - val_loss: 0.5546 - val_mse: 0.5483\n",
      "Epoch 1687/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5515 - mse: 0.5452 - val_loss: 0.5544 - val_mse: 0.5481\n",
      "Epoch 1688/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5505 - mse: 0.5442 - val_loss: 0.5548 - val_mse: 0.5485\n",
      "Epoch 1689/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5549 - mse: 0.5486 - val_loss: 0.5543 - val_mse: 0.5480\n",
      "Epoch 1690/2000\n",
      "276/277 [============================>.] - ETA: 0s - loss: 0.5494 - mse: 0.5431\n",
      "Epoch 01690: saving model to Regression_Model/hepg2.mle.linear-1690.ckpt\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5491 - mse: 0.5428 - val_loss: 0.5541 - val_mse: 0.5478\n",
      "Epoch 1691/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5553 - mse: 0.5490 - val_loss: 0.5542 - val_mse: 0.5479\n",
      "Epoch 1692/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5475 - mse: 0.5412 - val_loss: 0.5547 - val_mse: 0.5484\n",
      "Epoch 1693/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5572 - mse: 0.5509 - val_loss: 0.5549 - val_mse: 0.5486\n",
      "Epoch 1694/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5434 - mse: 0.5371 - val_loss: 0.5548 - val_mse: 0.5485\n",
      "Epoch 1695/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5524 - mse: 0.5461 - val_loss: 0.5551 - val_mse: 0.5488\n",
      "Epoch 1696/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5562 - mse: 0.5499 - val_loss: 0.5549 - val_mse: 0.5486\n",
      "Epoch 1697/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5476 - mse: 0.5413 - val_loss: 0.5549 - val_mse: 0.5486\n",
      "Epoch 1698/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5511 - mse: 0.5448 - val_loss: 0.5548 - val_mse: 0.5485\n",
      "Epoch 1699/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5453 - mse: 0.5390 - val_loss: 0.5546 - val_mse: 0.5483\n",
      "Epoch 1700/2000\n",
      "271/277 [============================>.] - ETA: 0s - loss: 0.5449 - mse: 0.5386\n",
      "Epoch 01700: saving model to Regression_Model/hepg2.mle.linear-1700.ckpt\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5451 - mse: 0.5388 - val_loss: 0.5543 - val_mse: 0.5480\n",
      "Epoch 1701/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5509 - mse: 0.5446 - val_loss: 0.5547 - val_mse: 0.5484\n",
      "Epoch 1702/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5504 - mse: 0.5441 - val_loss: 0.5554 - val_mse: 0.5491\n",
      "Epoch 1703/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5499 - mse: 0.5437 - val_loss: 0.5552 - val_mse: 0.5489\n",
      "Epoch 1704/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5507 - mse: 0.5444 - val_loss: 0.5547 - val_mse: 0.5484\n",
      "Epoch 1705/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5477 - mse: 0.5414 - val_loss: 0.5549 - val_mse: 0.5486\n",
      "Epoch 1706/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5457 - mse: 0.5395 - val_loss: 0.5549 - val_mse: 0.5487\n",
      "Epoch 1707/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5533 - mse: 0.5470 - val_loss: 0.5551 - val_mse: 0.5488\n",
      "Epoch 1708/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5472 - mse: 0.5409 - val_loss: 0.5548 - val_mse: 0.5485\n",
      "Epoch 1709/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5500 - mse: 0.5437 - val_loss: 0.5550 - val_mse: 0.5488\n",
      "Epoch 1710/2000\n",
      "273/277 [============================>.] - ETA: 0s - loss: 0.5502 - mse: 0.5439\n",
      "Epoch 01710: saving model to Regression_Model/hepg2.mle.linear-1710.ckpt\n",
      "277/277 [==============================] - 2s 8ms/step - loss: 0.5503 - mse: 0.5440 - val_loss: 0.5554 - val_mse: 0.5491\n",
      "Epoch 1711/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5548 - mse: 0.5485 - val_loss: 0.5552 - val_mse: 0.5489\n",
      "Epoch 1712/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5498 - mse: 0.5435 - val_loss: 0.5551 - val_mse: 0.5488\n",
      "Epoch 1713/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5571 - mse: 0.5508 - val_loss: 0.5552 - val_mse: 0.5489\n",
      "Epoch 1714/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5471 - mse: 0.5408 - val_loss: 0.5548 - val_mse: 0.5485\n",
      "Epoch 1715/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5511 - mse: 0.5448 - val_loss: 0.5547 - val_mse: 0.5484\n",
      "Epoch 1716/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5503 - mse: 0.5440 - val_loss: 0.5549 - val_mse: 0.5487\n",
      "Epoch 1717/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5577 - mse: 0.5514 - val_loss: 0.5553 - val_mse: 0.5490\n",
      "Epoch 1718/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5393 - mse: 0.5330 - val_loss: 0.5552 - val_mse: 0.5489\n",
      "Epoch 1719/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5519 - mse: 0.5456 - val_loss: 0.5550 - val_mse: 0.5487\n",
      "Epoch 1720/2000\n",
      "269/277 [============================>.] - ETA: 0s - loss: 0.5451 - mse: 0.5388\n",
      "Epoch 01720: saving model to Regression_Model/hepg2.mle.linear-1720.ckpt\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5444 - mse: 0.5381 - val_loss: 0.5549 - val_mse: 0.5486\n",
      "Epoch 1721/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5471 - mse: 0.5409 - val_loss: 0.5551 - val_mse: 0.5488\n",
      "Epoch 1722/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5564 - mse: 0.5501 - val_loss: 0.5543 - val_mse: 0.5480\n",
      "Epoch 1723/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5512 - mse: 0.5449 - val_loss: 0.5555 - val_mse: 0.5493\n",
      "Epoch 1724/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5426 - mse: 0.5363 - val_loss: 0.5550 - val_mse: 0.5487\n",
      "Epoch 1725/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5507 - mse: 0.5444 - val_loss: 0.5549 - val_mse: 0.5486\n",
      "Epoch 1726/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5506 - mse: 0.5443 - val_loss: 0.5550 - val_mse: 0.5487\n",
      "Epoch 1727/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5556 - mse: 0.5494 - val_loss: 0.5557 - val_mse: 0.5494\n",
      "Epoch 1728/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5537 - mse: 0.5474 - val_loss: 0.5558 - val_mse: 0.5496\n",
      "Epoch 1729/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5472 - mse: 0.5409 - val_loss: 0.5548 - val_mse: 0.5486\n",
      "Epoch 1730/2000\n",
      "274/277 [============================>.] - ETA: 0s - loss: 0.5508 - mse: 0.5445\n",
      "Epoch 01730: saving model to Regression_Model/hepg2.mle.linear-1730.ckpt\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5504 - mse: 0.5441 - val_loss: 0.5553 - val_mse: 0.5490\n",
      "Epoch 1731/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5442 - mse: 0.5379 - val_loss: 0.5557 - val_mse: 0.5494\n",
      "Epoch 1732/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5462 - mse: 0.5399 - val_loss: 0.5546 - val_mse: 0.5483\n",
      "Epoch 1733/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5480 - mse: 0.5417 - val_loss: 0.5551 - val_mse: 0.5488\n",
      "Epoch 1734/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5436 - mse: 0.5373 - val_loss: 0.5551 - val_mse: 0.5489\n",
      "Epoch 1735/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5486 - mse: 0.5423 - val_loss: 0.5551 - val_mse: 0.5488\n",
      "Epoch 1736/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5477 - mse: 0.5414 - val_loss: 0.5547 - val_mse: 0.5484\n",
      "Epoch 1737/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5533 - mse: 0.5470 - val_loss: 0.5551 - val_mse: 0.5488\n",
      "Epoch 1738/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5469 - mse: 0.5406 - val_loss: 0.5550 - val_mse: 0.5487\n",
      "Epoch 1739/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5459 - mse: 0.5396 - val_loss: 0.5546 - val_mse: 0.5483\n",
      "Epoch 1740/2000\n",
      "267/277 [===========================>..] - ETA: 0s - loss: 0.5445 - mse: 0.5382\n",
      "Epoch 01740: saving model to Regression_Model/hepg2.mle.linear-1740.ckpt\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5433 - mse: 0.5370 - val_loss: 0.5550 - val_mse: 0.5487\n",
      "Epoch 1741/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5539 - mse: 0.5476 - val_loss: 0.5554 - val_mse: 0.5491\n",
      "Epoch 1742/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5510 - mse: 0.5447 - val_loss: 0.5551 - val_mse: 0.5488\n",
      "Epoch 1743/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5485 - mse: 0.5422 - val_loss: 0.5556 - val_mse: 0.5493\n",
      "Epoch 1744/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5485 - mse: 0.5422 - val_loss: 0.5559 - val_mse: 0.5497\n",
      "Epoch 1745/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5523 - mse: 0.5460 - val_loss: 0.5545 - val_mse: 0.5482\n",
      "Epoch 1746/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5562 - mse: 0.5499 - val_loss: 0.5550 - val_mse: 0.5488\n",
      "Epoch 1747/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5476 - mse: 0.5414 - val_loss: 0.5549 - val_mse: 0.5486\n",
      "Epoch 1748/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5516 - mse: 0.5454 - val_loss: 0.5553 - val_mse: 0.5491\n",
      "Epoch 1749/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5451 - mse: 0.5388 - val_loss: 0.5552 - val_mse: 0.5489\n",
      "Epoch 1750/2000\n",
      "276/277 [============================>.] - ETA: 0s - loss: 0.5454 - mse: 0.5391\n",
      "Epoch 01750: saving model to Regression_Model/hepg2.mle.linear-1750.ckpt\n",
      "277/277 [==============================] - 2s 7ms/step - loss: 0.5458 - mse: 0.5396 - val_loss: 0.5554 - val_mse: 0.5491\n",
      "Epoch 1751/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5465 - mse: 0.5402 - val_loss: 0.5550 - val_mse: 0.5488\n",
      "Epoch 1752/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5528 - mse: 0.5465 - val_loss: 0.5548 - val_mse: 0.5486\n",
      "Epoch 1753/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5489 - mse: 0.5426 - val_loss: 0.5549 - val_mse: 0.5486\n",
      "Epoch 1754/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5477 - mse: 0.5414 - val_loss: 0.5553 - val_mse: 0.5490\n",
      "Epoch 1755/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5473 - mse: 0.5410 - val_loss: 0.5545 - val_mse: 0.5482\n",
      "Epoch 1756/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5512 - mse: 0.5449 - val_loss: 0.5550 - val_mse: 0.5487\n",
      "Epoch 1757/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5525 - mse: 0.5462 - val_loss: 0.5549 - val_mse: 0.5486\n",
      "Epoch 1758/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5566 - mse: 0.5504 - val_loss: 0.5549 - val_mse: 0.5487\n",
      "Epoch 1759/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5521 - mse: 0.5458 - val_loss: 0.5555 - val_mse: 0.5492\n",
      "Epoch 1760/2000\n",
      "269/277 [============================>.] - ETA: 0s - loss: 0.5510 - mse: 0.5448\n",
      "Epoch 01760: saving model to Regression_Model/hepg2.mle.linear-1760.ckpt\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5503 - mse: 0.5440 - val_loss: 0.5548 - val_mse: 0.5485\n",
      "Epoch 1761/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5544 - mse: 0.5481 - val_loss: 0.5550 - val_mse: 0.5487\n",
      "Epoch 1762/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5481 - mse: 0.5419 - val_loss: 0.5551 - val_mse: 0.5489\n",
      "Epoch 1763/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5502 - mse: 0.5439 - val_loss: 0.5549 - val_mse: 0.5486\n",
      "Epoch 1764/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5503 - mse: 0.5440 - val_loss: 0.5550 - val_mse: 0.5487\n",
      "Epoch 1765/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5523 - mse: 0.5461 - val_loss: 0.5554 - val_mse: 0.5491\n",
      "Epoch 1766/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5470 - mse: 0.5407 - val_loss: 0.5553 - val_mse: 0.5490\n",
      "Epoch 1767/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5488 - mse: 0.5426 - val_loss: 0.5553 - val_mse: 0.5490\n",
      "Epoch 1768/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5495 - mse: 0.5433 - val_loss: 0.5554 - val_mse: 0.5492\n",
      "Epoch 1769/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5552 - mse: 0.5489 - val_loss: 0.5555 - val_mse: 0.5492\n",
      "Epoch 1770/2000\n",
      "273/277 [============================>.] - ETA: 0s - loss: 0.5527 - mse: 0.5464\n",
      "Epoch 01770: saving model to Regression_Model/hepg2.mle.linear-1770.ckpt\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5522 - mse: 0.5459 - val_loss: 0.5551 - val_mse: 0.5489\n",
      "Epoch 1771/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5459 - mse: 0.5397 - val_loss: 0.5552 - val_mse: 0.5489\n",
      "Epoch 1772/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5547 - mse: 0.5484 - val_loss: 0.5551 - val_mse: 0.5489\n",
      "Epoch 1773/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5470 - mse: 0.5407 - val_loss: 0.5554 - val_mse: 0.5491\n",
      "Epoch 1774/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5510 - mse: 0.5448 - val_loss: 0.5554 - val_mse: 0.5491\n",
      "Epoch 1775/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5555 - mse: 0.5492 - val_loss: 0.5552 - val_mse: 0.5490\n",
      "Epoch 1776/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5505 - mse: 0.5442 - val_loss: 0.5553 - val_mse: 0.5490\n",
      "Epoch 1777/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5496 - mse: 0.5433 - val_loss: 0.5552 - val_mse: 0.5489\n",
      "Epoch 1778/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5462 - mse: 0.5400 - val_loss: 0.5549 - val_mse: 0.5486\n",
      "Epoch 1779/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5506 - mse: 0.5443 - val_loss: 0.5551 - val_mse: 0.5488\n",
      "Epoch 1780/2000\n",
      "266/277 [===========================>..] - ETA: 0s - loss: 0.5518 - mse: 0.5455\n",
      "Epoch 01780: saving model to Regression_Model/hepg2.mle.linear-1780.ckpt\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5488 - mse: 0.5425 - val_loss: 0.5553 - val_mse: 0.5490\n",
      "Epoch 1781/2000\n",
      "277/277 [==============================] - 2s 5ms/step - loss: 0.5435 - mse: 0.5372 - val_loss: 0.5555 - val_mse: 0.5492\n",
      "Epoch 1782/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5506 - mse: 0.5443 - val_loss: 0.5552 - val_mse: 0.5490\n",
      "Epoch 1783/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5467 - mse: 0.5404 - val_loss: 0.5552 - val_mse: 0.5489\n",
      "Epoch 1784/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5475 - mse: 0.5413 - val_loss: 0.5555 - val_mse: 0.5492\n",
      "Epoch 1785/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5497 - mse: 0.5434 - val_loss: 0.5553 - val_mse: 0.5490\n",
      "Epoch 1786/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5536 - mse: 0.5473 - val_loss: 0.5554 - val_mse: 0.5491\n",
      "Epoch 1787/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5511 - mse: 0.5448 - val_loss: 0.5553 - val_mse: 0.5490\n",
      "Epoch 1788/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5460 - mse: 0.5397 - val_loss: 0.5550 - val_mse: 0.5487\n",
      "Epoch 1789/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5532 - mse: 0.5469 - val_loss: 0.5555 - val_mse: 0.5493\n",
      "Epoch 1790/2000\n",
      "267/277 [===========================>..] - ETA: 0s - loss: 0.5504 - mse: 0.5441\n",
      "Epoch 01790: saving model to Regression_Model/hepg2.mle.linear-1790.ckpt\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5503 - mse: 0.5440 - val_loss: 0.5550 - val_mse: 0.5487\n",
      "Epoch 1791/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5520 - mse: 0.5457 - val_loss: 0.5548 - val_mse: 0.5485\n",
      "Epoch 1792/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5458 - mse: 0.5395 - val_loss: 0.5552 - val_mse: 0.5489\n",
      "Epoch 1793/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5514 - mse: 0.5452 - val_loss: 0.5548 - val_mse: 0.5485\n",
      "Epoch 1794/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5522 - mse: 0.5459 - val_loss: 0.5551 - val_mse: 0.5488\n",
      "Epoch 1795/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5510 - mse: 0.5447 - val_loss: 0.5547 - val_mse: 0.5484\n",
      "Epoch 1796/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5507 - mse: 0.5445 - val_loss: 0.5550 - val_mse: 0.5487\n",
      "Epoch 1797/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5559 - mse: 0.5496 - val_loss: 0.5558 - val_mse: 0.5495\n",
      "Epoch 1798/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5547 - mse: 0.5484 - val_loss: 0.5554 - val_mse: 0.5491\n",
      "Epoch 1799/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5494 - mse: 0.5431 - val_loss: 0.5552 - val_mse: 0.5489\n",
      "Epoch 1800/2000\n",
      "276/277 [============================>.] - ETA: 0s - loss: 0.5547 - mse: 0.5485\n",
      "Epoch 01800: saving model to Regression_Model/hepg2.mle.linear-1800.ckpt\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5538 - mse: 0.5475 - val_loss: 0.5552 - val_mse: 0.5490\n",
      "Epoch 1801/2000\n",
      "277/277 [==============================] - 2s 5ms/step - loss: 0.5423 - mse: 0.5360 - val_loss: 0.5550 - val_mse: 0.5488\n",
      "Epoch 1802/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5455 - mse: 0.5392 - val_loss: 0.5552 - val_mse: 0.5489\n",
      "Epoch 1803/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5457 - mse: 0.5395 - val_loss: 0.5555 - val_mse: 0.5492\n",
      "Epoch 1804/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5554 - mse: 0.5491 - val_loss: 0.5555 - val_mse: 0.5493\n",
      "Epoch 1805/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5530 - mse: 0.5467 - val_loss: 0.5553 - val_mse: 0.5491\n",
      "Epoch 1806/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5494 - mse: 0.5431 - val_loss: 0.5557 - val_mse: 0.5494\n",
      "Epoch 1807/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5511 - mse: 0.5449 - val_loss: 0.5560 - val_mse: 0.5497\n",
      "Epoch 1808/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5387 - mse: 0.5324 - val_loss: 0.5556 - val_mse: 0.5493\n",
      "Epoch 1809/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5560 - mse: 0.5498 - val_loss: 0.5554 - val_mse: 0.5491\n",
      "Epoch 1810/2000\n",
      "271/277 [============================>.] - ETA: 0s - loss: 0.5482 - mse: 0.5419\n",
      "Epoch 01810: saving model to Regression_Model/hepg2.mle.linear-1810.ckpt\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5476 - mse: 0.5414 - val_loss: 0.5556 - val_mse: 0.5493\n",
      "Epoch 1811/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5482 - mse: 0.5420 - val_loss: 0.5555 - val_mse: 0.5493\n",
      "Epoch 1812/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5590 - mse: 0.5528 - val_loss: 0.5548 - val_mse: 0.5486\n",
      "Epoch 1813/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5573 - mse: 0.5510 - val_loss: 0.5553 - val_mse: 0.5490\n",
      "Epoch 1814/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5436 - mse: 0.5374 - val_loss: 0.5548 - val_mse: 0.5485\n",
      "Epoch 1815/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5507 - mse: 0.5445 - val_loss: 0.5553 - val_mse: 0.5490\n",
      "Epoch 1816/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5485 - mse: 0.5423 - val_loss: 0.5548 - val_mse: 0.5485\n",
      "Epoch 1817/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5525 - mse: 0.5463 - val_loss: 0.5555 - val_mse: 0.5492\n",
      "Epoch 1818/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5413 - mse: 0.5350 - val_loss: 0.5552 - val_mse: 0.5489\n",
      "Epoch 1819/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5464 - mse: 0.5401 - val_loss: 0.5551 - val_mse: 0.5489\n",
      "Epoch 1820/2000\n",
      "269/277 [============================>.] - ETA: 0s - loss: 0.5546 - mse: 0.5483\n",
      "Epoch 01820: saving model to Regression_Model/hepg2.mle.linear-1820.ckpt\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5564 - mse: 0.5501 - val_loss: 0.5558 - val_mse: 0.5495\n",
      "Epoch 1821/2000\n",
      "277/277 [==============================] - 2s 5ms/step - loss: 0.5424 - mse: 0.5361 - val_loss: 0.5554 - val_mse: 0.5492\n",
      "Epoch 1822/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5488 - mse: 0.5425 - val_loss: 0.5552 - val_mse: 0.5489\n",
      "Epoch 1823/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5497 - mse: 0.5434 - val_loss: 0.5554 - val_mse: 0.5491\n",
      "Epoch 1824/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5597 - mse: 0.5535 - val_loss: 0.5555 - val_mse: 0.5493\n",
      "Epoch 1825/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5552 - mse: 0.5489 - val_loss: 0.5556 - val_mse: 0.5493\n",
      "Epoch 1826/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5499 - mse: 0.5436 - val_loss: 0.5554 - val_mse: 0.5492\n",
      "Epoch 1827/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5528 - mse: 0.5465 - val_loss: 0.5553 - val_mse: 0.5490\n",
      "Epoch 1828/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5480 - mse: 0.5417 - val_loss: 0.5551 - val_mse: 0.5489\n",
      "Epoch 1829/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5544 - mse: 0.5481 - val_loss: 0.5556 - val_mse: 0.5494\n",
      "Epoch 1830/2000\n",
      "273/277 [============================>.] - ETA: 0s - loss: 0.5463 - mse: 0.5400\n",
      "Epoch 01830: saving model to Regression_Model/hepg2.mle.linear-1830.ckpt\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5465 - mse: 0.5402 - val_loss: 0.5554 - val_mse: 0.5491\n",
      "Epoch 1831/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5411 - mse: 0.5349 - val_loss: 0.5550 - val_mse: 0.5487\n",
      "Epoch 1832/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5486 - mse: 0.5423 - val_loss: 0.5552 - val_mse: 0.5490\n",
      "Epoch 1833/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5506 - mse: 0.5443 - val_loss: 0.5553 - val_mse: 0.5490\n",
      "Epoch 1834/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5499 - mse: 0.5437 - val_loss: 0.5552 - val_mse: 0.5489\n",
      "Epoch 1835/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5449 - mse: 0.5386 - val_loss: 0.5549 - val_mse: 0.5486\n",
      "Epoch 1836/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5512 - mse: 0.5449 - val_loss: 0.5550 - val_mse: 0.5487\n",
      "Epoch 1837/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5475 - mse: 0.5412 - val_loss: 0.5552 - val_mse: 0.5489\n",
      "Epoch 1838/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5608 - mse: 0.5545 - val_loss: 0.5555 - val_mse: 0.5492\n",
      "Epoch 1839/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5515 - mse: 0.5452 - val_loss: 0.5553 - val_mse: 0.5490\n",
      "Epoch 1840/2000\n",
      "272/277 [============================>.] - ETA: 0s - loss: 0.5531 - mse: 0.5469\n",
      "Epoch 01840: saving model to Regression_Model/hepg2.mle.linear-1840.ckpt\n",
      "277/277 [==============================] - 2s 6ms/step - loss: 0.5517 - mse: 0.5455 - val_loss: 0.5554 - val_mse: 0.5491\n",
      "Epoch 1841/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5429 - mse: 0.5366 - val_loss: 0.5550 - val_mse: 0.5487\n",
      "Epoch 1842/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5528 - mse: 0.5465 - val_loss: 0.5553 - val_mse: 0.5490\n",
      "Epoch 1843/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5523 - mse: 0.5460 - val_loss: 0.5552 - val_mse: 0.5489\n",
      "Epoch 1844/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5512 - mse: 0.5449 - val_loss: 0.5551 - val_mse: 0.5488\n",
      "Epoch 1845/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5608 - mse: 0.5546 - val_loss: 0.5557 - val_mse: 0.5495\n",
      "Epoch 1846/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5488 - mse: 0.5425 - val_loss: 0.5554 - val_mse: 0.5491\n",
      "Epoch 1847/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5474 - mse: 0.5411 - val_loss: 0.5551 - val_mse: 0.5489\n",
      "Epoch 1848/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5494 - mse: 0.5431 - val_loss: 0.5554 - val_mse: 0.5491\n",
      "Epoch 1849/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5405 - mse: 0.5342 - val_loss: 0.5549 - val_mse: 0.5486\n",
      "Epoch 1850/2000\n",
      "273/277 [============================>.] - ETA: 0s - loss: 0.5552 - mse: 0.5489\n",
      "Epoch 01850: saving model to Regression_Model/hepg2.mle.linear-1850.ckpt\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5543 - mse: 0.5480 - val_loss: 0.5550 - val_mse: 0.5488\n",
      "Epoch 1851/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5500 - mse: 0.5438 - val_loss: 0.5551 - val_mse: 0.5488\n",
      "Epoch 1852/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5474 - mse: 0.5411 - val_loss: 0.5553 - val_mse: 0.5490\n",
      "Epoch 1853/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5503 - mse: 0.5440 - val_loss: 0.5550 - val_mse: 0.5488\n",
      "Epoch 1854/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5443 - mse: 0.5380 - val_loss: 0.5546 - val_mse: 0.5483\n",
      "Epoch 1855/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5470 - mse: 0.5407 - val_loss: 0.5547 - val_mse: 0.5484\n",
      "Epoch 1856/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5464 - mse: 0.5402 - val_loss: 0.5552 - val_mse: 0.5489\n",
      "Epoch 1857/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5521 - mse: 0.5459 - val_loss: 0.5552 - val_mse: 0.5490\n",
      "Epoch 1858/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5486 - mse: 0.5424 - val_loss: 0.5552 - val_mse: 0.5489\n",
      "Epoch 1859/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5524 - mse: 0.5462 - val_loss: 0.5552 - val_mse: 0.5489\n",
      "Epoch 1860/2000\n",
      "267/277 [===========================>..] - ETA: 0s - loss: 0.5589 - mse: 0.5526\n",
      "Epoch 01860: saving model to Regression_Model/hepg2.mle.linear-1860.ckpt\n",
      "277/277 [==============================] - 2s 6ms/step - loss: 0.5596 - mse: 0.5534 - val_loss: 0.5551 - val_mse: 0.5488\n",
      "Epoch 1861/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5469 - mse: 0.5406 - val_loss: 0.5553 - val_mse: 0.5491\n",
      "Epoch 1862/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5428 - mse: 0.5365 - val_loss: 0.5549 - val_mse: 0.5486\n",
      "Epoch 1863/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5465 - mse: 0.5402 - val_loss: 0.5551 - val_mse: 0.5489\n",
      "Epoch 1864/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5477 - mse: 0.5414 - val_loss: 0.5558 - val_mse: 0.5495\n",
      "Epoch 1865/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5528 - mse: 0.5465 - val_loss: 0.5551 - val_mse: 0.5489\n",
      "Epoch 1866/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5489 - mse: 0.5426 - val_loss: 0.5549 - val_mse: 0.5487\n",
      "Epoch 1867/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5511 - mse: 0.5448 - val_loss: 0.5549 - val_mse: 0.5486\n",
      "Epoch 1868/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5462 - mse: 0.5400 - val_loss: 0.5547 - val_mse: 0.5485\n",
      "Epoch 1869/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5524 - mse: 0.5461 - val_loss: 0.5548 - val_mse: 0.5486\n",
      "Epoch 1870/2000\n",
      "271/277 [============================>.] - ETA: 0s - loss: 0.5486 - mse: 0.5424\n",
      "Epoch 01870: saving model to Regression_Model/hepg2.mle.linear-1870.ckpt\n",
      "277/277 [==============================] - 2s 6ms/step - loss: 0.5474 - mse: 0.5412 - val_loss: 0.5549 - val_mse: 0.5487\n",
      "Epoch 1871/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5531 - mse: 0.5468 - val_loss: 0.5552 - val_mse: 0.5489\n",
      "Epoch 1872/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5469 - mse: 0.5407 - val_loss: 0.5553 - val_mse: 0.5491\n",
      "Epoch 1873/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5447 - mse: 0.5385 - val_loss: 0.5554 - val_mse: 0.5491\n",
      "Epoch 1874/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5494 - mse: 0.5432 - val_loss: 0.5555 - val_mse: 0.5493\n",
      "Epoch 1875/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5466 - mse: 0.5403 - val_loss: 0.5552 - val_mse: 0.5490\n",
      "Epoch 1876/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5440 - mse: 0.5378 - val_loss: 0.5552 - val_mse: 0.5490\n",
      "Epoch 1877/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5462 - mse: 0.5400 - val_loss: 0.5554 - val_mse: 0.5492\n",
      "Epoch 1878/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5550 - mse: 0.5488 - val_loss: 0.5556 - val_mse: 0.5493\n",
      "Epoch 1879/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5472 - mse: 0.5410 - val_loss: 0.5555 - val_mse: 0.5493\n",
      "Epoch 1880/2000\n",
      "269/277 [============================>.] - ETA: 0s - loss: 0.5489 - mse: 0.5426\n",
      "Epoch 01880: saving model to Regression_Model/hepg2.mle.linear-1880.ckpt\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5510 - mse: 0.5447 - val_loss: 0.5553 - val_mse: 0.5491\n",
      "Epoch 1881/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5498 - mse: 0.5436 - val_loss: 0.5557 - val_mse: 0.5495\n",
      "Epoch 1882/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5527 - mse: 0.5464 - val_loss: 0.5553 - val_mse: 0.5490\n",
      "Epoch 1883/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5460 - mse: 0.5397 - val_loss: 0.5552 - val_mse: 0.5489\n",
      "Epoch 1884/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5519 - mse: 0.5457 - val_loss: 0.5549 - val_mse: 0.5486\n",
      "Epoch 1885/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5425 - mse: 0.5362 - val_loss: 0.5548 - val_mse: 0.5486\n",
      "Epoch 1886/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5492 - mse: 0.5430 - val_loss: 0.5551 - val_mse: 0.5489\n",
      "Epoch 1887/2000\n",
      "277/277 [==============================] - 2s 5ms/step - loss: 0.5435 - mse: 0.5373 - val_loss: 0.5551 - val_mse: 0.5488\n",
      "Epoch 1888/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5454 - mse: 0.5392 - val_loss: 0.5549 - val_mse: 0.5487\n",
      "Epoch 1889/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5448 - mse: 0.5385 - val_loss: 0.5550 - val_mse: 0.5488\n",
      "Epoch 1890/2000\n",
      "276/277 [============================>.] - ETA: 0s - loss: 0.5500 - mse: 0.5438\n",
      "Epoch 01890: saving model to Regression_Model/hepg2.mle.linear-1890.ckpt\n",
      "277/277 [==============================] - 2s 6ms/step - loss: 0.5498 - mse: 0.5436 - val_loss: 0.5550 - val_mse: 0.5487\n",
      "Epoch 1891/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5573 - mse: 0.5510 - val_loss: 0.5553 - val_mse: 0.5490\n",
      "Epoch 1892/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5457 - mse: 0.5395 - val_loss: 0.5551 - val_mse: 0.5488\n",
      "Epoch 1893/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5465 - mse: 0.5403 - val_loss: 0.5551 - val_mse: 0.5488\n",
      "Epoch 1894/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5439 - mse: 0.5377 - val_loss: 0.5549 - val_mse: 0.5487\n",
      "Epoch 1895/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5486 - mse: 0.5423 - val_loss: 0.5554 - val_mse: 0.5492\n",
      "Epoch 1896/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5508 - mse: 0.5446 - val_loss: 0.5549 - val_mse: 0.5486\n",
      "Epoch 1897/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5467 - mse: 0.5405 - val_loss: 0.5550 - val_mse: 0.5487\n",
      "Epoch 1898/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5496 - mse: 0.5433 - val_loss: 0.5552 - val_mse: 0.5490\n",
      "Epoch 1899/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5561 - mse: 0.5499 - val_loss: 0.5554 - val_mse: 0.5491\n",
      "Epoch 1900/2000\n",
      "274/277 [============================>.] - ETA: 0s - loss: 0.5558 - mse: 0.5496\n",
      "Epoch 01900: saving model to Regression_Model/hepg2.mle.linear-1900.ckpt\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5552 - mse: 0.5489 - val_loss: 0.5551 - val_mse: 0.5489\n",
      "Epoch 1901/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5382 - mse: 0.5320 - val_loss: 0.5552 - val_mse: 0.5490\n",
      "Epoch 1902/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5482 - mse: 0.5420 - val_loss: 0.5556 - val_mse: 0.5493\n",
      "Epoch 1903/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5467 - mse: 0.5404 - val_loss: 0.5553 - val_mse: 0.5490\n",
      "Epoch 1904/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5538 - mse: 0.5476 - val_loss: 0.5551 - val_mse: 0.5489\n",
      "Epoch 1905/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5485 - mse: 0.5422 - val_loss: 0.5552 - val_mse: 0.5489\n",
      "Epoch 1906/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5520 - mse: 0.5457 - val_loss: 0.5552 - val_mse: 0.5489\n",
      "Epoch 1907/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5494 - mse: 0.5431 - val_loss: 0.5547 - val_mse: 0.5484\n",
      "Epoch 1908/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5494 - mse: 0.5432 - val_loss: 0.5548 - val_mse: 0.5486\n",
      "Epoch 1909/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5441 - mse: 0.5379 - val_loss: 0.5550 - val_mse: 0.5487\n",
      "Epoch 1910/2000\n",
      "272/277 [============================>.] - ETA: 0s - loss: 0.5495 - mse: 0.5433\n",
      "Epoch 01910: saving model to Regression_Model/hepg2.mle.linear-1910.ckpt\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5486 - mse: 0.5423 - val_loss: 0.5550 - val_mse: 0.5487\n",
      "Epoch 1911/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5471 - mse: 0.5408 - val_loss: 0.5547 - val_mse: 0.5484\n",
      "Epoch 1912/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5455 - mse: 0.5392 - val_loss: 0.5548 - val_mse: 0.5486\n",
      "Epoch 1913/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5499 - mse: 0.5437 - val_loss: 0.5547 - val_mse: 0.5485\n",
      "Epoch 1914/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5475 - mse: 0.5412 - val_loss: 0.5546 - val_mse: 0.5484\n",
      "Epoch 1915/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5451 - mse: 0.5388 - val_loss: 0.5551 - val_mse: 0.5488\n",
      "Epoch 1916/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5437 - mse: 0.5374 - val_loss: 0.5548 - val_mse: 0.5486\n",
      "Epoch 1917/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5454 - mse: 0.5391 - val_loss: 0.5549 - val_mse: 0.5487\n",
      "Epoch 1918/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5512 - mse: 0.5450 - val_loss: 0.5548 - val_mse: 0.5486\n",
      "Epoch 1919/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5451 - mse: 0.5388 - val_loss: 0.5548 - val_mse: 0.5485\n",
      "Epoch 1920/2000\n",
      "273/277 [============================>.] - ETA: 0s - loss: 0.5535 - mse: 0.5472\n",
      "Epoch 01920: saving model to Regression_Model/hepg2.mle.linear-1920.ckpt\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5522 - mse: 0.5460 - val_loss: 0.5545 - val_mse: 0.5482\n",
      "Epoch 1921/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5523 - mse: 0.5460 - val_loss: 0.5546 - val_mse: 0.5484\n",
      "Epoch 1922/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5460 - mse: 0.5398 - val_loss: 0.5545 - val_mse: 0.5482\n",
      "Epoch 1923/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5498 - mse: 0.5436 - val_loss: 0.5550 - val_mse: 0.5487\n",
      "Epoch 1924/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5571 - mse: 0.5508 - val_loss: 0.5550 - val_mse: 0.5488\n",
      "Epoch 1925/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5461 - mse: 0.5399 - val_loss: 0.5545 - val_mse: 0.5482\n",
      "Epoch 1926/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5473 - mse: 0.5410 - val_loss: 0.5550 - val_mse: 0.5487\n",
      "Epoch 1927/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5491 - mse: 0.5429 - val_loss: 0.5551 - val_mse: 0.5489\n",
      "Epoch 1928/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5486 - mse: 0.5424 - val_loss: 0.5548 - val_mse: 0.5486\n",
      "Epoch 1929/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5463 - mse: 0.5400 - val_loss: 0.5550 - val_mse: 0.5487\n",
      "Epoch 1930/2000\n",
      "265/277 [===========================>..] - ETA: 0s - loss: 0.5515 - mse: 0.5453\n",
      "Epoch 01930: saving model to Regression_Model/hepg2.mle.linear-1930.ckpt\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5536 - mse: 0.5473 - val_loss: 0.5549 - val_mse: 0.5487\n",
      "Epoch 1931/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5476 - mse: 0.5413 - val_loss: 0.5549 - val_mse: 0.5486\n",
      "Epoch 1932/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5538 - mse: 0.5476 - val_loss: 0.5551 - val_mse: 0.5488\n",
      "Epoch 1933/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5536 - mse: 0.5473 - val_loss: 0.5549 - val_mse: 0.5486\n",
      "Epoch 1934/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5548 - mse: 0.5485 - val_loss: 0.5551 - val_mse: 0.5489\n",
      "Epoch 1935/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5507 - mse: 0.5444 - val_loss: 0.5547 - val_mse: 0.5484\n",
      "Epoch 1936/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5501 - mse: 0.5439 - val_loss: 0.5547 - val_mse: 0.5485\n",
      "Epoch 1937/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5538 - mse: 0.5476 - val_loss: 0.5549 - val_mse: 0.5486\n",
      "Epoch 1938/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5499 - mse: 0.5437 - val_loss: 0.5547 - val_mse: 0.5484\n",
      "Epoch 1939/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5489 - mse: 0.5427 - val_loss: 0.5549 - val_mse: 0.5486\n",
      "Epoch 1940/2000\n",
      "274/277 [============================>.] - ETA: 0s - loss: 0.5518 - mse: 0.5456\n",
      "Epoch 01940: saving model to Regression_Model/hepg2.mle.linear-1940.ckpt\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5530 - mse: 0.5468 - val_loss: 0.5551 - val_mse: 0.5488\n",
      "Epoch 1941/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5481 - mse: 0.5419 - val_loss: 0.5549 - val_mse: 0.5486\n",
      "Epoch 1942/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5401 - mse: 0.5338 - val_loss: 0.5548 - val_mse: 0.5486\n",
      "Epoch 1943/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5567 - mse: 0.5505 - val_loss: 0.5549 - val_mse: 0.5487\n",
      "Epoch 1944/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5478 - mse: 0.5416 - val_loss: 0.5544 - val_mse: 0.5482\n",
      "Epoch 1945/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5538 - mse: 0.5476 - val_loss: 0.5547 - val_mse: 0.5485\n",
      "Epoch 1946/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5464 - mse: 0.5401 - val_loss: 0.5545 - val_mse: 0.5483\n",
      "Epoch 1947/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5469 - mse: 0.5406 - val_loss: 0.5548 - val_mse: 0.5485\n",
      "Epoch 1948/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5445 - mse: 0.5383 - val_loss: 0.5543 - val_mse: 0.5481\n",
      "Epoch 1949/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5462 - mse: 0.5400 - val_loss: 0.5546 - val_mse: 0.5484\n",
      "Epoch 1950/2000\n",
      "263/277 [===========================>..] - ETA: 0s - loss: 0.5479 - mse: 0.5417\n",
      "Epoch 01950: saving model to Regression_Model/hepg2.mle.linear-1950.ckpt\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5459 - mse: 0.5396 - val_loss: 0.5545 - val_mse: 0.5482\n",
      "Epoch 1951/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5533 - mse: 0.5470 - val_loss: 0.5546 - val_mse: 0.5484\n",
      "Epoch 1952/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5527 - mse: 0.5464 - val_loss: 0.5547 - val_mse: 0.5484\n",
      "Epoch 1953/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5511 - mse: 0.5449 - val_loss: 0.5549 - val_mse: 0.5486\n",
      "Epoch 1954/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5482 - mse: 0.5420 - val_loss: 0.5547 - val_mse: 0.5485\n",
      "Epoch 1955/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5437 - mse: 0.5375 - val_loss: 0.5541 - val_mse: 0.5479\n",
      "Epoch 1956/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5498 - mse: 0.5436 - val_loss: 0.5543 - val_mse: 0.5481\n",
      "Epoch 1957/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5449 - mse: 0.5387 - val_loss: 0.5549 - val_mse: 0.5487\n",
      "Epoch 1958/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5472 - mse: 0.5410 - val_loss: 0.5544 - val_mse: 0.5481\n",
      "Epoch 1959/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5515 - mse: 0.5453 - val_loss: 0.5546 - val_mse: 0.5484\n",
      "Epoch 1960/2000\n",
      "264/277 [===========================>..] - ETA: 0s - loss: 0.5480 - mse: 0.5417\n",
      "Epoch 01960: saving model to Regression_Model/hepg2.mle.linear-1960.ckpt\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5500 - mse: 0.5438 - val_loss: 0.5543 - val_mse: 0.5481\n",
      "Epoch 1961/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5476 - mse: 0.5414 - val_loss: 0.5544 - val_mse: 0.5481\n",
      "Epoch 1962/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5499 - mse: 0.5437 - val_loss: 0.5545 - val_mse: 0.5483\n",
      "Epoch 1963/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5505 - mse: 0.5442 - val_loss: 0.5549 - val_mse: 0.5486\n",
      "Epoch 1964/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5416 - mse: 0.5354 - val_loss: 0.5549 - val_mse: 0.5486\n",
      "Epoch 1965/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5519 - mse: 0.5457 - val_loss: 0.5549 - val_mse: 0.5487\n",
      "Epoch 1966/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5523 - mse: 0.5461 - val_loss: 0.5549 - val_mse: 0.5487\n",
      "Epoch 1967/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5550 - mse: 0.5488 - val_loss: 0.5548 - val_mse: 0.5485\n",
      "Epoch 1968/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5538 - mse: 0.5476 - val_loss: 0.5546 - val_mse: 0.5484\n",
      "Epoch 1969/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5517 - mse: 0.5455 - val_loss: 0.5545 - val_mse: 0.5483\n",
      "Epoch 1970/2000\n",
      "269/277 [============================>.] - ETA: 0s - loss: 0.5564 - mse: 0.5502\n",
      "Epoch 01970: saving model to Regression_Model/hepg2.mle.linear-1970.ckpt\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5545 - mse: 0.5482 - val_loss: 0.5549 - val_mse: 0.5486\n",
      "Epoch 1971/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5522 - mse: 0.5459 - val_loss: 0.5545 - val_mse: 0.5482\n",
      "Epoch 1972/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5527 - mse: 0.5465 - val_loss: 0.5548 - val_mse: 0.5485\n",
      "Epoch 1973/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5552 - mse: 0.5489 - val_loss: 0.5546 - val_mse: 0.5483\n",
      "Epoch 1974/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5405 - mse: 0.5343 - val_loss: 0.5548 - val_mse: 0.5486\n",
      "Epoch 1975/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5467 - mse: 0.5405 - val_loss: 0.5547 - val_mse: 0.5484\n",
      "Epoch 1976/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5518 - mse: 0.5456 - val_loss: 0.5547 - val_mse: 0.5485\n",
      "Epoch 1977/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5514 - mse: 0.5451 - val_loss: 0.5547 - val_mse: 0.5485\n",
      "Epoch 1978/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5512 - mse: 0.5450 - val_loss: 0.5548 - val_mse: 0.5485\n",
      "Epoch 1979/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5453 - mse: 0.5390 - val_loss: 0.5547 - val_mse: 0.5484\n",
      "Epoch 1980/2000\n",
      "275/277 [============================>.] - ETA: 0s - loss: 0.5447 - mse: 0.5385\n",
      "Epoch 01980: saving model to Regression_Model/hepg2.mle.linear-1980.ckpt\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5447 - mse: 0.5385 - val_loss: 0.5548 - val_mse: 0.5486\n",
      "Epoch 1981/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5466 - mse: 0.5403 - val_loss: 0.5544 - val_mse: 0.5482\n",
      "Epoch 1982/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5491 - mse: 0.5429 - val_loss: 0.5548 - val_mse: 0.5486\n",
      "Epoch 1983/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5451 - mse: 0.5389 - val_loss: 0.5545 - val_mse: 0.5483\n",
      "Epoch 1984/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5505 - mse: 0.5443 - val_loss: 0.5545 - val_mse: 0.5483\n",
      "Epoch 1985/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5405 - mse: 0.5343 - val_loss: 0.5546 - val_mse: 0.5483\n",
      "Epoch 1986/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5512 - mse: 0.5450 - val_loss: 0.5550 - val_mse: 0.5487\n",
      "Epoch 1987/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5449 - mse: 0.5387 - val_loss: 0.5548 - val_mse: 0.5485\n",
      "Epoch 1988/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5541 - mse: 0.5478 - val_loss: 0.5547 - val_mse: 0.5484\n",
      "Epoch 1989/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5526 - mse: 0.5464 - val_loss: 0.5548 - val_mse: 0.5485\n",
      "Epoch 1990/2000\n",
      "265/277 [===========================>..] - ETA: 0s - loss: 0.5430 - mse: 0.5368\n",
      "Epoch 01990: saving model to Regression_Model/hepg2.mle.linear-1990.ckpt\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5441 - mse: 0.5378 - val_loss: 0.5549 - val_mse: 0.5487\n",
      "Epoch 1991/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5516 - mse: 0.5453 - val_loss: 0.5552 - val_mse: 0.5489\n",
      "Epoch 1992/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5508 - mse: 0.5446 - val_loss: 0.5552 - val_mse: 0.5489\n",
      "Epoch 1993/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5457 - mse: 0.5395 - val_loss: 0.5552 - val_mse: 0.5489\n",
      "Epoch 1994/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5464 - mse: 0.5402 - val_loss: 0.5557 - val_mse: 0.5495\n",
      "Epoch 1995/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5419 - mse: 0.5357 - val_loss: 0.5553 - val_mse: 0.5491\n",
      "Epoch 1996/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5456 - mse: 0.5393 - val_loss: 0.5550 - val_mse: 0.5488\n",
      "Epoch 1997/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5500 - mse: 0.5438 - val_loss: 0.5553 - val_mse: 0.5490\n",
      "Epoch 1998/2000\n",
      "277/277 [==============================] - 1s 4ms/step - loss: 0.5468 - mse: 0.5405 - val_loss: 0.5551 - val_mse: 0.5489\n",
      "Epoch 1999/2000\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5496 - mse: 0.5433 - val_loss: 0.5551 - val_mse: 0.5489\n",
      "Epoch 2000/2000\n",
      "263/277 [===========================>..] - ETA: 0s - loss: 0.5514 - mse: 0.5451\n",
      "Epoch 02000: saving model to Regression_Model/hepg2.mle.linear-2000.ckpt\n",
      "277/277 [==============================] - 1s 5ms/step - loss: 0.5544 - mse: 0.5482 - val_loss: 0.5549 - val_mse: 0.5487\n"
     ]
    }
   ],
   "source": [
    " history = model.fit(training_generator,epochs=2000,validation_data=validation_generator,callbacks=[cp_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss\n",
      "mse\n",
      "val_loss\n",
      "val_mse\n"
     ]
    }
   ],
   "source": [
    "for key in history.history.keys():\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fa7a4b68c18>]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO3deXxU1f3/8dcnG0vCEiAssgWQVUTAgKiIVlHBDffC17ZqVbStVdtqxbovrVp/arUuiK0LaqGuBWVTEQRZhIDsi2xhJyQQtoSQZHJ+f8wkzCSTDbJw4/v5eOTB3Dt37v3kZnjPmXPvPdecc4iIiPdF1HQBIiJSORToIiK1hAJdRKSWUKCLiNQSCnQRkVoiqqY23KxZM5eYmFhTmxcR8aRFixalO+cSwj1XY4GemJhIcnJyTW1eRMSTzGxzSc+py0VEpJZQoIuI1BIKdBGRWkKBLiJSSyjQRURqCQW6iEgtoUAXEaklPBfoP6Ye5IUv15J+6EhNlyIickIpM9DN7C0z221mK0p43szsZTNbb2bLzKxv5Zd51LrUQ7z8zXr2ZuZU5WZERDynPC30d4AhpTw/FOgc+BkJvH78ZZVN9+UQEQlVZqA752YBe0tZZBgw1vnNBxqbWavKKrAos6pas4iIt1VGH3prYGvQ9LbAvGLMbKSZJZtZclpa2nFt1KEmuohIsMoI9HBt5rBp65wb45xLcs4lJSSEHSzsmDYmIiKVE+jbgLZB022AHZWw3lKpD11EJFRlBPpE4FeBs10GAPudczsrYb1hqQ9dRCS8MsdDN7NxwHlAMzPbBjwKRAM450YDk4FLgPVAFnBzVRUrIiIlKzPQnXMjynjeAb+rtIrKSV0uIiKhPHelqA6LioiE58FA99NpiyIioTwX6DooKiISnucCvYD60EVEQnku0NVAFxEJz3OBLiIi4Xku0E2d6CIiYXku0AuoD11EJJTnAl3tcxGR8DwX6AV0HrqISCjPBbq60EVEwvNcoBdQH7qISCjPBbpa6CIi4Xku0EVEJDzPBrp6XEREQnku0E0nLoqIhOW5QC/gdFRURCSE9wJdDXQRkbC8F+gBap+LiITyXKCrgS4iEp7nAr2AutBFREJ5LtA1fK6ISHieC/Sj1EQXEQnmuUBX+1xEJDzPBXoB9aGLiITyXKCrC11EJDzPBXoBNdBFREJ5LtA1louISHieC3QREQnPs4Gug6IiIqE8F+g6KCoiEp7nAr2Ahs8VEQnluUBXA11EJLxyBbqZDTGztWa23sxGhXk+3sw+M7NlZrbAzHpWfqmh1D4XEQlVZqCbWSTwKjAU6AGMMLMeRRb7C7DEOdcL+BXwUmUXerSgKluziIinlaeF3h9Y75zb6JzLAcYDw4os0wOYDuCcWwMkmlmLSq20CHWhi4iEKk+gtwa2Bk1vC8wLthS4GsDM+gPtgTZFV2RmI80s2cyS09LSjqlgXVgkIhJeeQI9XIIWbR8/A8Sb2RLg98APQF6xFzk3xjmX5JxLSkhIqHCxoQWoiS4iEiyqHMtsA9oGTbcBdgQv4Jw7ANwMYP47UGwK/FQ6nYcuIhJeeVroC4HOZtbBzGKA4cDE4AXMrHHgOYBbgVmBkK86aqCLiIQos4XunMszszuBaUAk8JZzbqWZ3RF4fjTQHRhrZj5gFXBLVRWsBrqISHjl6XLBOTcZmFxk3uigx/OAzpVbWhk1VefGREQ8wHtXiqoTXUQkLM8FuoiIhOfZQNeFRSIioTwX6OpxEREJz3OBXkAXFomIhPJcoKuBLiISnucCvYD60EVEQnku0NWHLiISnucCvYAa6CIioTwY6Gqii4iE48FA99NNokVEQnku0NWHLiISnucCvYDa5yIioTwX6Gqgi4iE57lAL6QmuohICM8FuobPFREJz3OBLiIi4Xk20DU4l4hIKM8FeoNZM/jinbupu3VzTZciInJC8VygR+7fR8/UDUQcPlzTpYiInFA8F+gWUXBQVF0uIiLBPBfoBZeKuvwarkNE5ATj2UA3jeUiIhLCs4GuwblEREJ5NtB1yyIRkVAeDnR1oouIBPNwoKuFLiISTIEuIlJLKNBFRGoJzwa68lxEJJTnAt3MX7LOQxcRCeW5QNdZLiIi4Xku0AvHcslXC11EJFi5At3MhpjZWjNbb2ajwjzfyMw+N7OlZrbSzG6u/FID2woEuk8tdBGREGUGuplFAq8CQ4EewAgz61Fksd8Bq5xzpwHnAc+bWUwl1wpARIS/5HyfWugiIsHK00LvD6x3zm10zuUA44FhRZZxQAPz3/AzDtgL5FVqpQEFga7TXEREQpUn0FsDW4OmtwXmBXsF6A7sAJYDdztXvE/EzEaaWbKZJaelpR1TwQVdLvn56nIREQlWnkC3MPOKNo8vBpYAJwG9gVfMrGGxFzk3xjmX5JxLSkhIqHCxAGYKdBGRcMoT6NuAtkHTbfC3xIPdDHzq/NYDm4BulVNiqIjIQMk6y0VEJER5An0h0NnMOgQOdA4HJhZZZgtwAYCZtQC6Ahsrs9ACEVZwlktVrF1ExLuiylrAOZdnZncC04BI4C3n3EozuyPw/GjgSeAdM1uOv4vmfudcelUUrD50EZHwygx0AOfcZGBykXmjgx7vAC6q3NLCi4jQ4FwiIuF47krRiMBYLmqhi4iE8lygW2TBhUUKdBGRYJ4L9IILi3STaBGRUB4M9MB46DptUUQkhOcCXWe5iIiE57lAjyzoclELXUQkhOcCPUKBLiISlucCvaDLJczYXyIiP2keDHS10EVEwvFcoBfcU9TpoKiISAjPBnq+zkMXEQnh2UBXC11EJJR3A10tdBGREN4NdA2ILiISwruBrtMWRURCeDbQ83XaoohICM8GuvrQRURCeTfQdZaLiEgIDwe6WugiIsE8HOhqoYuIBPNsoOcp0EVEQng20H15CnQRkWDeDXS10EVEQngv0AN8OigqIhLCe4GuLhcRkbC8G+jqchERCeHZQNeVoiIiobwb6Gqhi4iE8HCgq4UuIhLMu4GuLhcRkRDeDXR1uYiIhPBsoKMWuohICM8GurpcRERClSvQzWyIma01s/VmNirM8/eZ2ZLAzwoz85lZk8ovFx0UFREpQZmBbmaRwKvAUKAHMMLMegQv45x7zjnX2znXG3gA+NY5t7cqClYfuohIeOVpofcH1jvnNjrncoDxwLBSlh8BjKuM4sJSl4uISFjlCfTWwNag6W2BecWYWX1gCPDJ8ZdWAnW5iIiEVZ5AtzDzSkrTy4E5JXW3mNlIM0s2s+S0tLTy1lh0JYEKFOgiIsHKE+jbgLZB022AHSUsO5xSulucc2Occ0nOuaSEhITyVxksEOi5eT4NoSsiEqQ8gb4Q6GxmHcwsBn9oTyy6kJk1As4FJlRuicU2BECeL5+9mTlVuikRES+JKmsB51yemd0JTAMigbeccyvN7I7A86MDi14FfOmcy6yyaqEw0M053VdURCRImYEO4JybDEwuMm90kel3gHcqq7ASFQQ6kOdTl4uISAHPXimKc9z+3qKarUVE5ATi2UA3HKt2HqjhYkREThzeDXSdtigiEsK7gV7DZYiInGi8G+hqoYuIhPBuoJd4saqIyE+TdwNdeS4iEsKzgS4iIqG8F+gBBV0uGkZXRMTPe4EeaKGf0SEeQAN0iYgEeDbQY6MjAViydV9NViMicsLwbKBnHskF4Oa3F9ZkNSIiJwzPBjqBrhYdIxUR8fNsoOf5/EPnHsjOq8lqREROGJ4PdIDdB7JrqhoRkROGdwM9z1c4a9u+wzVVjYjICcN7gR7hL/nsjk0KZ2Xn+EpaWkTkJ8Ozgd4iM6Nw1uFcBbqIiPcCPdJ//jmvvFI4K1MtdBERDwZ6RPGSJy/bWQOFiIicWLwX6EF+1jUBgKkrdzFz7e4arkZEpGZ5OtDf/FVS4eObdMWoiPzEeTrQoyI9Xb6ISKXydiJmZzN+5IDCybvG/UDiqEkagVFEfpK8HeibNjGgY1OaxsYAMHHpDgAyczQcgIj89Hg70AMysnJCprfuzaqhSkREao63Az0wDECXFg1CZl/68nf84l/fk6WWuoj8hNSKQH//1jOKPfXd+nSem7a2uisSEakx3g70gGZxdcLOf3tOSvUWIiJSg7wd6EF3t/j2vvPCLpI4ahLXvzGPxFGT2J+VW02FiYhUv1oT6O2a1C9xsQWb9gLwwldreeDTZTin0xpFpPaJqukCjtvEiZCait12W5mLvjtvMwDdWzWkf4cmdGvZsKqrExGpNt5uoTsHw4bByJEArH1qCJ0SYst82SMTVjLkH7OZsGQ7iaMmsWu/7ngkIt7n7UDv2jVksk5UJNP/dB6PXNajXC+/e/wSAKavSa300kREqlu5At3MhpjZWjNbb2ajSljmPDNbYmYrzezbyi2zYm4+O5HICCt7wYB8B+t3H9S9SUXE08oMdDOLBF4FhgI9gBFm1qPIMo2B14ArnHOnANdVQa2l+81vYPv2gnpIfnAwV5x2Urle+vD/VjD4hVkMeHp6seeyc338d+EWHUgVkRNeeVro/YH1zrmNzrkcYDwwrMgy/wd86pzbAuCcq/7ByUePhtdeK5yMj43hpeG9mf3nn5V7FfkOxs5LCZn3/Jdruf+T5Xy9WuOti8iJrTxnubQGtgZNbwOKXprZBYg2s5lAA+Al59zYoisys5HASIB27dodS72l84Xeis7MaNukPh/dcSazf0wjrm4Uf5u8ptRVPDJhJbExUZzSuiFZOT52Bg6YHjqic9hF5MRWnkAP1xldtP8hCjgduACoB8wzs/nOuR9DXuTcGGAMQFJSUuX3YYS5PR1Av8Qm9EtsAsCOfdm8Mzel1NX86aOlxVdtxXfD/sO5NKoXXfE6RUSqQHm6XLYBbYOm2wA7wiwz1TmX6ZxLB2YBp1VOiWHcfnv4+WFCt6jf/ezk4qv7/mPGfPpUqa+7e/wSnHNMXbGTdakHWbR5L6c9/iVfr9IZMiJyYihPC30h0NnMOgDbgeH4+8yDTQBeMbMoIAZ/l8yLlVloiGbNws8vaKG/9BIMGgR9+hR/aVwMd51/Mi9/s75w3gMz3ynXZjs8MLnYvFvHJrPkkQtZs+sgAzo2BWDVjgM0qBtF21KuXhURqWxlBrpzLs/M7gSmAZHAW865lWZ2R+D50c651WY2FVgG5AP/cs6tqLKqi/SVFypood9zT0HxYRYx/nhRV+4Z3IXHP1/J2Sc3g2ePr5zeT3wFwNJHL6JRvWgueXk2ACnPXHp8KxYRqYByXfrvnJsMTC4yb3SR6eeA5yqvtFK0bx9+fgl96OEXNR4f1jNk3td/HMS1o+ex7xgH8fphSwYp6ZmF0xmZOURFGveMX8L9Q7uFjNu+fd9h5qxPp1vLBvRq0/iYticiEsxq6vzqpKQkl5ycfGwvzs+H996Dm24Knf+LX8DYsUeDvTy/29q10K1byPKJoyYdW13l9Ksz2zM2MK4MHG3Jz12fztSVu3hiWE+Wb9vP9n1ZDOnZqkprERFvMbNFzrmkcM9589L/iAi46KLi899/Hz799Oj0mDGwpvTTFEOWD/jqD4Po0iIOgPZNQ/vBuxa5O9KxCA5zgHkb9vCnD5fyf//6nrHzNnPd6Llc/sp33PH+Yr5Zk0riqElsyyj5tno/bMlgb6b/Nnwb0g4dd30i4k3eHW2xSZPw838MOlPy9tuhbl04fLhCq+7cogFf/uFcAPJ8+eTlO/7y2XI+XbydJ6/sSZPYaAa/MKtw+cHrvqfD3u28ecbVFf41AEa8OT9kemFKRuHj12duAGDgszP44vcD6dm6UeFz2bk+cnz5XPXaXLq3asj9Q7py09sLuaBbc/59U79St7k3MwdfviOhQfibg4SzYNNeerZuSP0Y775tRGoz7/7PrFPHf6n/jh3QLyi8/vKX0OWys2HjRoiJgagoaNmyQpuJiowgKhJeuL43L1zfu9jzV/dtzQvPPglwzIFemuBwv+yf3xU+vn9IN56devTbx+qdB1iX6m+dT1+zmy+W7eCyXsWHPtiyJ4tb3l3Iut3+Zdf9dSj5znHTWwu5Z3BnzgicqVPUj6kHuf6Nedx4Zvtixx5E5MTgzT70ospx/nmhf/wD7r776PTTTx/9EKjAvvhuXTpfrvL3dxds//mn3uP91Agy6jcq49XVI+WZS3nru020aFiXI3k+/vhh8QumADo2i2Vj4GBu0TNztuzJol3T+kxbuYvb31tErzaNqBMVwas39KV5g7pV/juISKjS+tB/eoEO8N13/n74BQsgK+uYAr2k7e9q2Y6UOYsZPuZoN8qw3icxYUnRa7FCxeTlkhcRQX5E5LHVEEb3Vg1ZvfNAhV6T8syl7M3MIS8/nw/mb+Gl6ev44NYzuOFf3xdb9skre7JtbxYPXNK9cN78jXvo0CyWejGRbNmTFdJFdLzOeno6ic1i+c9tAyptnSJeU1qge7fLJdiYMYU3uSiXgQPLXubwYbjmGnjxxWLjrodIDb1StOWuLdRrFXonpGev6cWEJTuok5fDC9f1IqF5PNe/MS9kmR+fv4rZ7Xvzy+GlX7FaERUNc4B/zd7IU5NWh8xLDur2Cfbw//yXGrwxayMAU+4+h+Fj5tO4fjRt4uuxYvsBxt02gP2Hczi1TWNaN65XbB2Tlu3kb5NX89nvziqxxT9vwx56t23Mjv3Z7Ai6GYlzjoUpGfRLjMfK+FB3znEkL5+60ZX3gSlyoqkdLXSoeCs9nOB9MWUKXHJJ8flFXXUV/O9/ofMOHCC3fiyb0jPJzvXR659Pk//KK/gwon155GZm0fnBKQAsf+wiDuf4aN7IH3bLt2Rw+atzStxcTF4uv53/IaPPuIbs6KAAdI62+1PZ2jj0GEFkvg9fJbb6j1di0/rMvM8/Auat7ybz9Wr/B2JS+3g+uuNMMrJyia0TyYbdmbRpUo8FG/dy69jQ90lBt9DnS3fw+3E/8Pdre3F9UluSnvqaC3s05+mrexXb7ivfrOP/ffkjSx+5iEb1w4+/s2VPFo3qRZf4PMDhHB/RkUZUpDdPEBPvq/0tdIAffgh7qX+FzJgBW7dCYuLRMAeYNQvuvx8efRRyc+HIEbjySn/QFw1zgIYNiX76abrceSc0agr5+URw9BzRqLxcOtZzvJAxnwbufBo0PNpy7XnTNcyKa0bEiOGcuzgSi4rCcnNovX83zfueSvdP3uWeOeNwGC8NPDoCw5S3f0/3tBQeu2Ak7yRdAcBZKUv4z38fYtgvn2fpSaHfMtrsT+VAnVgO1I0Luyv6bV3B0lZdyYmq3MHHUvZkcfGLszi3a0JhmAMkb87gghe+ZWPa0QuzoiONXF/xD9M73ltE6/h6NI2LAeDvU9cwtGdL0g8dYdyCrYxbsBUzGNG/HX+76lQA/vXdJgDO/X8z2JeVy+nt4/nkN2eRk5dPl4emcN/FXXlu2lpaN67HnFHnA/Dtj2kktY8nts7R/ybdH5nKeV0T+OtVpxIdYTSLq0PKnkw6Jvj3Y0EDqeAbw4JNe/HlO87sFP5gc2l27DtM5pE8Oh/nqbJZOXlkZOWG/YZUYNHmDDq3iKNhXQ0252W1J9B79/YHbEaG/xz1Y2n9n39++Pnn+k9hZOjQ8q/rgQfgm2/8F0EVYQkJfHPwoH9i7SL/GTsFz33zDe0AJn7IBoBFi8h59jliPhwPO3bw+gT/sAc39mrGv+tEcehIHt13b6R7WgoAj00fQ9fsPYyY8wmrmncA4LfzP2JO+9PIiqlHbkQkE075Gd+NvgWAa274O6ubdyArxv+fvU5eDlet+IZnpr0CQErjVtx+9YOsTUgsrPGKVTNZ2aITG5oeHbPt2ckvMatDX5pl7eNAnVjq5uUwYMtyPul5PvPa9yI3IqrwW9Ta1IOsTT0Ysk/6bl9Now2H2Njp6BlL4cIcYOrKXTTMDpxvXzeO9EM5nPrYlyHL/HrB/9i2pjkze7SgXZP69Fk+l6emvcb5I9+AqBgWbc5gxprd5OX7t/HqDP/YPtv3HSYnL59v1qRyx/uL6d+hCXec25Ffv5PMiP7+33fm2jTOfuabkO0N630Sj15+Cs9NW8O4BVuZ/8AFtGxUt7BrbdUTF4ec7pnnyycywjAzNqVnEl8/msb1Ywqfe2n6Ov4ZGG+orCEkDmTncig7j5NKCOwb31rAwpQMNv7tEvZk5hQ7VTU718c1r8/l7JOb8sGt/uMTO/YdZvqa3fxygP+q7HWpB+nQLJaoyAjy8x0LUvYWjl0ULPVANnF1okI+BL0k/dARtuzNon5MpCdvIl97ulyCOQeffw4tWsCAWnQA7eKLYdq0wkmXnEzGjDk0ue/uUl5U3E3XPso7Hz8eMu/LZdtJe+JphkwaS9PDxfve9147nPvzT2Zd07bMfNM/2uWwXz5P0rZVJG1fzdAf55a6za879eOVs4YzbNVM0mLj2R3XhI9PHQzAoI2LGPvRowAMufmfHI6uw7sfPsrvht1Pm/27WdSmO1eunMF5Gxfx62sfIycqmpRnL/O/duSbNMvax+LWRw/Mxmft54d/3gBA4v1fUCcvh1UvXEuky+fckWPY2SCB3MhInPm/M0Xk++i8ZyufvH8f9w29myndjh5jifLlcc2K6Xx06uDCA9Z3zP+YhMwMnrzgtpDf8axOTZm7YU/h9F8u6ca742ZhLp+0uCaMHtiUn105iHkb9hRee/Diz0/jD//1n31070VdaNmoHvcWGb75gm7NSc/M4ZURfQoHfMs8kseIN+fzhwu78OCny9mxP5v1fx1KVGQEB7NzCz/gVj8xhO6PTAXgt+d14rWZG/jniD70bN2IDs38N1Qfv2ALoz5dTpPYGBY/fGHhtxaARQ8NJivHxzl/n8Htgzpy78VdC7sLH7/iFG44o13hBxP4r7LulBDL9D+dV+J7wTlHRlYudaIiiDCjXox/v361KpXdB7O54Yz2HMnzcSg7j2krU/m/M9rhnGPuhj3MXpfO0J4tOa1t5QyXsXzbfn5MPciSrfuIjDA+TN5KVo6/0fTtfedx29hkPrh1QIWu16hqtf8sl9K0a+fvRpHSRUaWPOhZFbnt6od4s4xhi8trS6MWvDjwBua168X8128qnP95t3O4fM3swul/nD2Ce+aMA+DqXzxHj9SNPPXV6yHr2t4ggccuvD2ktjyL4IkLbqPj3u3ctPgLAK741Qs8+eXrnLZrHV90HcgjF/2G1z/7G2sTEnmz/1Vcu3w6d88dF7Luh575iJ+/OIqvTz6DfDO+Obk/n753L5viT2Lk1Q+xo2ECl6ydwxWrZvJF90HE5Rzmv70upNOebVy1cgZX923D3ilf0XXzahad1I2/nn8LDbMz2R3XhFXNO/D3S05m/JtfcM+c/7Cg7Slcu3Y2CRmpxOZmc/O1jzK3XS/67lhD/OGDNDl8gFN/fR3/+DaFw9F1SNq2miH9OvD19+uY2fF06uce4TdDTyV6zSr2fvo561t24KpTErg3ozk3L5rIPXPGMaPj6bw24DpWNe9Iny6tWLl6C1ev+IbTH7qbgf1PZvr36znwyOPEP3AvV3RqxMfpkXyWvIU5u7KJ8eXRsHEs40eeSb3dqQwauxKfRXBln9b8L+issK+u78SV7y2na/pmtjdMYF98c+4Z3IUJS7Yz5e5z+GT+Ru6dsIb+DR1vndWIuIFnsuv7H8iIi2drZCzxUdCvVX2y68dRNycbfD52pmYQh4+Rj4xnZYtOtDqYzsE69dnZoJm/ruxMhgzuzSez1vDw4I6MOLsTbN7MZ9kNmbJiFzee08k/sJ8Zc9anM2PNbh66rAeLNu9l5/7ssNeAALBnD8TFhXwrr6jSAh3nXI38nH766a5a/PnPzvnb7M4lJTnXrt3Raf3oRz8n5M/mRi2cA5cdGVXiMnkWUeN1OnA745o4B25/ndjCxwU/uwLTubFxLjeuQeH8FXc9cMyRBiQ7Fz5Xa/+h+ocegkmT/H3ZCxfC5s2Qlgavvgp33VV8+chIuOEGePNNWL0aVq7098k//DC0KmGgrMceg6VBX5MHDy65nmFFbsd6+umh0z2DrsJs0aLUX63Q4sWQkFC+ZQvUrcBFQVFB/aEtW8KQIeV73YoV/n08alTFagvY3Dj8Vb2pcU3Y0aCEMfGlymVF+1uX37U/rUr+Dgdj6tHikL/rqo4vr8Tl0mL93S4L2vQgOajLraj0+o14+/TL+XfSMN7sd2Vh/R/3vIAZHU8vXGZ2+978r8e5TOlyFgDTOg/gy84DWJ2QyAsDb+DzbufwSc/z2VOvIV90O4dZiX3I696jcDvbGjVnc3xoyzwtNh6AlQ1asddFsa5pW5a06sKm+NYV3S3lUvu7XErj8/kPXv7pT/6vQI3L6JdzDubMgbPPhhtv9I/4+N57/g8AM//z2dlQrx68/Tb06gWHDvlvtrFtG+zf7w/s/fv9o0L27etfF8C+ff4PmsREuOwyf9//44F+7rffhh49/MHarZt/Xf36wbPPwi23+Ic18Pn8Adqli/81hw/7x7vJzIScHIiP99f3+uv+4YcvvdT/IWfm75Lavt2/Dz74AJ57rvhQxD/8ALNnh34I7t4NKSkc6NKdKesy+HnbGP/NR/LzISXlaC3B9u3z75P9+/0fqhER8PzzoV9BneNQjo+oCOO+j5fx86S2DOzsD46np6xm1/5smjeow5uzN3FulwTa7dxI6qIVvF4vhcgrLuf9Jqfw0PTNzHvgfD5K3sbp7eO56Y3vyI2MpuOebZhzPDGgKR9O+J68iCi+Prk/XdM3s6xlZxpnHyQzph55EZGYczQ8kknLg3s4WCeW1gd2s/ikbvgiIrhpYEf+M3MtrQ6ms6tBU7Kj63LajrVk1GtIVkzdwtNF0+s3pltaCsNPjmX/pK+Y1aEPK1p04khUDJhRNzebhMx9pMU2pu/2NTTKPkTHvdt544xryIv0f5A2zD6EzyLIrFPyDVOifHn02bGGVgfTmdj93JJP43Wu3Kf41svJ5khUdKVe7HbcnCMhM4O0uBLGcvKIJ4edwi/PTDym1/60+9ClVtp9IJv+f5vO2zf342ddm5e5/NQVu7jj/UUA/HNEHy4/7ST2Z+XSqH40t767kK9X7+akRnVDLlwCiK8fTUaY8fFTnrmUYa/OYenWfWG3F23M6/0AAAk6SURBVPy6d3/dn3O7+L9BFQzN3LxBHd66qV/h+DwN6kZxMDu0NfqvXyUVOwdfaof7Lu4a9naY5aFAFymFc44NaYdo1ySWXfuzqRcTyVnPTCfX55g76nzmb9xDTFQE3Vo2ZPiY+TSNjWHaHwYVvvbLVanc/p7/w+LXZ3fg+n5tyPO5wrAOPu1wza4DvD9/M/de1JVG9aJ5d24K9WOi6NwijqteCz1TqGAYhr5P+u+INaJ/W8Yt2Mrndw7kjVkb+GLZzsJllz12Eb2KnLr5+BWn8OjElYA/QI7k5fPy9HVl7o81Tw6h28NTS13m9kEdC68QPl7X9G3DJ4u3Vcq6wmkaG8OewPDSJ4opd59D91bHdlqkAl2kgrJzfeQ7V+6hgvv/9Wt6tm7EW4Fhi/N8+Tw8YQW3D+pEYuD0wNIUnGr4yGU9eOKLVcDRD4LsXB+LN2dwZqem5PocMVH+7rAV2/cz7NU5zBt1Ps0b1uXzpTvo3qohi7dkEF8/hgt7tCj8RpDyzKU45+jwwGRuO6cDF3RvETLe0HldE7jxrEQa1IkiKbFJ4evmPXA+Bw7n0bZJPepGRdLxL/4bl712Q19++8FiACbdNZBLX/6OZ64+lfRDR+jbLp7f/mdxqXf+umdwZ47k5TPynI40rh9N90emkp2bz+/PP7nw/PvyuOK0k7hlYAeGhbm6+tJerbj3oq50aBbL6p0HeGbKGr79MY3B3ZtzJC+f2evSy72dsjx6eQ+u6tO68HaUZTme21Mq0EU8ID/fYQZpB48QFRlBk9iY417n3A3ptGtSnzbxxfvfv9+4h79OXs2Ht59ZbIybDxf6r7a9LqltyPwuD04hx5dPyjOX8uzUNcz6MY2Jdw4kMqJ4v/zWvVnk+PKZuz6dhyesZOXjF5d4wdGeQ0dIP5RD15YNyM93XPfGPJrExvDVqqNXE48c1JFd+7N58NLupB7I5sDhvMJjKwUfQC8N701OXj6N6kVz0SmhB9VzfflMX72bi09pwfrdh7jwxVmEc9NZibwzNwWA28/tyIXdW9C5eQO+XZfGXeN+4KM7zuS60fOIjjQm33UOdaMjC68PKKhj9p9/xl8nrWbqyl3F1t8sLobkhy4Mu+3yUKCLSKVIP3SE7Fxf2A+IqrAvKwfnYE/mEU5uXvIQCImjJtGzdUO++P055V735OU7+e0Hi7nhjHbcfHYHzPzfhk45qRGvzljPWZ2a0qddfLHXFXSDBQ8TUeD+j5fRolFd/nhhF/J8+ezNzOHzZTvp3DyOr1alct+QrtSPjjyusYAU6CJSq+3NzKF+TGSFRtN0zjFp+U6GnNKyQgHrnOO1mRu49NRW5epOq2w/jcG5ROQn61i6p8ys5Cs6y3jdsZ6hUtVq/4VFIiI/EQp0EZFaQoEuIlJLKNBFRGoJBbqISC2hQBcRqSUU6CIitYQCXUSklqixK0XNLA3YfIwvbwZU3sg6ledErQtO3NpUV8WoroqpjXW1d86FvaNNjQX68TCz5JIufa1JJ2pdcOLWproqRnVVzE+tLnW5iIjUEgp0EZFawquBPqamCyjBiVoXnLi1qa6KUV0V85Oqy5N96CIiUpxXW+giIlKEAl1EpJbwXKCb2RAzW2tm681sVDVvu62ZzTCz1Wa20szuDsx/zMy2m9mSwM8lQa95IFDrWjO7uAprSzGz5YHtJwfmNTGzr8xsXeDf+KDlq7wuM+satE+WmNkBM7unJvaXmb1lZrvNbEXQvArvHzM7PbCf15vZy2ZW/Gaax1/Xc2a2xsyWmdlnZtY4MD/RzA4H7bfR1VxXhf9u1VTXf4NqSjGzJYH51bm/SsqG6n2POec88wNEAhuAjkAMsBToUY3bbwX0DTxuAPwI9AAeA+4Ns3yPQI11gA6B2iOrqLYUoFmReX8HRgUejwKere66ivztdgHta2J/AYOAvsCK49k/wALgTMCAKcDQKqjrIiAq8PjZoLoSg5crsp7qqKvCf7fqqKvI888Dj9TA/iopG6r1Pea1Fnp/YL1zbqNzLgcYDwyrro0753Y65xYHHh8EVgOtS3nJMGC8c+6Ic24TsB7/71BdhgHvBh6/C1xZg3VdAGxwzpV2dXCV1eWcmwXsDbO9cu8fM2sFNHTOzXP+/3ljg15TaXU55750zuUFJucDbUpbR3XVVYoa3V8FAi3Z64Fxpa2jiuoqKRuq9T3mtUBvDWwNmt5G6YFaZcwsEegDfB+YdWfgK/JbQV+rqrNeB3xpZovMbGRgXgvn3E7wv+GA5jVQV4HhhP5Hq+n9BRXfP60Dj6urPoBf42+lFehgZj+Y2bdmVnCL++qsqyJ/t+reX+cAqc65dUHzqn1/FcmGan2PeS3Qw/UlVft5l2YWB3wC3OOcOwC8DnQCegM78X/tg+qt92znXF9gKPA7MxtUyrLVuh/NLAa4AvgoMOtE2F+lKamO6t5vDwJ5wAeBWTuBds65PsAfgf+YWcNqrKuif7fq/nuOILTRUO37K0w2lLhoCTUcV21eC/RtQNug6TbAjuoswMyi8f/BPnDOfQrgnEt1zvmcc/nAmxztJqi2ep1zOwL/7gY+C9SQGvgKV/A1c3d11xUwFFjsnEsN1Fjj+yugovtnG6HdH1VWn5ndCFwG3BD46k3g6/mewONF+Ptdu1RXXcfwd6vO/RUFXA38N6jeat1f4bKBan6PeS3QFwKdzaxDoNU3HJhYXRsP9NH9G1jtnHshaH6roMWuAgqOwE8EhptZHTPrAHTGf8CjsuuKNbMGBY/xH1RbEdj+jYHFbgQmVGddQUJaTjW9v4JUaP8EvjIfNLMBgffCr4JeU2nMbAhwP3CFcy4raH6CmUUGHncM1LWxGuuq0N+tuuoKGAyscc4VdldU5/4qKRuo7vfY8RzZrYkf4BL8R5A3AA9W87YH4v/6swxYEvi5BHgPWB6YPxFoFfSaBwO1ruU4j6SXUldH/EfMlwIrC/YL0BSYDqwL/NukOusKbKc+sAdoFDSv2vcX/g+UnUAu/lbQLceyf4Ak/EG2AXiFwNXWlVzXevz9qwXvsdGBZa8J/H2XAouBy6u5rgr/3aqjrsD8d4A7iixbnfurpGyo1veYLv0XEaklvNblIiIiJVCgi4jUEgp0EZFaQoEuIlJLKNBFRGoJBbqISC2hQBcRqSX+PxxqR4SrwiD0AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(history.history['loss'], label='Loss of training data')\n",
    "plt.plot(history.history['val_loss'], color='red', label='Loss of validation data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(x,y,pasid,trainid,dataset,label_mean,label_std,bestEpoch=2000):\n",
    "    test_generator = EvaDataGenerator(x,LENGTH)\n",
    "    model = Regression_CNN(LENGTH)\n",
    "    MODEL_PATH = 'Regression_Model/'+trainid+'-'+str(bestEpoch)+'.ckpt'\n",
    "    model.load_weights(MODEL_PATH)\n",
    "    print('load weight success')\n",
    "    pred = model.predict_generator(test_generator)\n",
    "    OUT=open(dataset+'.'+trainid,'w')\n",
    "    OUT.write(\"pas_id\\tpredict\\tpolyA\\tpredict_readCount\\tpolyA_readCount\\n\")\n",
    "    for i in range(len(pred)):\n",
    "        predict = pred[i][0]\n",
    "        predict_readCount = np.exp(predict*label_std+label_mean)\n",
    "        truth = y[i]\n",
    "        truth_readCount   = np.exp(truth*label_std+label_mean)\n",
    "        OUT.write('%s\\t%s\\t%s\\t%s\\t%s\\n'%(pasid[i],predict,truth,predict_readCount,truth_readCount))\n",
    "    OUT.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load weight success\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 8889 is out of bounds for axis 0 with size 8889",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-95fb84d635c1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtestid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'hepg2.mle.linear'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtestid\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel_mean\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel_std\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-28-444b6731095d>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(x, y, pasid, trainid, dataset, label_mean, label_std, bestEpoch)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mtruth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mtruth_readCount\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruth\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlabel_std\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mlabel_mean\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mOUT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'%s\\t%s\\t%s\\t%s\\t%s\\n'\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpasid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtruth\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpredict_readCount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtruth_readCount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mOUT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 8889 is out of bounds for axis 0 with size 8889"
     ]
    }
   ],
   "source": [
    "testid='hepg2.mle.linear'\n",
    "evaluate(train_x,train_y,train_id,testid,'train',label_mean,label_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(valid_x,valid_y,valid_id,testid,'valid',label_mean,label_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of dataset: 11575\n"
     ]
    }
   ],
   "source": [
    "#train_data,train_labels,train_pasid,valid_data,valid_labels,valid_pasid = prep_data('coverage_data/thle2_control.pAs.usage.txt',5)\n",
    "train_data,train_labels,train_pasid,valid_data,valid_labels,valid_pasid = prep_data('coverage_data/Finetune.snu398_control.usage.txt',5)\n",
    "data_mean,data_std,data_max,label_mean,label_std, train_x,train_y,valid_x,valid_y = normalization(train_data,train_labels,valid_data,valid_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load weight success\n",
      "load weight success\n"
     ]
    }
   ],
   "source": [
    "LENGTH=1001\n",
    "testid='thle2.mle.linear'\n",
    "bestEpoch = '2000'\n",
    "evaluate(train_x,train_y,train_pasid,testid,'train',label_mean,label_std,bestEpoch)\n",
    "evaluate(valid_x,valid_y,valid_pasid,testid,'valid',label_mean,label_std,bestEpoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(train_data,train_labels,data_mean,data_std,label_mean,label_std):\n",
    "    train_data = np.log(train_data+0.01)\n",
    "    train_labels = np.log(train_labels+1)\n",
    "    train_data = (train_data-data_mean)/data_std\n",
    "    train_labels = (train_labels-label_mean)/label_std\n",
    "    \n",
    "    return train_data,train_labels\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate1(x,y,pasid,trainid,dataset,label_mean,label_std,bestEpoch=2000):\n",
    "    test_generator = EvaDataGenerator(x,LENGTH)\n",
    "    model = Regression_CNN(LENGTH)\n",
    "    MODEL_PATH = 'Regression_Model/'+trainid+'-'+str(bestEpoch)+'.ckpt'\n",
    "    model.load_weights(MODEL_PATH)\n",
    "    print('load weight success')\n",
    "    pred = model.predict_generator(test_generator)\n",
    "    OUT=open(dataset+'.'+trainid,'w')\n",
    "    OUT.write(\"pas_id\\tpredict\\tpolyA\\tpredict_readCount\\tpolyA_readCount\\n\")\n",
    "    for i in range(len(pred)):\n",
    "        predict = pred[i][0]\n",
    "        predict_readCount = np.exp(predict*label_std+label_mean)-1\n",
    "        truth = y[i]\n",
    "        truth_readCount   = np.exp(truth*label_std+label_mean)-1\n",
    "        OUT.write('%s\\t%s\\t%s\\t%s\\t%s\\n'%(pasid[i],predict,truth,predict_readCount,truth_readCount))\n",
    "    OUT.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of dataset: 12803\n"
     ]
    }
   ],
   "source": [
    "train_data1,train_labels1,train_pasid1,valid_data1,valid_labels1,valid_pasid1 = prep_data('coverage_data/all.snu398_control.usage.txt',5)\n",
    "x=np.concatenate((train_data1, valid_data1), axis=0)\n",
    "y=np.concatenate((train_labels1, valid_labels1), axis=0)\n",
    "pasid=np.concatenate((train_pasid1, valid_pasid1), axis=0)\n",
    "x,y= standardize(x,y,data_mean,data_std,label_mean,label_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load weight success\n"
     ]
    }
   ],
   "source": [
    "LENGTH=1001\n",
    "testid='Regression.f_snu398.shift16.1001'\n",
    "bestEpoch = '2000'\n",
    "evaluate1(x,y,pasid,testid,'all',label_mean,label_std,bestEpoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11408,)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.concatenate((train_pasid1, valid_pasid1), axis=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9126,)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1,labels,pasid = get_data('usage_data/BL6_REP1.pAs.predict.coverage.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits = 5,random_state=len(labels)+1,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = list(kf.split(labels))\n",
    "train_index,valid_index = folds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUT=open('test','w')\n",
    "OUT.write(\"RNA_readCount\\n\")\n",
    "for coverage in valid_data:\n",
    "    mean = np.mean(coverage)\n",
    "    OUT.write('%s\\n'%(mean))\n",
    "    #for i in coverage:\n",
    "     #   OUT.write('%s\\n'%(i[0]))\n",
    "OUT.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.min(train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
