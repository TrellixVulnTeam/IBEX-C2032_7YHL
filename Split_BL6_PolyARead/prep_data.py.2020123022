#!/usr/bin/env python3

import numpy as np
import os
import sys
import argparse
import glob
from sklearn.model_selection import KFold

#from supporting import *


def get_files(root_dir,block_dir,RoundName,label):
	Round = int(RoundName[-1])
	blockfiles = glob.glob(block_dir+"/chr*")
	blockfiles = np.array(sorted(blockfiles))
	if(label=='positive'):
		root_dir = root_dir+'/positive/'+RoundName[0:-1]
	else:
		root_dir = root_dir+'/negative/'+RoundName
	files = []
	for f in blockfiles:
		f2 = f.replace(block_dir,root_dir)
		files.append(f2)
	files = np.array(files)
	np.random.seed(0)
	####Chose 21 blocks as Test Set
	test_index = np.random.choice(range(len(files)),21,replace=False)
	####The remaining sets as train and valid sets
	new_files = np.delete(files,test_index)
	#### 5 Fold cross validation
	kf = KFold(n_splits = 5,random_state=0,shuffle=True)
	train_round = []
	valid_round = []
	for train,valid in kf.split(new_files):
		train_round.append(train)
		valid_round.append(valid)

	train_sets = new_files[train_round[Round]]
	valid_sets = new_files[valid_round[Round]]
	test_sets  = files[test_index]

	return train_sets,valid_sets,test_sets


def get_data(root_dir,block_dir,polyA_file,RoundName,label,split):
	"""
	Process all files in the input directory to read sequences.
	Sequences are encoded with one-hot.
	data_root: input directory
	label: the label (True or False) for the sequences in the input directory
	"""
	data1 = [] # seq
	data2 = [] # coverage
	data3 = [] # polyA containing reads 
	
	train_sets,valid_sets,_ = get_files(root_dir,block_dir,RoundName,label)


	sets = None
	if split == 'train':
		sets = train_sets
	elif split == 'valid':
		sets = valid_sets

	polyA_dict = dict()
	with open(polyA_file,'r') as f:
		for line in f:
			line=line.rstrip('\n').split('\t')
			polyA_dict[line[0]] = line[1]

			
	for file_path in sets:
		with open(file_path,'r') as f:
			for line in f:
				line=line.rstrip('\n').split('\t')
				if (line[0]!='#pas_id')&(line[0]!='pas_id'):
					SEQUENCE=list(line[7])
					SEQUENCE=SEQUENCE[0:176]
					alphabet = np.array(['A', 'T', 'C', 'G'])
					seq = np.array(SEQUENCE, dtype = '|U1').reshape(-1, 1)
					seq_data = (seq == alphabet).astype(np.float32)
					data1.append(seq_data)
					COVERAGE=[float(x) for x in line[8:184]]
					data2.append(COVERAGE)
					chromosome=line[2]
					position=int(line[3])
					strand=line[4]
					#polyACount = [line[0] for i in range(176)]
					polyACount = [0.0 for i in range(176)]
					for i in range(176):
						if (strand == "+"):
							pos = position-100+i
						else:
							pos = position+100-i
						pasid = chromosome+':'+str(pos)+':'+strand
						if pasid in polyA_dict.keys():
							#polyACount[i] = line[0]+':'+polyA_dict[pasid]
							polyACount[i] = float(polyA_dict[pasid])

					data3.append(polyACount)

	data1 = np.stack(data1).reshape([-1, 176, 4])
	data2 = np.stack(data2).reshape([-1, 176, 1])
	data3 = np.stack(data3).reshape([-1, 176, 1])



	if label == "positive":
		labels = np.ones(data1.shape[0])
	else:
		labels = np.zeros(data1.shape[0])
	return data1, data2, data3, labels


def shuffle(dataset1, dataset2, dataset3, labels, randomState=None):
	"""
	Shuffle sequences and labels jointly
	"""
	if randomState is None:
		permutation = np.random.permutation(labels.shape[0])
	else:
		permutation = randomState.permutation(labels.shape[0])
	shuffled_data1 = dataset1[permutation,:,:]
	shuffled_data2 = dataset2[permutation,:,:]
	shuffled_data3 = dataset3[permutation,:,:]
	shuffled_labels = labels[permutation]
	return shuffled_data1, shuffled_data2, shuffled_data3, shuffled_labels


def data_split(pos_data1, pos_data2,pos_data3, pos_labels, neg_data1, neg_data2,neg_data3, neg_labels, num_folds):
	"""
	Split the dataset into num_folds folds.
	Then split train, valid, and test sets according to the input dict split.
	"""
	split = {
		'train': [i for i in range(num_folds-1)],
		'valid': [num_folds-1],
		}
	pos_data1_folds = np.array_split(pos_data1, num_folds)
	pos_data2_folds = np.array_split(pos_data2, num_folds)
	pos_data3_folds = np.array_split(pos_data3, num_folds)
	neg_data1_folds = np.array_split(neg_data1, num_folds)
	neg_data2_folds = np.array_split(neg_data2, num_folds)
	neg_data3_folds = np.array_split(neg_data3, num_folds)
	pos_label_folds = np.array_split(pos_labels, num_folds)
	neg_label_folds = np.array_split(neg_labels, num_folds)

	train_pos_data1 = np.concatenate([pos_data1_folds[i] for i in split['train']], axis=0)
	train_pos_data2 = np.concatenate([pos_data2_folds[i] for i in split['train']], axis=0)
	train_pos_data3 = np.concatenate([pos_data3_folds[i] for i in split['train']], axis=0)
	train_pos_labels = np.concatenate([pos_label_folds[i] for i in split['train']], axis=0)
	valid_pos_data1 = np.concatenate([pos_data1_folds[i] for i in split['valid']], axis=0)
	valid_pos_data2 = np.concatenate([pos_data2_folds[i] for i in split['valid']], axis=0)
	valid_pos_data3 = np.concatenate([pos_data3_folds[i] for i in split['valid']], axis=0)
	valid_pos_labels = np.concatenate([pos_label_folds[i] for i in split['valid']], axis=0)

	train_neg_data1 = np.concatenate([neg_data1_folds[i] for i in split['train']], axis=0)
	train_neg_data2 = np.concatenate([neg_data2_folds[i] for i in split['train']], axis=0)
	train_neg_data3 = np.concatenate([neg_data3_folds[i] for i in split['train']], axis=0)
	train_neg_labels = np.concatenate([neg_label_folds[i] for i in split['train']], axis=0)
	valid_neg_data1 = np.concatenate([neg_data1_folds[i] for i in split['valid']], axis=0)
	valid_neg_data2 = np.concatenate([neg_data2_folds[i] for i in split['valid']], axis=0)
	valid_neg_data3 = np.concatenate([neg_data3_folds[i] for i in split['valid']], axis=0)
	valid_neg_labels = np.concatenate([neg_label_folds[i] for i in split['valid']], axis=0)

	train_data1 = np.concatenate((train_pos_data1, train_neg_data1), axis=0)
	train_data2 = np.concatenate((train_pos_data2, train_neg_data2), axis=0)
	train_data3 = np.concatenate((train_pos_data3, train_neg_data3), axis=0)
	valid_data1 = np.concatenate((valid_pos_data1, valid_neg_data1), axis=0)
	valid_data2 = np.concatenate((valid_pos_data2, valid_neg_data2), axis=0)
	valid_data3 = np.concatenate((valid_pos_data3, valid_neg_data3), axis=0)
	train_labels = np.concatenate((train_pos_labels, train_neg_labels), axis=0)
	valid_labels = np.concatenate((valid_pos_labels, valid_neg_labels), axis=0)

	data = {}
	data['train_dataset1'],data['train_dataset2'],data['train_dataset3'], data['train_labels'] = shuffle(train_data1, train_data2,train_data3, train_labels)
	data['valid_dataset1'],data['valid_dataset2'],data['valid_dataset3'], data['valid_labels'] = shuffle(valid_data1, valid_data2, valid_data3, valid_labels)

	if 'test' in split:
		test_pos_data1 = np.concatenate([pos_data1_folds[i] for i in split['test']], axis=0)
		test_pos_data2 = np.concatenate([pos_data2_folds[i] for i in split['test']], axis=0)
		test_pos_labels = np.concatenate([pos_label_folds[i] for i in split['test']], axis=0)
		test_neg_data1 = np.concatenate([neg_data1_folds[i] for i in split['test']], axis=0)
		test_neg_data2 = np.concatenate([neg_data2_folds[i] for i in split['test']], axis=0)
		test_neg_labels = np.concatenate([neg_label_folds[i] for i in split['test']], axis=0)
		test_data1 = np.concatenate((test_pos_data1, test_neg_data1), axis=0)
		test_data2 = np.concatenate((test_pos_data2, test_neg_data2), axis=0)
		test_labels = np.concatenate((test_pos_labels, test_neg_labels), axis=0)
		data['test_dataset1'], data['test_dataset2'], data['test_labels'] = shuffle(test_data1, test_data2, test_labels)

	return data


#def prep_data(ROOT_DIR,ROUNDNAME,OUT,NUM_FOLDS):
def prep_data(ROOT_DIR,BLOCK_DIR,polyA_file,ROUNDNAME,NUM_FOLDS,OUT=None):
	train_pos_data1, train_pos_data2, train_pos_data3, train_pos_labels = get_data(ROOT_DIR,BLOCK_DIR,polyA_file,ROUNDNAME, 'positive','train')
	train_neg_data1, train_neg_data2, train_neg_data3, train_neg_labels = get_data(ROOT_DIR,BLOCK_DIR,polyA_file,ROUNDNAME, 'negative','train')
	train_data1 = np.concatenate((train_pos_data1, train_neg_data1), axis=0)
	train_data2 = np.concatenate((train_pos_data2, train_neg_data2), axis=0)
	train_data3 = np.concatenate((train_pos_data3, train_neg_data3), axis=0)
	train_labels = np.concatenate((train_pos_labels, train_neg_labels), axis=0)


	valid_pos_data1, valid_pos_data2, valid_pos_data3, valid_pos_labels = get_data(ROOT_DIR,BLOCK_DIR,polyA_file,ROUNDNAME, 'positive','valid')
	valid_neg_data1, valid_neg_data2, valid_neg_data3, valid_neg_labels = get_data(ROOT_DIR,BLOCK_DIR,polyA_file,ROUNDNAME, 'negative','valid')
	valid_data1 = np.concatenate((valid_pos_data1, valid_neg_data1), axis=0)
	valid_data2 = np.concatenate((valid_pos_data2, valid_neg_data2), axis=0)
	valid_data3 = np.concatenate((valid_pos_data3, valid_neg_data3), axis=0)
	valid_labels = np.concatenate((valid_pos_labels, valid_neg_labels), axis=0)

	data = {}
	data['train_dataset1'],data['train_dataset2'], data['train_dataset3'], data['train_labels'] = shuffle(train_data1, train_data2 ,train_data3, train_labels)
	data['valid_dataset1'],data['valid_dataset2'], data['valid_dataset3'], data['valid_labels'] = shuffle(valid_data1, valid_data2 ,valid_data3, valid_labels)


	#Assert(pos_sets,neg_sets)

	#dataset = data_split(pos_data1, pos_data2, pos_data3, pos_labels,neg_data1,neg_data2,neg_data3, neg_labels, NUM_FOLDS)
	dataset = data
	print('Size of training dataset: %d'%dataset['train_labels'].shape[0])
	print('Size of validation dataset: %d'%dataset['valid_labels'].shape[0])

	if OUT is not None:
		np.savez(OUT, **dataset)
		print('Finish writing dataset to %s'%OUT)
	return dataset

def Assert(pos_sets,neg_sets):
	n = len(pos_sets)
	m = len(neg_sets)
	if(m!=n):
		print("Warning! Size of positive and negative datasets is different!")

	else:
		indicator = 0
		for i in range(n):
			pos_file = pos_sets[i]
			neg_file = neg_sets[i]
			pos_baseName = pos_file.split('/')[-1]
			neg_baseName = neg_file.split('/')[-1]
			if(pos_baseName != neg_baseName):
				print('Warning! Name of positive and negative file is different')
				indicator = 1
		if(indicator==0):
			print('Positive and negative dataset is normal')

if __name__ == "__main__":
	### Argument Parser
	parser = argparse.ArgumentParser()
	parser.add_argument('--root_dir', help='Directory of files containing positive and negative files')
	parser.add_argument('--block_dir', help='Directory of files containing scan transcriptime files')
	parser.add_argument('--round',help='Round of training')
	parser.add_argument('--out', help='Save the processed dataset to')
	parser.add_argument('--polyAfile', help='polyA file from RAN-Seq conataining position and polyA read counts')
	parser.add_argument('--nfolds', default=5, type=int, help='Seperate the data into how many folds')
	opts = parser.parse_args()

	############Global Parameters ############
	NUM_FOLDS = opts.nfolds
	ROOT_DIR  = opts.root_dir
	BLOCK_DIR  = opts.block_dir
	ROUNDNAME  = opts.round
	polyA_file = opts.polyAfile
	OUT       = opts.out
	############Global  Parameters ############
	prep_data(ROOT_DIR,BLOCK_DIR,polyA_file,ROUNDNAME,NUM_FOLDS,OUT)
